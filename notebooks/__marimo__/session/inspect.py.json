{
    "version": "1",
    "metadata": {
        "marimo_version": "0.17.8"
    },
    "cells": [
        {
            "id": "Hbol",
            "code_hash": "156e17b03bed41added15a777cab255d",
            "outputs": [
                {
                    "type": "data",
                    "data": {
                        "text/plain": ""
                    }
                }
            ],
            "console": []
        },
        {
            "id": "UiRI",
            "code_hash": "c08056d74e55b1d244be1dc792e99946",
            "outputs": [
                {
                    "type": "data",
                    "data": {
                        "application/json": "[{\"name\": \"sentinels\", \"description\": \"# Overview\\n\\nThe sentinels module is a small utility providing the Sentinel class, along with useful instances.\\n\\n# What are Sentinels?\\n\\nSentinels are objects with special meanings. They can be thought of as singletons, but they service the need of having 'special' values in your code, that have special meanings (see example below).\\n\\n# Why Do I Need Sentinels?\\n\\nLet's take *NOTHING* for example. This sentinel is automatically provided with the sentinels import:\\n\\n```python\\n>>> from sentinels import NOTHING\\n\\n```\\n\\nLet's say you're writing a wrapper around a Python dictionary, which supports a special kind of method, *get_default_or_raise*. This method behaves like *get*, but when it does not receive a default and the key does not exist, it raises a *KeyError*. How would you implement such a thing? The naive method is this:\\n\\n```python\\n>>> class MyDict(dict):\\n...     def get_default_or_raise(self, key, default=None):\\n...         if key not in self and default is None:\\n...             raise KeyError(key)\\n...         return self.get(key, default)\\n\\n```\\n\\nOr even this:\\n\\n```python\\n>>> class MyDict(dict):\\n...     def get_default_or_raise(self, key, default=None):\\n...         returned = self.get(key, default)\\n...         if returned is None:\\n...             raise KeyError(key)\\n...         return returned\\n\\n```\\n\\nBut the problem with the above two pieces of code is the same -- when writing a general utility class, we don't know how it will be used later on. More importantly, **None might be a perfectly valid dictionary value!**\\n\\nThis is where NOTHING comes in handy:\\n\\n```python\\n>>> class MyDict(dict):\\n...     def get_default_or_raise(self, key, default=NOTHING):\\n...         returned = self.get(key, default)\\n...         if returned is NOTHING:\\n...             raise KeyError(key)\\n...         return returned\\n\\n```\\n\\nAnd Tada!\\n\\n# Semantics\\n\\nSentinels are always equal to themselves:\\n\\n```python\\n>>> NOTHING == NOTHING\\nTrue\\n\\n```\\n\\nBut never to another object:\\n\\n```python\\n>>> from sentinels import Sentinel\\n>>> NOTHING == 2\\nFalse\\n>>> NOTHING == \\\"NOTHING\\\"\\nFalse\\n\\n```\\n\\nCopying sentinels returns the same object:\\n\\n```python\\n>>> import copy\\n>>> copy.deepcopy(NOTHING) is NOTHING\\nTrue\\n\\n```\\n\\nAnd of course also pickling/unpickling:\\n\\n```python\\n>>> import pickle\\n>>> NOTHING is pickle.loads(pickle.dumps(NOTHING))\\nTrue\\n\\n```\", \"description_content_type\": \"text/markdown\", \"summary\": \"Various objects to denote special meanings in python\", \"latest_version\": \"1.1.1\", \"weekly_downloads\": 815973, \"description_cleaned\": \"Overview\\nThe sentinels module is a small utility providing the Sentinel class, along with useful instances.\\nWhat are Sentinels?\\nSentinels are objects with special meanings. They can be thought of as singletons, but they service the need of having 'special' values in your code, that have special meanings (see example below).\\nWhy Do I Need Sentinels?\\nLet's take\\nNOTHING\\nfor example. This sentinel is automatically provided with the sentinels import:\\n>>>\\nfrom\\nsentinels\\nimport\\nNOTHING\\nLet's say you're writing a wrapper around a Python dictionary, which supports a special kind of method,\\nget_default_or_raise\\n. This method behaves like\\nget\\n, but when it does not receive a default and the key does not exist, it raises a\\nKeyError\\n. How would you implement such a thing? The naive method is this:\\n>>>\\nclass\\nMyDict\\n(\\ndict\\n):\\n...\\ndef\\nget_default_or_raise\\n(\\nself\\n,\\nkey\\n,\\ndefault\\n=\\nNone\\n):\\n...\\nif\\nkey\\nnot\\nin\\nself\\nand\\ndefault\\nis\\nNone\\n:\\n...\\nraise\\nKeyError\\n(\\nkey\\n)\\n...\\nreturn\\nself\\n.\\nget\\n(\\nkey\\n,\\ndefault\\n)\\nOr even this:\\n>>>\\nclass\\nMyDict\\n(\\ndict\\n):\\n...\\ndef\\nget_default_or_raise\\n(\\nself\\n,\\nkey\\n,\\ndefault\\n=\\nNone\\n):\\n...\\nreturned\\n=\\nself\\n.\\nget\\n(\\nkey\\n,\\ndefault\\n)\\n...\\nif\\nreturned\\nis\\nNone\\n:\\n...\\nraise\\nKeyError\\n(\\nkey\\n)\\n...\\nreturn\\nreturned\\nBut the problem with the above two pieces of code is the same -- when writing a general utility class, we don't know how it will be used later on. More importantly,\\nNone might be a perfectly valid dictionary value!\\nThis is where NOTHING comes in handy:\\n>>>\\nclass\\nMyDict\\n(\\ndict\\n):\\n...\\ndef\\nget_default_or_raise\\n(\\nself\\n,\\nkey\\n,\\ndefault\\n=\\nNOTHING\\n):\\n...\\nreturned\\n=\\nself\\n.\\nget\\n(\\nkey\\n,\\ndefault\\n)\\n...\\nif\\nreturned\\nis\\nNOTHING\\n:\\n...\\nraise\\nKeyError\\n(\\nkey\\n)\\n...\\nreturn\\nreturned\\nAnd Tada!\\nSemantics\\nSentinels are always equal to themselves:\\n>>>\\nNOTHING\\n==\\nNOTHING\\nTrue\\nBut never to another object:\\n>>>\\nfrom\\nsentinels\\nimport\\nSentinel\\n>>>\\nNOTHING\\n==\\n2\\nFalse\\n>>>\\nNOTHING\\n==\\n\\\"NOTHING\\\"\\nFalse\\nCopying sentinels returns the same object:\\n>>>\\nimport\\ncopy\\n>>>\\ncopy\\n.\\ndeepcopy\\n(\\nNOTHING\\n)\\nis\\nNOTHING\\nTrue\\nAnd of course also pickling/unpickling:\\n>>>\\nimport\\npickle\\n>>>\\nNOTHING\\nis\\npickle\\n.\\nloads\\n(\\npickle\\n.\\ndumps\\n(\\nNOTHING\\n))\\nTrue\"}, {\"name\": \"mongomock\", \"description\": \".. image:: https://img.shields.io/pypi/v/mongomock.svg?style=flat-square\\n    :target: https://pypi.python.org/pypi/mongomock\\n.. image:: https://img.shields.io/github/actions/workflow/status/mongomock/mongomock/lint-and-test.yml?branch=develop&style=flat-square\\n    :target: https://github.com/mongomock/mongomock/actions?query=workflow%3Alint-and-test\\n.. image:: https://img.shields.io/pypi/l/mongomock.svg?style=flat-square\\n    :target: https://pypi.python.org/pypi/mongomock\\n.. image:: https://img.shields.io/codecov/c/github/mongomock/mongomock.svg?style=flat-square\\n    :target: https://codecov.io/gh/mongomock/mongomock\\n\\n\\nWhat is this?\\n-------------\\nMongomock is a small library to help testing Python code that interacts with MongoDB via Pymongo.\\n\\nTo understand what it's useful for, we can take the following code:\\n\\n.. code-block:: python\\n\\n def increase_votes(collection):\\n     for document in collection.find():\\n         collection.update_one(document, {'$set': {'votes': document['votes'] + 1}})\\n\\nThe above code can be tested in several ways:\\n\\n1. It can be tested against a real mongodb instance with pymongo.\\n2. It can receive a record-replay style mock as an argument. In this manner we record the\\n   expected calls (find, and then a series of updates), and replay them later.\\n3. It can receive a carefully hand-crafted mock responding to find() and update() appropriately.\\n\\nOption number 1 is obviously the best approach here, since we are testing against a real mongodb\\ninstance. However, a mongodb instance needs to be set up for this, and cleaned before/after the\\ntest. You might want to run your tests in continuous integration servers, on your laptop, or\\nother bizarre platforms - which makes the mongodb requirement a liability.\\n\\nWe are left with #2 and #3. Unfortunately they are very high maintenance in real scenarios,\\nsince they replicate the series of calls made in the code, violating the DRY rule. Let's see\\n#2 in action - we might write our test like so:\\n\\n.. code-block:: python\\n\\n def test_increase_votes():\\n     objects = [dict(...), dict(...), ...]\\n     collection_mock = my_favorite_mock_library.create_mock(Collection)\\n     record()\\n     collection_mock.find().AndReturn(objects)\\n     for obj in objects:\\n         collection_mock.update_one(obj, {'$set': {'votes': obj['votes']}})\\n     replay()\\n     increase_votes(collection_mock)\\n     verify()\\n\\nLet's assume the code changes one day, because the author just learned about the '$inc' instruction:\\n\\n.. code-block:: python\\n\\n def increase_votes(collection):\\n     collection.update_many({}, {'$inc': {'votes': 1}})\\n\\nThis breaks the test, although the end result being tested is just the same. The test also repeats\\nlarge portions of the code we already wrote.\\n\\nWe are left, therefore, with option #3 -- you want something to behave like a mongodb database\\ncollection, without being one. This is exactly what this library aims to provide. With mongomock,\\nthe test simply becomes:\\n\\n.. code-block:: python\\n\\n def test_increase_votes():\\n     collection = mongomock.MongoClient().db.collection\\n     objects = [dict(votes=1), dict(votes=2), ...]\\n     for obj in objects:\\n         obj['_id'] = collection.insert_one(obj).inserted_id\\n     increase_votes(collection)\\n     for obj in objects:\\n         stored_obj = collection.find_one({'_id': obj['_id']})\\n         stored_obj['votes'] -= 1\\n         assert stored_obj == obj # by comparing all fields we make sure only votes changed\\n\\nThis code checks *increase_votes* with respect to its functionality, not syntax or algorithm, and\\ntherefore is much more robust as a test.\\n\\nIf the code to be tested is creating the connection itself with pymongo, you can use\\nmongomock.patch (NOTE: you should use :code:`pymongo.MongoClient(...)` rather than\\n:code:`from pymongo import MongoClient`, as shown below):\\n\\n.. code-block:: python\\n\\n  @mongomock.patch(servers=(('server.example.com', 27017),))\\n  def test_increate_votes_endpoint():\\n    objects = [dict(votes=1), dict(votes=2), ...]\\n    client = pymongo.MongoClient('server.example.com')\\n    client.db.collection.insert_many(objects)\\n    call_endpoint('/votes')\\n    ... verify client.db.collection\\n\\n\\nImportant Note About Project Status & Development\\n-------------------------------------------------\\n\\nMongoDB is complex. This library aims at a reasonably complete mock of MongoDB for testing purposes,\\nnot a perfect replica. This means some features are not likely to make it in any time soon.\\n\\nAlso, since many corner cases are encountered along the way, our goal is to try and TDD our way into\\ncompleteness. This means that every time we encounter a missing or broken (incompatible) feature,\\nwe write a test for it and fix it. There are probably lots of such issues hiding around lurking,\\nso feel free to open issues and/or pull requests and help the project out!\\n\\n**NOTE**: We don't include pymongo functionality as \\\"stubs\\\" or \\\"placeholders\\\". Since this library is\\nused to validate production code, it is unacceptable to behave differently than the real pymongo\\nimplementation. In such cases it is better to throw `NotImplementedError` than implement a modified\\nversion of the original behavior.\\n\\nUpgrading to Pymongo v4\\n-----------------------\\n\\nThe major version 4 of Pymongo changed the API quite a bit. The Mongomock library has evolved to\\nhelp you ease the migration:\\n\\n1. Upgrade to Mongomock v4 or above: if your tests are running with Pymongo installed, Mongomock\\n   will adapt its own API to the version of Pymongo installed.\\n2. Upgrade to Pymongo v4 or above: your tests using Mongomock will fail exactly where your code\\n   would fail in production, so that you can fix it before releasing.\\n\\nContributing\\n------------\\n\\nWhen submitting a PR, please make sure that:\\n\\n1. You include tests for the feature you are adding or bug you are fixing. Preferably, the test\\n   should compare against the real MongoDB engine (see `examples in tests`_ for reference).\\n2. No existing test got deleted or unintentionally castrated\\n3. The build passes on your PR.\\n\\nTo download, setup and perfom tests, run the following commands on Mac / Linux:\\n\\n.. code-block:: console\\n\\n $ git clone git@github.com:mongomock/mongomock.git\\n $ pipx install hatch\\n $ cd mongomock\\n $ hatch test\\n\\nAlternatively, docker-compose can be used to simplify dependency management for local development:\\n\\n.. code-block:: console\\n\\n $ git clone git@github.com:mongomock/mongomock.git\\n $ cd mongomock\\n $ docker compose build\\n $ docker compose run --rm mongomock\\n\\nIf you want to run ``hatch`` against a specific environment in the container:\\n\\n.. code-block:: console\\n\\n $ docker compose run --rm mongomock hatch test -py=3.11 -i pymongo=4\\n\\nIf you'd like to run only one test, you can also add the test name at the end of your command:\\n\\n.. code-block:: console\\n\\n $ docker compose run --rm mongomock hatch test -py=3.12 -i pymongo=4 tests/test__mongomock.py::MongoClientCollectionTest::test__insert\\n\\nNOTE: If the MongoDB image was updated, or you want to try a different MongoDB version in\\n``docker-compose``, you'll have to issue a ``docker compose down`` before you do anything else to\\nensure you're running against the intended version.\\n\\nutcnow\\n~~~~~~\\n\\nWhen developing features that need to make use of \\\"now,\\\" please use the libraries :code:`utcnow`\\nhelper method in the following way:\\n\\n.. code-block:: python\\n\\n   import mongomock\\n   # Awesome code!\\n   now_reference = mongomock.utcnow()\\n\\nThis provides users a consistent way to mock the notion of \\\"now\\\" in mongomock if they so choose.\\nPlease see `utcnow docstring for more details <mongomock/helpers.py#L52>`_.\\n\\nBranching model\\n~~~~~~~~~~~~~~~\\n\\nThe branching model used for this project follows the `gitflow workflow`_.  This means that pull\\nrequests should be issued against the `develop` branch and *not* the `master` branch. If you want\\nto contribute to the legacy 2.x branch then your pull request should go into the `support/2.x`\\nbranch.\\n\\nReleasing\\n~~~~~~~~~\\n\\nWhen ready for a release, tag the `develop` branch with a new tag (please keep semver names) and\\npush your tags to GitHub. The CI should do the rest.\\n\\nTo add release notes, create a release in GitHub's `Releases Page <https://github.com/mongomock/mongomock/releases>`_\\nthen generate the release notes locally with:\\n\\n.. code-block:: bash\\n\\n  python -c \\\"from pbr import git; git.write_git_changelog()\\\"\\n\\nThen you can get the relevant section in the generated `Changelog` file.\\n\\nAcknowledgements\\n----------------\\n\\nMongomock has originally been developed by `Rotem Yaari <https://github.com/vmalloc/>`_, then by\\n`Martin Domke <https://github.com/mdomke>`_. It is currently being developed and maintained by\\n`Pascal Corpet <https://github.com/pcorpet>`_ .\\n\\nAlso, many thanks go to the following people for helping out, contributing pull requests and fixing\\nbugs:\\n\\n* Alec Perkins\\n* Alexandre Viau\\n* Austin W Ellis\\n* Andrey Ovchinnikov\\n* Arthur Hirata\\n* Baruch Oxman\\n* Corey Downing\\n* Craig Hobbs\\n* Daniel Murray\\n* David Fischer\\n* Diego Garcia\\n* Dmitriy Kostochko\\n* Drew Winstel\\n* Eddie Linder\\n* Edward D'Souza\\n* Emily Rosengren\\n* Eugene Chernyshov\\n* Grigoriy Osadchenko\\n* Israel Teixeira\\n* Jacob Perkins\\n* Jason Burchfield\\n* Jason Sommer\\n* Jeff Browning\\n* Jeff McGee\\n* Jo\\u00ebl Franusic\\n* `Jonathan Hed\\u00e9n <https://github.com/jheden/>`_\\n* Julian Hille\\n* Krzysztof P\\u0142ocharz\\n* Lyon Zhang\\n* `Lucas Rangel Cezimbra <https://github.com/Lrcezimbra/>`_\\n* Marc Prewitt\\n* Marcin Barczynski\\n* Marian Galik\\n* Micha\\u0142 Albrycht\\n* Mike Ho\\n* Nigel Choi\\n* Omer Gertel\\n* Omer Katz\\n* Papp Gy\\u0151z\\u0151\\n* Paul Glass\\n* Scott Sexton\\n* Srinivas Reddy Thatiparthy\\n* Taras Boiko\\n* Todd Tomkinson\\n* `Xinyan Lu <https://github.com/lxy1992/>`_\\n* Zachary Carter\\n* catty (ca77y _at_ live.com)\\n* emosenkis\\n* hthieu1110\\n* \\u05d9ppetlinskiy\\n* pacud\\n* tipok\\n* waskew (waskew _at_ narrativescience.com)\\n* jmsantorum (jmsantorum [at] gmail [dot] com)\\n* lidongyong\\n* `Juan Gutierrez <https://github.com/juannyg/>`_\\n\\n.. _examples in tests: https://github.com/mongomock/mongomock/blob/develop/tests/test__mongomock.py\\n.. _gitflow workflow: https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"Fake pymongo stub for testing simple MongoDB-dependent code\", \"latest_version\": \"4.3.0\", \"weekly_downloads\": 815018, \"description_cleaned\": \"What is this?\\nMongomock is a small library to help testing Python code that interacts with MongoDB via Pymongo.\\nTo understand what it\\u2019s useful for, we can take the following code:\\ndef\\nincrease_votes\\n(\\ncollection\\n):\\nfor\\ndocument\\nin\\ncollection\\n.\\nfind\\n():\\ncollection\\n.\\nupdate_one\\n(\\ndocument\\n,\\n{\\n'$set'\\n:\\n{\\n'votes'\\n:\\ndocument\\n[\\n'votes'\\n]\\n+\\n1\\n}})\\nThe above code can be tested in several ways:\\nIt can be tested against a real mongodb instance with pymongo.\\nIt can receive a record-replay style mock as an argument. In this manner we record the\\nexpected calls (find, and then a series of updates), and replay them later.\\nIt can receive a carefully hand-crafted mock responding to find() and update() appropriately.\\nOption number 1 is obviously the best approach here, since we are testing against a real mongodb\\ninstance. However, a mongodb instance needs to be set up for this, and cleaned before/after the\\ntest. You might want to run your tests in continuous integration servers, on your laptop, or\\nother bizarre platforms - which makes the mongodb requirement a liability.\\nWe are left with #2 and #3. Unfortunately they are very high maintenance in real scenarios,\\nsince they replicate the series of calls made in the code, violating the DRY rule. Let\\u2019s see\\n#2 in action - we might write our test like so:\\ndef\\ntest_increase_votes\\n():\\nobjects\\n=\\n[\\ndict\\n(\\n...\\n),\\ndict\\n(\\n...\\n),\\n...\\n]\\ncollection_mock\\n=\\nmy_favorite_mock_library\\n.\\ncreate_mock\\n(\\nCollection\\n)\\nrecord\\n()\\ncollection_mock\\n.\\nfind\\n()\\n.\\nAndReturn\\n(\\nobjects\\n)\\nfor\\nobj\\nin\\nobjects\\n:\\ncollection_mock\\n.\\nupdate_one\\n(\\nobj\\n,\\n{\\n'$set'\\n:\\n{\\n'votes'\\n:\\nobj\\n[\\n'votes'\\n]}})\\nreplay\\n()\\nincrease_votes\\n(\\ncollection_mock\\n)\\nverify\\n()\\nLet\\u2019s assume the code changes one day, because the author just learned about the \\u2018$inc\\u2019 instruction:\\ndef\\nincrease_votes\\n(\\ncollection\\n):\\ncollection\\n.\\nupdate_many\\n({},\\n{\\n'$inc'\\n:\\n{\\n'votes'\\n:\\n1\\n}})\\nThis breaks the test, although the end result being tested is just the same. The test also repeats\\nlarge portions of the code we already wrote.\\nWe are left, therefore, with option #3 \\u2013 you want something to behave like a mongodb database\\ncollection, without being one. This is exactly what this library aims to provide. With mongomock,\\nthe test simply becomes:\\ndef\\ntest_increase_votes\\n():\\ncollection\\n=\\nmongomock\\n.\\nMongoClient\\n()\\n.\\ndb\\n.\\ncollection\\nobjects\\n=\\n[\\ndict\\n(\\nvotes\\n=\\n1\\n),\\ndict\\n(\\nvotes\\n=\\n2\\n),\\n...\\n]\\nfor\\nobj\\nin\\nobjects\\n:\\nobj\\n[\\n'_id'\\n]\\n=\\ncollection\\n.\\ninsert_one\\n(\\nobj\\n)\\n.\\ninserted_id\\nincrease_votes\\n(\\ncollection\\n)\\nfor\\nobj\\nin\\nobjects\\n:\\nstored_obj\\n=\\ncollection\\n.\\nfind_one\\n({\\n'_id'\\n:\\nobj\\n[\\n'_id'\\n]})\\nstored_obj\\n[\\n'votes'\\n]\\n-=\\n1\\nassert\\nstored_obj\\n==\\nobj\\n# by comparing all fields we make sure only votes changed\\nThis code checks\\nincrease_votes\\nwith respect to its functionality, not syntax or algorithm, and\\ntherefore is much more robust as a test.\\nIf the code to be tested is creating the connection itself with pymongo, you can use\\nmongomock.patch (NOTE: you should use\\npymongo.MongoClient(...)\\nrather than\\nfrom pymongo import MongoClient\\n, as shown below):\\n@mongomock\\n.\\npatch\\n(\\nservers\\n=\\n((\\n'server.example.com'\\n,\\n27017\\n),))\\ndef\\ntest_increate_votes_endpoint\\n():\\nobjects\\n=\\n[\\ndict\\n(\\nvotes\\n=\\n1\\n),\\ndict\\n(\\nvotes\\n=\\n2\\n),\\n...\\n]\\nclient\\n=\\npymongo\\n.\\nMongoClient\\n(\\n'server.example.com'\\n)\\nclient\\n.\\ndb\\n.\\ncollection\\n.\\ninsert_many\\n(\\nobjects\\n)\\ncall_endpoint\\n(\\n'/votes'\\n)\\n...\\nverify\\nclient\\n.\\ndb\\n.\\ncollection\\nImportant Note About Project Status & Development\\nMongoDB is complex. This library aims at a reasonably complete mock of MongoDB for testing purposes,\\nnot a perfect replica. This means some features are not likely to make it in any time soon.\\nAlso, since many corner cases are encountered along the way, our goal is to try and TDD our way into\\ncompleteness. This means that every time we encounter a missing or broken (incompatible) feature,\\nwe write a test for it and fix it. There are probably lots of such issues hiding around lurking,\\nso feel free to open issues and/or pull requests and help the project out!\\nNOTE\\n: We don\\u2019t include pymongo functionality as \\u201cstubs\\u201d or \\u201cplaceholders\\u201d. Since this library is\\nused to validate production code, it is unacceptable to behave differently than the real pymongo\\nimplementation. In such cases it is better to throw\\nNotImplementedError\\nthan implement a modified\\nversion of the original behavior.\\nUpgrading to Pymongo v4\\nThe major version 4 of Pymongo changed the API quite a bit. The Mongomock library has evolved to\\nhelp you ease the migration:\\nUpgrade to Mongomock v4 or above: if your tests are running with Pymongo installed, Mongomock\\nwill adapt its own API to the version of Pymongo installed.\\nUpgrade to Pymongo v4 or above: your tests using Mongomock will fail exactly where your code\\nwould fail in production, so that you can fix it before releasing.\\nContributing\\nWhen submitting a PR, please make sure that:\\nYou include tests for the feature you are adding or bug you are fixing. Preferably, the test\\nshould compare against the real MongoDB engine (see\\nexamples in tests\\nfor reference).\\nNo existing test got deleted or unintentionally castrated\\nThe build passes on your PR.\\nTo download, setup and perfom tests, run the following commands on Mac / Linux:\\n$\\ngit\\nclone\\ngit@github.com:mongomock/mongomock.git\\n$\\npipx\\ninstall\\nhatch\\n$\\ncd\\nmongomock\\n$\\nhatch\\ntest\\nAlternatively, docker-compose can be used to simplify dependency management for local development:\\n$\\ngit\\nclone\\ngit@github.com:mongomock/mongomock.git\\n$\\ncd\\nmongomock\\n$\\ndocker\\ncompose\\nbuild\\n$\\ndocker\\ncompose\\nrun\\n--rm\\nmongomock\\nIf you want to run\\nhatch\\nagainst a specific environment in the container:\\n$\\ndocker\\ncompose\\nrun\\n--rm\\nmongomock\\nhatch\\ntest\\n-py\\n=\\n3\\n.11\\n-i\\npymongo\\n=\\n4\\nIf you\\u2019d like to run only one test, you can also add the test name at the end of your command:\\n$\\ndocker\\ncompose\\nrun\\n--rm\\nmongomock\\nhatch\\ntest\\n-py\\n=\\n3\\n.12\\n-i\\npymongo\\n=\\n4\\ntests/test__mongomock.py::MongoClientCollectionTest::test__insert\\nNOTE: If the MongoDB image was updated, or you want to try a different MongoDB version in\\ndocker-compose\\n, you\\u2019ll have to issue a\\ndocker compose down\\nbefore you do anything else to\\nensure you\\u2019re running against the intended version.\\nutcnow\\nWhen developing features that need to make use of \\u201cnow,\\u201d please use the libraries\\nutcnow\\nhelper method in the following way:\\nimport\\nmongomock\\n# Awesome code!\\nnow_reference\\n=\\nmongomock\\n.\\nutcnow\\n()\\nThis provides users a consistent way to mock the notion of \\u201cnow\\u201d in mongomock if they so choose.\\nPlease see\\nutcnow docstring for more details\\n.\\nBranching model\\nThe branching model used for this project follows the\\ngitflow workflow\\n.  This means that pull\\nrequests should be issued against the\\ndevelop\\nbranch and\\nnot\\nthe\\nmaster\\nbranch. If you want\\nto contribute to the legacy 2.x branch then your pull request should go into the\\nsupport/2.x\\nbranch.\\nReleasing\\nWhen ready for a release, tag the\\ndevelop\\nbranch with a new tag (please keep semver names) and\\npush your tags to GitHub. The CI should do the rest.\\nTo add release notes, create a release in GitHub\\u2019s\\nReleases Page\\nthen generate the release notes locally with:\\npython\\n-c\\n\\\"from pbr import git; git.write_git_changelog()\\\"\\nThen you can get the relevant section in the generated\\nChangelog\\nfile.\\nAcknowledgements\\nMongomock has originally been developed by\\nRotem Yaari\\n, then by\\nMartin Domke\\n. It is currently being developed and maintained by\\nPascal Corpet\\n.\\nAlso, many thanks go to the following people for helping out, contributing pull requests and fixing\\nbugs:\\nAlec Perkins\\nAlexandre Viau\\nAustin W Ellis\\nAndrey Ovchinnikov\\nArthur Hirata\\nBaruch Oxman\\nCorey Downing\\nCraig Hobbs\\nDaniel Murray\\nDavid Fischer\\nDiego Garcia\\nDmitriy Kostochko\\nDrew Winstel\\nEddie Linder\\nEdward D\\u2019Souza\\nEmily Rosengren\\nEugene Chernyshov\\nGrigoriy Osadchenko\\nIsrael Teixeira\\nJacob Perkins\\nJason Burchfield\\nJason Sommer\\nJeff Browning\\nJeff McGee\\nJo\\u00ebl Franusic\\nJonathan Hed\\u00e9n\\nJulian Hille\\nKrzysztof P\\u0142ocharz\\nLyon Zhang\\nLucas Rangel Cezimbra\\nMarc Prewitt\\nMarcin Barczynski\\nMarian Galik\\nMicha\\u0142 Albrycht\\nMike Ho\\nNigel Choi\\nOmer Gertel\\nOmer Katz\\nPapp Gy\\u0151z\\u0151\\nPaul Glass\\nScott Sexton\\nSrinivas Reddy Thatiparthy\\nTaras Boiko\\nTodd Tomkinson\\nXinyan Lu\\nZachary Carter\\ncatty (ca77y _at_ live.com)\\nemosenkis\\nhthieu1110\\n\\u05d9ppetlinskiy\\npacud\\ntipok\\nwaskew (waskew _at_ narrativescience.com)\\njmsantorum (jmsantorum [at] gmail [dot] com)\\nlidongyong\\nJuan Gutierrez\"}, {\"name\": \"ariadne\", \"description\": \"[![Ariadne](https://ariadnegraphql.org/img/logo-horizontal-sm.png)](https://ariadnegraphql.org)\\n\\n[![Documentation](https://img.shields.io/badge/docs-ariadnegraphql.org-brightgreen.svg)](https://ariadnegraphql.org)\\n[![Codecov](https://codecov.io/gh/mirumee/ariadne/branch/master/graph/badge.svg)](https://codecov.io/gh/mirumee/ariadne)\\n\\n- - - - -\\n\\n# Ariadne\\n\\nAriadne is a Python library for implementing [GraphQL](http://graphql.github.io/) servers.\\n\\n- **Schema-first:** Ariadne enables Python developers to use schema-first approach to the API implementation. This is the leading approach used by the GraphQL community and supported by dozens of frontend and backend developer tools, examples, and learning resources. Ariadne makes all of this immediately available to you and other members of your team.\\n- **Simple:** Ariadne offers small, consistent and easy to memorize API that lets developers focus on business problems, not the boilerplate.\\n- **Open:** Ariadne was designed to be modular and open for customization. If you are missing or unhappy with something, extend or easily swap with your own.\\n\\nDocumentation is available [here](https://ariadnegraphql.org).\\n\\n\\n## Features\\n\\n- Simple, quick to learn and easy to memorize API.\\n- Compatibility with GraphQL.js version 15.5.1.\\n- Queries, mutations and input types.\\n- Asynchronous resolvers and query execution.\\n- Subscriptions.\\n- Custom scalars, enums and schema directives.\\n- Unions and interfaces.\\n- File uploads.\\n- Defining schema using SDL strings.\\n- Loading schema from `.graphql`, `.gql`, and `.graphqls` files.\\n- WSGI middleware for implementing GraphQL in existing sites.\\n- Apollo Tracing and [OpenTracing](http://opentracing.io) extensions for API monitoring.\\n- Opt-in automatic resolvers mapping between `camelCase` and `snake_case`, and a `@convert_kwargs_to_snake_case` function decorator for converting `camelCase` kwargs to `snake_case`.\\n- Built-in simple synchronous dev server for quick GraphQL experimentation and GraphQL Playground.\\n- Support for [Apollo GraphQL extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=apollographql.vscode-apollo).\\n- GraphQL syntax validation via `gql()` helper function. Also provides colorization if Apollo GraphQL extension is installed.\\n- No global state or object registry, support for multiple GraphQL APIs in same codebase with explicit type reuse.\\n- Support for `Apollo Federation`.\\n\\n\\n## Installation\\n\\nAriadne can be installed with pip:\\n\\n```console\\npip install ariadne\\n```\\n\\nAriadne requires Python 3.9 or higher.\\n\\n\\n## Quickstart\\n\\nThe following example creates an API defining `Person` type and single query field `people` returning a list of two persons. It also starts a local dev server with [GraphQL Playground](https://github.com/prisma/graphql-playground) available on the `http://127.0.0.1:8000` address.\\n\\nStart by installing [uvicorn](http://www.uvicorn.org/), an ASGI server we will use to serve the API:\\n\\n```console\\npip install uvicorn\\n```\\n\\nThen create an `example.py` file for your example application:\\n\\n```python\\nfrom ariadne import ObjectType, QueryType, gql, make_executable_schema\\nfrom ariadne.asgi import GraphQL\\n\\n# Define types using Schema Definition Language (https://graphql.org/learn/schema/)\\n# Wrapping string in gql function provides validation and better error traceback\\ntype_defs = gql(\\\"\\\"\\\"\\n    type Query {\\n        people: [Person!]!\\n    }\\n\\n    type Person {\\n        firstName: String\\n        lastName: String\\n        age: Int\\n        fullName: String\\n    }\\n\\\"\\\"\\\")\\n\\n# Map resolver functions to Query fields using QueryType\\nquery = QueryType()\\n\\n# Resolvers are simple python functions\\n@query.field(\\\"people\\\")\\ndef resolve_people(*_):\\n    return [\\n        {\\\"firstName\\\": \\\"John\\\", \\\"lastName\\\": \\\"Doe\\\", \\\"age\\\": 21},\\n        {\\\"firstName\\\": \\\"Bob\\\", \\\"lastName\\\": \\\"Boberson\\\", \\\"age\\\": 24},\\n    ]\\n\\n\\n# Map resolver functions to custom type fields using ObjectType\\nperson = ObjectType(\\\"Person\\\")\\n\\n@person.field(\\\"fullName\\\")\\ndef resolve_person_fullname(person, *_):\\n    return \\\"%s %s\\\" % (person[\\\"firstName\\\"], person[\\\"lastName\\\"])\\n\\n# Create executable GraphQL schema\\nschema = make_executable_schema(type_defs, query, person)\\n\\n# Create an ASGI app using the schema, running in debug mode\\napp = GraphQL(schema, debug=True)\\n```\\n\\nFinally run the server:\\n\\n```console\\nuvicorn example:app\\n```\\n\\nFor more guides and examples, please see the [documentation](https://ariadnegraphql.org).\\n\\n## Versioning policy ##\\n`ariadne` follows a custom versioning scheme where the minor version increases for breaking changes, while the patch version increments for bug fixes, enhancements, and other non-breaking updates.\\n\\nSince `ariadne` has not yet reached a stable API, this approach is in place until version 1.0.0. Once the API stabilizes, the project will adopt [Semantic Versioning](https://semver.org/).\\n\\n\\n## Contributing\\n\\nWe are welcoming contributions to Ariadne! If you've found a bug or issue, feel free to use [GitHub issues](https://github.com/mirumee/ariadne/issues). If you have any questions or feedback, don't hesitate to catch us on [GitHub discussions](https://github.com/mirumee/ariadne/discussions/).\\n\\nFor guidance and instructions, please see [CONTRIBUTING.md](CONTRIBUTING.md).\\n\\nWebsite and the docs have their own GitHub repository: [mirumee/ariadne-website](https://github.com/mirumee/ariadne-website)\\n\\nAlso make sure you follow [@AriadneGraphQL](https://twitter.com/AriadneGraphQL) on Twitter for latest updates, news and random musings!\\n\\n**Crafted with \\u2764\\ufe0f by [Mirumee Software](http://mirumee.com)**\\nhello@mirumee.com\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Ariadne is a Python library for implementing GraphQL servers.\", \"latest_version\": \"0.26.2\", \"weekly_downloads\": 814007, \"description_cleaned\": \"Ariadne\\nAriadne is a Python library for implementing\\nGraphQL\\nservers.\\nSchema-first:\\nAriadne enables Python developers to use schema-first approach to the API implementation. This is the leading approach used by the GraphQL community and supported by dozens of frontend and backend developer tools, examples, and learning resources. Ariadne makes all of this immediately available to you and other members of your team.\\nSimple:\\nAriadne offers small, consistent and easy to memorize API that lets developers focus on business problems, not the boilerplate.\\nOpen:\\nAriadne was designed to be modular and open for customization. If you are missing or unhappy with something, extend or easily swap with your own.\\nDocumentation is available\\nhere\\n.\\nFeatures\\nSimple, quick to learn and easy to memorize API.\\nCompatibility with GraphQL.js version 15.5.1.\\nQueries, mutations and input types.\\nAsynchronous resolvers and query execution.\\nSubscriptions.\\nCustom scalars, enums and schema directives.\\nUnions and interfaces.\\nFile uploads.\\nDefining schema using SDL strings.\\nLoading schema from\\n.graphql\\n,\\n.gql\\n, and\\n.graphqls\\nfiles.\\nWSGI middleware for implementing GraphQL in existing sites.\\nApollo Tracing and\\nOpenTracing\\nextensions for API monitoring.\\nOpt-in automatic resolvers mapping between\\ncamelCase\\nand\\nsnake_case\\n, and a\\n@convert_kwargs_to_snake_case\\nfunction decorator for converting\\ncamelCase\\nkwargs to\\nsnake_case\\n.\\nBuilt-in simple synchronous dev server for quick GraphQL experimentation and GraphQL Playground.\\nSupport for\\nApollo GraphQL extension for Visual Studio Code\\n.\\nGraphQL syntax validation via\\ngql()\\nhelper function. Also provides colorization if Apollo GraphQL extension is installed.\\nNo global state or object registry, support for multiple GraphQL APIs in same codebase with explicit type reuse.\\nSupport for\\nApollo Federation\\n.\\nInstallation\\nAriadne can be installed with pip:\\npip install ariadne\\nAriadne requires Python 3.9 or higher.\\nQuickstart\\nThe following example creates an API defining\\nPerson\\ntype and single query field\\npeople\\nreturning a list of two persons. It also starts a local dev server with\\nGraphQL Playground\\navailable on the\\nhttp://127.0.0.1:8000\\naddress.\\nStart by installing\\nuvicorn\\n, an ASGI server we will use to serve the API:\\npip install uvicorn\\nThen create an\\nexample.py\\nfile for your example application:\\nfrom\\nariadne\\nimport\\nObjectType\\n,\\nQueryType\\n,\\ngql\\n,\\nmake_executable_schema\\nfrom\\nariadne.asgi\\nimport\\nGraphQL\\n# Define types using Schema Definition Language (https://graphql.org/learn/schema/)\\n# Wrapping string in gql function provides validation and better error traceback\\ntype_defs\\n=\\ngql\\n(\\n\\\"\\\"\\\"\\ntype Query {\\npeople: [Person!]!\\n}\\ntype Person {\\nfirstName: String\\nlastName: String\\nage: Int\\nfullName: String\\n}\\n\\\"\\\"\\\"\\n)\\n# Map resolver functions to Query fields using QueryType\\nquery\\n=\\nQueryType\\n()\\n# Resolvers are simple python functions\\n@query\\n.\\nfield\\n(\\n\\\"people\\\"\\n)\\ndef\\nresolve_people\\n(\\n*\\n_\\n):\\nreturn\\n[\\n{\\n\\\"firstName\\\"\\n:\\n\\\"John\\\"\\n,\\n\\\"lastName\\\"\\n:\\n\\\"Doe\\\"\\n,\\n\\\"age\\\"\\n:\\n21\\n},\\n{\\n\\\"firstName\\\"\\n:\\n\\\"Bob\\\"\\n,\\n\\\"lastName\\\"\\n:\\n\\\"Boberson\\\"\\n,\\n\\\"age\\\"\\n:\\n24\\n},\\n]\\n# Map resolver functions to custom type fields using ObjectType\\nperson\\n=\\nObjectType\\n(\\n\\\"Person\\\"\\n)\\n@person\\n.\\nfield\\n(\\n\\\"fullName\\\"\\n)\\ndef\\nresolve_person_fullname\\n(\\nperson\\n,\\n*\\n_\\n):\\nreturn\\n\\\"\\n%s\\n%s\\n\\\"\\n%\\n(\\nperson\\n[\\n\\\"firstName\\\"\\n],\\nperson\\n[\\n\\\"lastName\\\"\\n])\\n# Create executable GraphQL schema\\nschema\\n=\\nmake_executable_schema\\n(\\ntype_defs\\n,\\nquery\\n,\\nperson\\n)\\n# Create an ASGI app using the schema, running in debug mode\\napp\\n=\\nGraphQL\\n(\\nschema\\n,\\ndebug\\n=\\nTrue\\n)\\nFinally run the server:\\nuvicorn example:app\\nFor more guides and examples, please see the\\ndocumentation\\n.\\nVersioning policy\\nariadne\\nfollows a custom versioning scheme where the minor version increases for breaking changes, while the patch version increments for bug fixes, enhancements, and other non-breaking updates.\\nSince\\nariadne\\nhas not yet reached a stable API, this approach is in place until version 1.0.0. Once the API stabilizes, the project will adopt\\nSemantic Versioning\\n.\\nContributing\\nWe are welcoming contributions to Ariadne! If you've found a bug or issue, feel free to use\\nGitHub issues\\n. If you have any questions or feedback, don't hesitate to catch us on\\nGitHub discussions\\n.\\nFor guidance and instructions, please see\\nCONTRIBUTING.md\\n.\\nWebsite and the docs have their own GitHub repository:\\nmirumee/ariadne-website\\nAlso make sure you follow\\n@AriadneGraphQL\\non Twitter for latest updates, news and random musings!\\nCrafted with \\u2764\\ufe0f by\\nMirumee Software\\nhello@mirumee.com\"}, {\"name\": \"pymemcache\", \"description\": \"pymemcache\\n==========\\n\\n.. image:: https://img.shields.io/pypi/v/pymemcache.svg\\n    :target: https://pypi.python.org/pypi/pymemcache\\n\\n.. image:: https://readthedocs.org/projects/pymemcache/badge/?version=master\\n        :target: https://pymemcache.readthedocs.io/en/latest/\\n        :alt: Master Documentation Status\\n\\nA comprehensive, fast, pure-Python memcached client.\\n\\npymemcache supports the following features:\\n\\n* Complete implementation of the memcached text protocol.\\n* Connections using UNIX sockets, or TCP over IPv4 or IPv6.\\n* Configurable timeouts for socket connect and send/recv calls.\\n* Access to the \\\"noreply\\\" flag, which can significantly increase the speed of writes.\\n* Flexible, modular and simple approach to serialization and deserialization.\\n* The (optional) ability to treat network and memcached errors as cache misses.\\n\\nInstalling pymemcache\\n=====================\\n\\nInstall from pip:\\n\\n.. code-block:: bash\\n\\n  pip install pymemcache\\n\\nFor development, clone from github and run the tests:\\n\\n.. code-block:: bash\\n\\n    git clone https://github.com/pinterest/pymemcache.git\\n    cd pymemcache\\n\\nRun the tests (make sure you have a local memcached server running):\\n\\n.. code-block:: bash\\n\\n    tox\\n\\nUsage\\n=====\\n\\nSee the documentation here: https://pymemcache.readthedocs.io/en/latest/\\n\\nDjango\\n------\\n\\nSince version 3.2, Django has included a pymemcache-based cache backend.\\nSee `its documentation \\n<https://docs.djangoproject.com/en/stable/topics/cache/#memcached>`__.\\n\\nOn older Django versions, you can use\\n`django-pymemcache <https://github.com/django-pymemcache/django-pymemcache>`_.\\n\\nComparison with Other Libraries\\n===============================\\n\\npylibmc\\n-------\\n\\nThe pylibmc library is a wrapper around libmemcached, implemented in C. It is\\nfast, implements consistent hashing, the full memcached protocol and timeouts.\\nIt does not provide access to the \\\"noreply\\\" flag. It also isn't pure Python,\\nso using it with libraries like gevent is out of the question, and its\\ndependency on libmemcached poses challenges (e.g., it must be built against\\nthe same version of libmemcached that it will use at runtime).\\n\\npython-memcached\\n----------------\\n\\nThe python-memcached library implements the entire memcached text protocol, has\\na single timeout for all socket calls and has a flexible approach to\\nserialization and deserialization. It is also written entirely in Python, so\\nit works well with libraries like gevent. However, it is tied to using thread\\nlocals, doesn't implement \\\"noreply\\\", can't treat errors as cache misses and is\\nslower than both pylibmc and pymemcache. It is also tied to a specific method\\nfor handling clusters of memcached servers.\\n\\nmemcache_client\\n---------------\\n\\nThe team at mixpanel put together a pure Python memcached client as well. It\\nhas more fine grained support for socket timeouts, only connects to a single\\nhost. However, it doesn't support most of the memcached API (just get, set,\\ndelete and stats), doesn't support \\\"noreply\\\", has no serialization or\\ndeserialization support and can't treat errors as cache misses.\\n\\nExternal Links\\n==============\\n\\nThe memcached text protocol reference page:\\n  https://github.com/memcached/memcached/blob/master/doc/protocol.txt\\n\\nThe python-memcached library (another pure-Python library):\\n  https://github.com/linsomniac/python-memcached\\n\\nMixpanel's Blog post about their memcached client for Python:\\n  https://engineering.mixpanel.com/we-went-down-so-we-wrote-a-better-pure-python-memcache-client-b409a9fe07a9\\n\\nMixpanel's pure Python memcached client:\\n  https://github.com/mixpanel/memcache_client\\n  \\nBye-bye python-memcached, hello pymemcache (migration guide)\\n  https://jugmac00.github.io/blog/bye-bye-python-memcached-hello-pymemcache/\\n\\nCredits\\n=======\\n\\n* `Charles Gordon <http://github.com/cgordon>`_\\n* `Dave Dash <http://github.com/davedash>`_\\n* `Dan Crosta <http://github.com/dcrosta>`_\\n* `Julian Berman <http://github.com/Julian>`_\\n* `Mark Shirley <http://github.com/maspwr>`_\\n* `Tim Bart <http://github.com/pims>`_\\n* `Thomas Orozco <http://github.com/krallin>`_\\n* `Marc Abramowitz <http://github.com/msabramo>`_\\n* `Marc-Andre Courtois <http://github.com/mcourtois>`_\\n* `Julien Danjou <http://github.com/jd>`_\\n* `INADA Naoki <http://github.com/methane>`_\\n* `James Socol <http://github.com/jsocol>`_\\n* `Joshua Harlow <http://github.com/harlowja>`_\\n* `John Anderson <http://github.com/sontek>`_\\n* `Adam Chainz <http://github.com/adamchainz>`_\\n* `Ernest W. Durbin III <https://github.com/ewdurbin>`_\\n* `Remco van Oosterhout <https://github.com/Vhab>`_\\n* `Nicholas Charriere <https://github.com/nichochar>`_\\n* `Joe Gordon <https://github.com/jogo>`_\\n* `Jon Parise <https://github.com/jparise>`_\\n* `Stephen Rosen <https://github.com/sirosen>`_\\n* `Feras Alazzeh <https://github.com/FerasAlazzeh>`_\\n* `Mois\\u00e9s Guimar\\u00e3es de Medeiros <https://github.com/moisesguimaraes>`_\\n* `Nick Pope <https://github.com/ngnpope>`_\\n* `Herv\\u00e9 Beraud <https://github.com/4383>`_\\n* `Martin J\\u00f8rgensen <https://github.com/martinnj>`_\\n\\nWe're Hiring!\\n=============\\nAre you really excited about open-source? Or great software engineering?\\nPinterest is `hiring <https://careers.pinterest.com/>`_!\\n\\nChangelog\\n=========\\nNew in version 4.0.0\\n--------------------\\n* Dropped Python 2 and 3.6 support\\n  `#321 <https://github.com/pinterest/pymemcache/pull/321>`_\\n  `#363 <https://github.com/pinterest/pymemcache/pull/363>`_\\n* Begin adding typing\\n* Add pluggable compression serde\\n  `#407 <https://github.com/pinterest/pymemcache/pull/407>`_\\n\\n\\nNew in version 3.5.2\\n--------------------\\n* Handle blank ``STAT`` values.\\n\\nNew in version 3.5.1\\n--------------------\\n* ``Client.get`` returns the default when using ``ignore_exc`` and if memcached\\n  is unavailable\\n* Added ``noreply`` support to ``HashClient.flush_all``.\\n\\nNew in version 3.5.0\\n--------------------\\n* Sockets are now closed on ``MemcacheUnexpectedCloseError``.\\n* Added support for TCP keepalive for client sockets on Linux platforms.\\n* Added retrying mechanisms by wrapping clients.\\n\\nNew in version 3.4.4\\n--------------------\\n* Idle connections will be removed from the pool after ``pool_idle_timeout``.\\n\\nNew in version 3.4.3\\n--------------------\\n* Fix ``HashClient.{get,set}_many()`` with UNIX sockets.\\n\\nNew in version 3.4.2\\n--------------------\\n* Remove trailing space for commands that don't take arguments, such as\\n  ``stats``. This was a violation of the memcached protocol.\\n\\nNew in version 3.4.1\\n--------------------\\n* CAS operations will now raise ``MemcacheIllegalInputError`` when ``None`` is\\n  given as the ``cas`` value.\\n\\nNew in version 3.4.0\\n--------------------\\n* Added IPv6 support for TCP socket connections. Note that IPv6 may be used in\\n  preference to IPv4 when passing a domain name as the host if an IPv6 address\\n  can be resolved for that domain.\\n* ``HashClient`` now supports UNIX sockets.\\n\\nNew in version 3.3.0\\n--------------------\\n* ``HashClient`` can now be imported from the top-level ``pymemcache`` package\\n  (e.g. ``pymemcache.HashClient``).\\n* ``HashClient.get_many()`` now longer stores ``False`` for missing keys from\\n  unavailable clients. Instead, the result won't contain the key at all.\\n* Added missing ``HashClient.close()`` and ``HashClient.quit()``.\\n\\nNew in version 3.2.0\\n--------------------\\n* ``PooledClient`` and ``HashClient`` now support custom ``Client`` classes\\n\\nNew in version 3.1.1\\n--------------------\\n* Improve ``MockMemcacheClient`` to behave even more like ``Client``\\n\\nNew in version 3.1.0\\n--------------------\\n* Add TLS support for TCP sockets.\\n* Fix corner case when dead hashed server comes back alive.\\n\\nNew in version 3.0.1\\n--------------------\\n* Make MockMemcacheClient more consistent with the real client.\\n* Pass ``encoding`` from HashClient to its pooled clients when ``use_pooling``\\n  is enabled.\\n\\nNew in version 3.0.0\\n--------------------\\n* The serialization API has been reworked. Instead of consuming a serializer\\n  and deserializer as separate arguments, client objects now expect an argument\\n  ``serde`` to be an object which implements ``serialize`` and ``deserialize``\\n  as methods. (``serialize`` and ``deserialize`` are still supported but\\n  considered deprecated.)\\n* Validate integer inputs for ``expire``, ``delay``, ``incr``, ``decr``, and\\n  ``memlimit`` -- non-integer values now raise ``MemcacheIllegalInputError``\\n* Validate inputs for ``cas`` -- values which are not integers or strings of\\n  0-9 now raise ``MemcacheIllegalInputError``\\n* Add ``prepend`` and ``append`` support to ``MockMemcacheClient``.\\n* Add the ``touch`` method to ``HashClient``.\\n* Added official support for Python 3.8.\\n\\nNew in version 2.2.2\\n--------------------\\n* Fix ``long_description`` string in Python packaging.\\n\\nNew in version 2.2.1\\n--------------------\\n* Fix ``flags`` when setting multiple differently-typed values at once.\\n\\nNew in version 2.2.0\\n--------------------\\n* Drop official support for Python 3.4.\\n* Use ``setup.cfg`` metadata instead ``setup.py`` config to generate package.\\n* Add ``default_noreply`` parameter to ``HashClient``.\\n* Add ``encoding`` parameter to ``Client`` constructors (defaults to ``ascii``).\\n* Add ``flags`` parameter to write operation methods.\\n* Handle unicode key values in ``MockMemcacheClient`` correctly.\\n* Improve ASCII encoding failure exception.\\n\\nNew in version 2.1.1\\n--------------------\\n* Fix ``setup.py`` dependency on six already being installed.\\n\\nNew in version 2.1.0\\n--------------------\\n* Public classes and exceptions can now be imported from the top-level\\n  ``pymemcache`` package (e.g. ``pymemcache.Client``).\\n  `#197 <https://github.com/pinterest/pymemcache/pull/197>`_\\n* Add UNIX domain socket support and document server connection options.\\n  `#206 <https://github.com/pinterest/pymemcache/pull/206>`_\\n* Add support for the ``cache_memlimit`` command.\\n  `#211 <https://github.com/pinterest/pymemcache/pull/211>`_\\n* Commands key are now always sent in their original order.\\n  `#209 <https://github.com/pinterest/pymemcache/pull/209>`_\\n  \\nNew in version 2.0.0\\n--------------------\\n* Change set_many and set_multi api return value. `#179 <https://github.com/pinterest/pymemcache/pull/179>`_\\n* Fix support for newbytes from python-future. `#187 <https://github.com/pinterest/pymemcache/pull/187>`_\\n* Add support for Python 3.7, and drop support for Python 3.3\\n* Properly batch Client.set_many() call. `#182 <https://github.com/pinterest/pymemcache/pull/182>`_\\n* Improve _check_key() and _store_cmd() performance. `#183 <https://github.com/pinterest/pymemcache/pull/183>`_\\n* Properly batch Client.delete_many() call. `#184 <https://github.com/pinterest/pymemcache/pull/184>`_\\n* Add option to explicitly set pickle version used by serde. `#190 <https://github.com/pinterest/pymemcache/pull/190>`_\\n\\nNew in version 1.4.4\\n--------------------\\n* pypy3 to travis test matrix\\n* full benchmarks in test\\n* fix flake8 issues\\n* Have mockmemcacheclient support non-ascii strings\\n* Switch from using pickle format 0 to the highest available version. See `#156 <https://github.com/pinterest/pymemcache/pull/156>`_\\n\\n  *Warning*: different versions of python have different highest pickle versions: https://docs.python.org/3/library/pickle.html\\n\\n\\nNew in version 1.4.3\\n--------------------\\n* Documentation improvements\\n* Fixed cachedump stats command, see `#103 <https://github.com/pinterest/pymemcache/issues/103>`_\\n* Honor default_value in HashClient\\n\\nNew in version 1.4.2\\n--------------------\\n* Drop support for python 2.6, see `#109 <https://github.com/pinterest/pymemcache/issues/139>`_\\n\\nNew in version 1.4.1\\n--------------------\\n* Python 3 serializations fixes `#131 <https://github.com/pinterest/pymemcache/pull/131>`_\\n* Drop support for pypy3\\n* Comment cleanup\\n* Add gets_many to hash_client\\n* Better checking for illegal chars in key\\n\\nNew in version 1.4.0\\n--------------------\\n* Unicode keys support. It is now possible to pass the flag ``allow_unicode_keys`` when creating the clients, thanks @jogo!\\n* Fixed a bug where PooledClient wasn't following ``default_noreply`` arg set on init, thanks @kols!\\n* Improved documentation\\n\\nNew in version 1.3.8\\n--------------------\\n* use cpickle instead of pickle when possible (python2)\\n\\nNew in version 1.3.7\\n--------------------\\n* default parameter on get(key, default=0)\\n* fixed docs to autogenerate themselves with sphinx\\n* fix linter to work with python3\\n* improve error message on illegal Input for the key\\n* refactor stat parsing\\n* fix MockMemcacheClient\\n* fix unicode char in middle of key bug\\n\\nNew in version 1.3.6\\n--------------------\\n* Fix flake8 and cleanup tox building\\n* Fix security vulnerability by sanitizing key input\\n\\nNew in version 1.3.5\\n--------------------\\n* Bug fix for HashClient when retries is set to zero.\\n* Adding the VERSION command to the clients.\\n\\nNew in version 1.3.4\\n--------------------\\n* Bug fix for the HashClient that corrects behavior when there are no working servers.\\n\\nNew in version 1.3.3\\n--------------------\\n* Adding caching to the Travis build.\\n* A bug fix for pluggable hashing in HashClient.\\n* Adding a default_noreply argument to the Client ctor.\\n\\nNew in version 1.3.2\\n--------------------\\n* Making the location of Memcache Exceptions backwards compatible.\\n\\nNew in version 1.3.0\\n--------------------\\n* Python 3 Support\\n* Introduced HashClient that uses consistent hasing for allocating keys across many memcached nodes. It also can detect servers going down and rebalance keys across the available nodes.\\n* Retry sock.recv() when it raises EINTR\\n\\nNew in version 1.2.9\\n--------------------\\n* Introduced PooledClient a thread-safe pool of clients\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"A comprehensive, fast, pure Python memcached client\", \"latest_version\": \"4.0.0\", \"weekly_downloads\": 812517, \"description_cleaned\": \"pymemcache\\nA comprehensive, fast, pure-Python memcached client.\\npymemcache supports the following features:\\nComplete implementation of the memcached text protocol.\\nConnections using UNIX sockets, or TCP over IPv4 or IPv6.\\nConfigurable timeouts for socket connect and send/recv calls.\\nAccess to the \\u201cnoreply\\u201d flag, which can significantly increase the speed of writes.\\nFlexible, modular and simple approach to serialization and deserialization.\\nThe (optional) ability to treat network and memcached errors as cache misses.\\nInstalling pymemcache\\nInstall from pip:\\npip\\ninstall\\npymemcache\\nFor development, clone from github and run the tests:\\ngit\\nclone\\nhttps://github.com/pinterest/pymemcache.git\\ncd\\npymemcache\\nRun the tests (make sure you have a local memcached server running):\\ntox\\nUsage\\nSee the documentation here:\\nhttps://pymemcache.readthedocs.io/en/latest/\\nDjango\\nSince version 3.2, Django has included a pymemcache-based cache backend.\\nSee\\nits documentation\\n.\\nOn older Django versions, you can use\\ndjango-pymemcache\\n.\\nComparison with Other Libraries\\npylibmc\\nThe pylibmc library is a wrapper around libmemcached, implemented in C. It is\\nfast, implements consistent hashing, the full memcached protocol and timeouts.\\nIt does not provide access to the \\u201cnoreply\\u201d flag. It also isn\\u2019t pure Python,\\nso using it with libraries like gevent is out of the question, and its\\ndependency on libmemcached poses challenges (e.g., it must be built against\\nthe same version of libmemcached that it will use at runtime).\\npython-memcached\\nThe python-memcached library implements the entire memcached text protocol, has\\na single timeout for all socket calls and has a flexible approach to\\nserialization and deserialization. It is also written entirely in Python, so\\nit works well with libraries like gevent. However, it is tied to using thread\\nlocals, doesn\\u2019t implement \\u201cnoreply\\u201d, can\\u2019t treat errors as cache misses and is\\nslower than both pylibmc and pymemcache. It is also tied to a specific method\\nfor handling clusters of memcached servers.\\nmemcache_client\\nThe team at mixpanel put together a pure Python memcached client as well. It\\nhas more fine grained support for socket timeouts, only connects to a single\\nhost. However, it doesn\\u2019t support most of the memcached API (just get, set,\\ndelete and stats), doesn\\u2019t support \\u201cnoreply\\u201d, has no serialization or\\ndeserialization support and can\\u2019t treat errors as cache misses.\\nExternal Links\\nThe memcached text protocol reference page:\\nhttps://github.com/memcached/memcached/blob/master/doc/protocol.txt\\nThe python-memcached library (another pure-Python library):\\nhttps://github.com/linsomniac/python-memcached\\nMixpanel\\u2019s Blog post about their memcached client for Python:\\nhttps://engineering.mixpanel.com/we-went-down-so-we-wrote-a-better-pure-python-memcache-client-b409a9fe07a9\\nMixpanel\\u2019s pure Python memcached client:\\nhttps://github.com/mixpanel/memcache_client\\nBye-bye python-memcached, hello pymemcache (migration guide)\\nhttps://jugmac00.github.io/blog/bye-bye-python-memcached-hello-pymemcache/\\nCredits\\nCharles Gordon\\nDave Dash\\nDan Crosta\\nJulian Berman\\nMark Shirley\\nTim Bart\\nThomas Orozco\\nMarc Abramowitz\\nMarc-Andre Courtois\\nJulien Danjou\\nINADA Naoki\\nJames Socol\\nJoshua Harlow\\nJohn Anderson\\nAdam Chainz\\nErnest W. Durbin III\\nRemco van Oosterhout\\nNicholas Charriere\\nJoe Gordon\\nJon Parise\\nStephen Rosen\\nFeras Alazzeh\\nMois\\u00e9s Guimar\\u00e3es de Medeiros\\nNick Pope\\nHerv\\u00e9 Beraud\\nMartin J\\u00f8rgensen\\nWe\\u2019re Hiring!\\nAre you really excited about open-source? Or great software engineering?\\nPinterest is\\nhiring\\n!\\nChangelog\\nNew in version 4.0.0\\nDropped Python 2 and 3.6 support\\n#321\\n#363\\nBegin adding typing\\nAdd pluggable compression serde\\n#407\\nNew in version 3.5.2\\nHandle blank\\nSTAT\\nvalues.\\nNew in version 3.5.1\\nClient.get\\nreturns the default when using\\nignore_exc\\nand if memcached\\nis unavailable\\nAdded\\nnoreply\\nsupport to\\nHashClient.flush_all\\n.\\nNew in version 3.5.0\\nSockets are now closed on\\nMemcacheUnexpectedCloseError\\n.\\nAdded support for TCP keepalive for client sockets on Linux platforms.\\nAdded retrying mechanisms by wrapping clients.\\nNew in version 3.4.4\\nIdle connections will be removed from the pool after\\npool_idle_timeout\\n.\\nNew in version 3.4.3\\nFix\\nHashClient.{get,set}_many()\\nwith UNIX sockets.\\nNew in version 3.4.2\\nRemove trailing space for commands that don\\u2019t take arguments, such as\\nstats\\n. This was a violation of the memcached protocol.\\nNew in version 3.4.1\\nCAS operations will now raise\\nMemcacheIllegalInputError\\nwhen\\nNone\\nis\\ngiven as the\\ncas\\nvalue.\\nNew in version 3.4.0\\nAdded IPv6 support for TCP socket connections. Note that IPv6 may be used in\\npreference to IPv4 when passing a domain name as the host if an IPv6 address\\ncan be resolved for that domain.\\nHashClient\\nnow supports UNIX sockets.\\nNew in version 3.3.0\\nHashClient\\ncan now be imported from the top-level\\npymemcache\\npackage\\n(e.g.\\npymemcache.HashClient\\n).\\nHashClient.get_many()\\nnow longer stores\\nFalse\\nfor missing keys from\\nunavailable clients. Instead, the result won\\u2019t contain the key at all.\\nAdded missing\\nHashClient.close()\\nand\\nHashClient.quit()\\n.\\nNew in version 3.2.0\\nPooledClient\\nand\\nHashClient\\nnow support custom\\nClient\\nclasses\\nNew in version 3.1.1\\nImprove\\nMockMemcacheClient\\nto behave even more like\\nClient\\nNew in version 3.1.0\\nAdd TLS support for TCP sockets.\\nFix corner case when dead hashed server comes back alive.\\nNew in version 3.0.1\\nMake MockMemcacheClient more consistent with the real client.\\nPass\\nencoding\\nfrom HashClient to its pooled clients when\\nuse_pooling\\nis enabled.\\nNew in version 3.0.0\\nThe serialization API has been reworked. Instead of consuming a serializer\\nand deserializer as separate arguments, client objects now expect an argument\\nserde\\nto be an object which implements\\nserialize\\nand\\ndeserialize\\nas methods. (\\nserialize\\nand\\ndeserialize\\nare still supported but\\nconsidered deprecated.)\\nValidate integer inputs for\\nexpire\\n,\\ndelay\\n,\\nincr\\n,\\ndecr\\n, and\\nmemlimit\\n\\u2013 non-integer values now raise\\nMemcacheIllegalInputError\\nValidate inputs for\\ncas\\n\\u2013 values which are not integers or strings of\\n0-9 now raise\\nMemcacheIllegalInputError\\nAdd\\nprepend\\nand\\nappend\\nsupport to\\nMockMemcacheClient\\n.\\nAdd the\\ntouch\\nmethod to\\nHashClient\\n.\\nAdded official support for Python 3.8.\\nNew in version 2.2.2\\nFix\\nlong_description\\nstring in Python packaging.\\nNew in version 2.2.1\\nFix\\nflags\\nwhen setting multiple differently-typed values at once.\\nNew in version 2.2.0\\nDrop official support for Python 3.4.\\nUse\\nsetup.cfg\\nmetadata instead\\nsetup.py\\nconfig to generate package.\\nAdd\\ndefault_noreply\\nparameter to\\nHashClient\\n.\\nAdd\\nencoding\\nparameter to\\nClient\\nconstructors (defaults to\\nascii\\n).\\nAdd\\nflags\\nparameter to write operation methods.\\nHandle unicode key values in\\nMockMemcacheClient\\ncorrectly.\\nImprove ASCII encoding failure exception.\\nNew in version 2.1.1\\nFix\\nsetup.py\\ndependency on six already being installed.\\nNew in version 2.1.0\\nPublic classes and exceptions can now be imported from the top-level\\npymemcache\\npackage (e.g.\\npymemcache.Client\\n).\\n#197\\nAdd UNIX domain socket support and document server connection options.\\n#206\\nAdd support for the\\ncache_memlimit\\ncommand.\\n#211\\nCommands key are now always sent in their original order.\\n#209\\nNew in version 2.0.0\\nChange set_many and set_multi api return value.\\n#179\\nFix support for newbytes from python-future.\\n#187\\nAdd support for Python 3.7, and drop support for Python 3.3\\nProperly batch Client.set_many() call.\\n#182\\nImprove _check_key() and _store_cmd() performance.\\n#183\\nProperly batch Client.delete_many() call.\\n#184\\nAdd option to explicitly set pickle version used by serde.\\n#190\\nNew in version 1.4.4\\npypy3 to travis test matrix\\nfull benchmarks in test\\nfix flake8 issues\\nHave mockmemcacheclient support non-ascii strings\\nSwitch from using pickle format 0 to the highest available version. See\\n#156\\nWarning\\n: different versions of python have different highest pickle versions:\\nhttps://docs.python.org/3/library/pickle.html\\nNew in version 1.4.3\\nDocumentation improvements\\nFixed cachedump stats command, see\\n#103\\nHonor default_value in HashClient\\nNew in version 1.4.2\\nDrop support for python 2.6, see\\n#109\\nNew in version 1.4.1\\nPython 3 serializations fixes\\n#131\\nDrop support for pypy3\\nComment cleanup\\nAdd gets_many to hash_client\\nBetter checking for illegal chars in key\\nNew in version 1.4.0\\nUnicode keys support. It is now possible to pass the flag\\nallow_unicode_keys\\nwhen creating the clients, thanks @jogo!\\nFixed a bug where PooledClient wasn\\u2019t following\\ndefault_noreply\\narg set on init, thanks @kols!\\nImproved documentation\\nNew in version 1.3.8\\nuse cpickle instead of pickle when possible (python2)\\nNew in version 1.3.7\\ndefault parameter on get(key, default=0)\\nfixed docs to autogenerate themselves with sphinx\\nfix linter to work with python3\\nimprove error message on illegal Input for the key\\nrefactor stat parsing\\nfix MockMemcacheClient\\nfix unicode char in middle of key bug\\nNew in version 1.3.6\\nFix flake8 and cleanup tox building\\nFix security vulnerability by sanitizing key input\\nNew in version 1.3.5\\nBug fix for HashClient when retries is set to zero.\\nAdding the VERSION command to the clients.\\nNew in version 1.3.4\\nBug fix for the HashClient that corrects behavior when there are no working servers.\\nNew in version 1.3.3\\nAdding caching to the Travis build.\\nA bug fix for pluggable hashing in HashClient.\\nAdding a default_noreply argument to the Client ctor.\\nNew in version 1.3.2\\nMaking the location of Memcache Exceptions backwards compatible.\\nNew in version 1.3.0\\nPython 3 Support\\nIntroduced HashClient that uses consistent hasing for allocating keys across many memcached nodes. It also can detect servers going down and rebalance keys across the available nodes.\\nRetry sock.recv() when it raises EINTR\\nNew in version 1.2.9\\nIntroduced PooledClient a thread-safe pool of clients\"}, {\"name\": \"strawberry-graphql\", \"description\": \"<img src=\\\"https://github.com/strawberry-graphql/strawberry/raw/main/.github/logo.png\\\" width=\\\"124\\\" height=\\\"150\\\">\\n\\n# Strawberry GraphQL\\n\\n> Python GraphQL library based on dataclasses\\n\\n[![CircleCI](https://img.shields.io/circleci/token/307b40d5e152e074d34f84d30d226376a15667d5/project/github/strawberry-graphql/strawberry/main.svg?style=for-the-badge)](https://circleci.com/gh/strawberry-graphql/strawberry/tree/main)\\n[![Discord](https://img.shields.io/discord/689806334337482765?label=discord&logo=discord&logoColor=white&style=for-the-badge&color=blue)](https://discord.gg/ZkRTEJQ)\\n[![PyPI](https://img.shields.io/pypi/v/strawberry-graphql?logo=pypi&logoColor=white&style=for-the-badge)](https://pypi.org/project/strawberry-graphql/)\\n\\n## Installation ( Quick Start )\\n\\nThe quick start method provides a server and CLI to get going quickly. Install\\nwith:\\n\\n```shell\\npip install \\\"strawberry-graphql[cli]\\\"\\n```\\n\\n## Getting Started\\n\\nCreate a file called `app.py` with the following code:\\n\\n```python\\nimport strawberry\\n\\n\\n@strawberry.type\\nclass User:\\n    name: str\\n    age: int\\n\\n\\n@strawberry.type\\nclass Query:\\n    @strawberry.field\\n    def user(self) -> User:\\n        return User(name=\\\"Patrick\\\", age=100)\\n\\n\\nschema = strawberry.Schema(query=Query)\\n```\\n\\nThis will create a GraphQL schema defining a `User` type and a single query\\nfield `user` that will return a hardcoded user.\\n\\nTo serve the schema using the dev server run the following command:\\n\\n```shell\\nstrawberry dev app\\n```\\n\\nOpen the dev server by clicking on the following link:\\n[http://0.0.0.0:8000/graphql](http://0.0.0.0:8000/graphql)\\n\\nThis will open GraphiQL where you can test the API.\\n\\n### Type-checking\\n\\nStrawberry comes with a [mypy] plugin that enables statically type-checking your\\nGraphQL schema. To enable it, add the following lines to your `mypy.ini`\\nconfiguration:\\n\\n```ini\\n[mypy]\\nplugins = strawberry.ext.mypy_plugin\\n```\\n\\n[mypy]: http://www.mypy-lang.org/\\n\\n### Django Integration\\n\\nA Django view is provided for adding a GraphQL endpoint to your application.\\n\\n1. Add the app to your `INSTALLED_APPS`.\\n\\n```python\\nINSTALLED_APPS = [\\n    ...,  # your other apps\\n    \\\"strawberry.django\\\",\\n]\\n```\\n\\n2. Add the view to your `urls.py` file.\\n\\n```python\\nfrom strawberry.django.views import GraphQLView\\nfrom .schema import schema\\n\\nurlpatterns = [\\n    ...,\\n    path(\\\"graphql\\\", GraphQLView.as_view(schema=schema)),\\n]\\n```\\n\\n## Examples\\n\\n* [Various examples on how to use Strawberry](https://github.com/strawberry-graphql/examples)\\n* [Full stack example using Starlette, SQLAlchemy, Typescript codegen and Next.js](https://github.com/jokull/python-ts-graphql-demo)\\n* [Quart + Strawberry tutorial](https://github.com/rockyburt/Ketchup)\\n\\n## Contributing\\n\\nWe use [poetry](https://github.com/sdispater/poetry) to manage dependencies, to\\nget started follow these steps:\\n\\n```shell\\ngit clone https://github.com/strawberry-graphql/strawberry\\ncd strawberry\\npoetry install\\npoetry run pytest\\n```\\n\\nFor all further detail, check out the [Contributing Page](CONTRIBUTING.md)\\n\\n\\n### Pre commit\\n\\nWe have a configuration for\\n[pre-commit](https://github.com/pre-commit/pre-commit), to add the hook run the\\nfollowing command:\\n\\n```shell\\npre-commit install\\n```\\n\\n## Links\\n\\n- Project homepage: https://strawberry.rocks\\n- Repository: https://github.com/strawberry-graphql/strawberry\\n- Issue tracker: https://github.com/strawberry-graphql/strawberry/issues\\n  - In case of sensitive bugs like security vulnerabilities, please contact\\n    patrick.arminio@gmail.com directly instead of using the issue tracker. We\\n    value your effort to improve the security and privacy of this project!\\n\\n## Licensing\\n\\nThe code in this project is licensed under MIT license. See [LICENSE](./LICENSE)\\nfor more information.\\n\\n![Recent Activity](https://images.repography.com/0/strawberry-graphql/strawberry/recent-activity/d751713988987e9331980363e24189ce.svg)\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"A library for creating GraphQL APIs\", \"latest_version\": \"0.285.0\", \"weekly_downloads\": 811954, \"description_cleaned\": \"Strawberry GraphQL\\nPython GraphQL library based on dataclasses\\nInstallation ( Quick Start )\\nThe quick start method provides a server and CLI to get going quickly. Install\\nwith:\\npip\\ninstall\\n\\\"strawberry-graphql[cli]\\\"\\nGetting Started\\nCreate a file called\\napp.py\\nwith the following code:\\nimport\\nstrawberry\\n@strawberry\\n.\\ntype\\nclass\\nUser\\n:\\nname\\n:\\nstr\\nage\\n:\\nint\\n@strawberry\\n.\\ntype\\nclass\\nQuery\\n:\\n@strawberry\\n.\\nfield\\ndef\\nuser\\n(\\nself\\n)\\n->\\nUser\\n:\\nreturn\\nUser\\n(\\nname\\n=\\n\\\"Patrick\\\"\\n,\\nage\\n=\\n100\\n)\\nschema\\n=\\nstrawberry\\n.\\nSchema\\n(\\nquery\\n=\\nQuery\\n)\\nThis will create a GraphQL schema defining a\\nUser\\ntype and a single query\\nfield\\nuser\\nthat will return a hardcoded user.\\nTo serve the schema using the dev server run the following command:\\nstrawberry\\ndev\\napp\\nOpen the dev server by clicking on the following link:\\nhttp://0.0.0.0:8000/graphql\\nThis will open GraphiQL where you can test the API.\\nType-checking\\nStrawberry comes with a\\nmypy\\nplugin that enables statically type-checking your\\nGraphQL schema. To enable it, add the following lines to your\\nmypy.ini\\nconfiguration:\\n[mypy]\\nplugins\\n=\\nstrawberry.ext.mypy_plugin\\nDjango Integration\\nA Django view is provided for adding a GraphQL endpoint to your application.\\nAdd the app to your\\nINSTALLED_APPS\\n.\\nINSTALLED_APPS\\n=\\n[\\n...\\n,\\n# your other apps\\n\\\"strawberry.django\\\"\\n,\\n]\\nAdd the view to your\\nurls.py\\nfile.\\nfrom\\nstrawberry.django.views\\nimport\\nGraphQLView\\nfrom\\n.schema\\nimport\\nschema\\nurlpatterns\\n=\\n[\\n...\\n,\\npath\\n(\\n\\\"graphql\\\"\\n,\\nGraphQLView\\n.\\nas_view\\n(\\nschema\\n=\\nschema\\n)),\\n]\\nExamples\\nVarious examples on how to use Strawberry\\nFull stack example using Starlette, SQLAlchemy, Typescript codegen and Next.js\\nQuart + Strawberry tutorial\\nContributing\\nWe use\\npoetry\\nto manage dependencies, to\\nget started follow these steps:\\ngit\\nclone\\nhttps://github.com/strawberry-graphql/strawberry\\ncd\\nstrawberry\\npoetry\\ninstall\\npoetry\\nrun\\npytest\\nFor all further detail, check out the\\nContributing Page\\nPre commit\\nWe have a configuration for\\npre-commit\\n, to add the hook run the\\nfollowing command:\\npre-commit\\ninstall\\nLinks\\nProject homepage:\\nhttps://strawberry.rocks\\nRepository:\\nhttps://github.com/strawberry-graphql/strawberry\\nIssue tracker:\\nhttps://github.com/strawberry-graphql/strawberry/issues\\nIn case of sensitive bugs like security vulnerabilities, please contact\\npatrick.arminio@gmail.com\\ndirectly instead of using the issue tracker. We\\nvalue your effort to improve the security and privacy of this project!\\nLicensing\\nThe code in this project is licensed under MIT license. See\\nLICENSE\\nfor more information.\"}, {\"name\": \"groovy\", \"description\": \"<p align=\\\"center\\\">\\n    <a href=\\\"https://pypi.org/project/groovy/\\\"><img alt=\\\"PyPI\\\" src=\\\"https://img.shields.io/pypi/v/groovy\\\"></a>\\n    <img alt=\\\"Python version\\\" src=\\\"https://img.shields.io/badge/python-3.10+-success\\\">\\n    <a href=\\\"https://github.com/abidlabs/groovy/actions/workflows/format.yml\\\"><img alt=\\\"Format\\\" src=\\\"https://github.com/abidlabs/groovy/actions/workflows/format.yml/badge.svg\\\"></a>\\n    <a href=\\\"https://github.com/abidlabs/groovy/actions/workflows/test.yml\\\"><img alt=\\\"Test\\\" src=\\\"https://github.com/abidlabs/groovy/actions/workflows/test.yml/badge.svg\\\"></a>\\n</p>\\n\\n\\n<p align=\\\"center\\\">\\n<img src=\\\"https://github.com/user-attachments/assets/49d40a48-3c73-4911-8033-250cfa7aafd4\\\" width=400>\\n</p>\\n\\nGroovy is a Python-to-JavaScript transpiler, meaning that it converts Python functions to their JavaScript equivalents. It is used in the [Gradio](https://gradio.app) library, so that developers can write functions in Python and have them run as fast as client-side JavaScript \\u26a1. Instead of aiming for full coverage of Python features, groovy prioritizes **clear error reporting** for unsupported Python code, making it easier for developers to modify their functions accordingly.\\n\\n### \\ud83d\\ude80 Features\\n- Converts simple Python functions into JavaScript equivalents.\\n- Supports a subset of the Python standard library along with some Gradio-specific classes.\\n- Provides **complete error reporting** when a function can't be transpiled (either due to no equivalent in JavaScript or ambiguity).\\n\\n### \\ud83d\\udce6 Installation\\nInstall groovy via pip:\\n```bash\\npip install groovy\\n```\\n\\n### \\ud83d\\udd27 Usage\\n```python\\nfrom groovy import transpile\\n\\ndef sum_range(n: int):\\n    total = 0\\n    for i in range(n):\\n        total = total + i\\n    return total\\n\\njs_code = transpile(sum)\\nprint(js_code)\\n```\\nproduces:\\n\\n```\\nfunction sum_range(n) {\\n    let total = 0;\\n    for (let i of Array.from({length: n}, (_, i) => i)) {\\n        total = (total + i);\\n    }\\n    return total;\\n```\\n\\nNote that the JavaScript function is not necessarily minimized or optimized (yet) but it should return exactly the same value when called with the same arguments. While the internal implementation of the transpiled JavaScript function may differ slightly from the Python version (e.g. Python tuples may get converted to JS arrays), groovy tries to guarantee that the return values will be identical for the same inputs.\\n\\nIf groovy encounters unsupported syntax, it will **complain clearly** (throw a `TranspilationError` with all of the issues along with line numbers and the code that caused the issue, making it easy for developers to fix their code.\\n\\n\\n```python\\ndef example_function(x, y):\\n    z = x + y\\n    if z > 10:\\n        print(z)\\n    else:\\n        print(0)\\n\\ntranspile(example_function)\\n```\\n\\nraises:\\n\\n```\\nTranspilerError: 2 issues found:\\n\\n* Line 4: Unsupported function \\\"print()\\\"\\n>> print(z)\\n* Line 6: Unsupported function \\\"print()\\\"\\n>> print(0)\\n```\\n\\n### \\ud83e\\udd14 Ambiguity\\ngroovy takes a conservative approach when encountering ambiguous types. Instead of making assumptions about types, it will raise a `TranspilationError`. For example, a simple sum function without type hints will fail:\\n\\n```python\\ndef sum(a, b):\\n    return a + b\\n\\ntranspile(sum)  # Raises TranspilationError: Cannot determine types for parameters 'a' and 'b'\\n```\\n\\nThis is because `+` could mean different things in JavaScript depending on the types (string concatenation vs numeric addition vs. custom behavior for a custom class). Adding type hints (which is **strongly recommended** for all usage) resolves the ambiguity:\\n\\n```python\\ndef sum(a: int, b: int):\\n    return a + b\\n\\ntranspile(sum)  # Works! Produces: function sum(a, b) { return (a + b); }\\n```\\n\\n### \\ud83d\\udd25 Supported Syntax\\n\\ngroovy supports the following Python syntax and built-in functions. Other functions will not be transpiled.\\n\\n**General**\\n\\n| Python Syntax | JavaScript Equivalent |\\n|--------------|----------------------|\\n| `def function(x: int)` | `function function(x)` |\\n| `x + y`, `x - y`, `x * y`, `x / y` | `(x + y)`, `(x - y)`, `(x * y)`, `(x / y)` |\\n| `x > y`, `x < y`, `x >= y`, `x <= y` | `(x > y)`, `(x < y)`, `(x >= y)`, `(x <= y)` |\\n| `x == y`, `x != y` | `(x === y)`, `(x !== y)` |\\n| `x in list`, `x not in list` | `list.includes(x)`, `!list.includes(x)` |\\n| `x = value` | `let x = value` |\\n| `if condition: ... elif: ... else: ...` | `if (condition) { ... } else if { ... } else { ... }` |\\n| `for x in range(n)` | `for (let x of Array.from({length: n}, (_, i) => i))` |\\n| `for x in list` | `for (let x of list)` |\\n| `while condition` | `while (condition)` |\\n| `[x for x in list if condition]` | `list.filter(x => condition).map(x => x)` |\\n| `len(list)` | `list.length` |\\n| `len(dict)` | `Object.keys(dict).length` |\\n| `[1, 2, 3]` | `[1, 2, 3]` |\\n| `(1, 2, 3)` | `[1, 2, 3]` |\\n| `{\\\"key\\\": \\\"value\\\"}` | `{\\\"key\\\": \\\"value\\\"}` |\\n| `x and y`, `x or y` | `(x && y)`, `(x || y)` |\\n| `None` | `null` |\\n\\n\\n**Gradio-specific syntax**\\n\\n| Python Syntax | JavaScript Equivalent |\\n|--------------|----------------------|\\n| `gr.Component(param=value)` | `{\\\"param\\\": \\\"value\\\", \\\"__type__\\\": \\\"update\\\"}` |\\n\\n\\n### \\ud83d\\udcdc License\\ngroovy is open-source under the [MIT License](https://github.com/abidlabs/groovy/blob/main/LICENSE).\\n\\n---\\nContributions to increase coverage of the Python library that groovy can transpile are welcome! We welcome AI-generated PRs if the rationale is clear to follow, PRs are not too large in scope, and tests are included.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"A small Python library created to help developers protect their applications from Server Side Request Forgery (SSRF) attacks.\", \"latest_version\": \"0.1.2\", \"weekly_downloads\": 810394, \"description_cleaned\": \"Groovy is a Python-to-JavaScript transpiler, meaning that it converts Python functions to their JavaScript equivalents. It is used in the\\nGradio\\nlibrary, so that developers can write functions in Python and have them run as fast as client-side JavaScript \\u26a1. Instead of aiming for full coverage of Python features, groovy prioritizes\\nclear error reporting\\nfor unsupported Python code, making it easier for developers to modify their functions accordingly.\\n\\ud83d\\ude80 Features\\nConverts simple Python functions into JavaScript equivalents.\\nSupports a subset of the Python standard library along with some Gradio-specific classes.\\nProvides\\ncomplete error reporting\\nwhen a function can't be transpiled (either due to no equivalent in JavaScript or ambiguity).\\n\\ud83d\\udce6 Installation\\nInstall groovy via pip:\\npip\\ninstall\\ngroovy\\n\\ud83d\\udd27 Usage\\nfrom\\ngroovy\\nimport\\ntranspile\\ndef\\nsum_range\\n(\\nn\\n:\\nint\\n):\\ntotal\\n=\\n0\\nfor\\ni\\nin\\nrange\\n(\\nn\\n):\\ntotal\\n=\\ntotal\\n+\\ni\\nreturn\\ntotal\\njs_code\\n=\\ntranspile\\n(\\nsum\\n)\\nprint\\n(\\njs_code\\n)\\nproduces:\\nfunction sum_range(n) {\\n    let total = 0;\\n    for (let i of Array.from({length: n}, (_, i) => i)) {\\n        total = (total + i);\\n    }\\n    return total;\\nNote that the JavaScript function is not necessarily minimized or optimized (yet) but it should return exactly the same value when called with the same arguments. While the internal implementation of the transpiled JavaScript function may differ slightly from the Python version (e.g. Python tuples may get converted to JS arrays), groovy tries to guarantee that the return values will be identical for the same inputs.\\nIf groovy encounters unsupported syntax, it will\\ncomplain clearly\\n(throw a\\nTranspilationError\\nwith all of the issues along with line numbers and the code that caused the issue, making it easy for developers to fix their code.\\ndef\\nexample_function\\n(\\nx\\n,\\ny\\n):\\nz\\n=\\nx\\n+\\ny\\nif\\nz\\n>\\n10\\n:\\nprint\\n(\\nz\\n)\\nelse\\n:\\nprint\\n(\\n0\\n)\\ntranspile\\n(\\nexample_function\\n)\\nraises:\\nTranspilerError: 2 issues found:\\n\\n* Line 4: Unsupported function \\\"print()\\\"\\n>> print(z)\\n* Line 6: Unsupported function \\\"print()\\\"\\n>> print(0)\\n\\ud83e\\udd14 Ambiguity\\ngroovy takes a conservative approach when encountering ambiguous types. Instead of making assumptions about types, it will raise a\\nTranspilationError\\n. For example, a simple sum function without type hints will fail:\\ndef\\nsum\\n(\\na\\n,\\nb\\n):\\nreturn\\na\\n+\\nb\\ntranspile\\n(\\nsum\\n)\\n# Raises TranspilationError: Cannot determine types for parameters 'a' and 'b'\\nThis is because\\n+\\ncould mean different things in JavaScript depending on the types (string concatenation vs numeric addition vs. custom behavior for a custom class). Adding type hints (which is\\nstrongly recommended\\nfor all usage) resolves the ambiguity:\\ndef\\nsum\\n(\\na\\n:\\nint\\n,\\nb\\n:\\nint\\n):\\nreturn\\na\\n+\\nb\\ntranspile\\n(\\nsum\\n)\\n# Works! Produces: function sum(a, b) { return (a + b); }\\n\\ud83d\\udd25 Supported Syntax\\ngroovy supports the following Python syntax and built-in functions. Other functions will not be transpiled.\\nGeneral\\nPython Syntax\\nJavaScript Equivalent\\ndef function(x: int)\\nfunction function(x)\\nx + y\\n,\\nx - y\\n,\\nx * y\\n,\\nx / y\\n(x + y)\\n,\\n(x - y)\\n,\\n(x * y)\\n,\\n(x / y)\\nx > y\\n,\\nx < y\\n,\\nx >= y\\n,\\nx <= y\\n(x > y)\\n,\\n(x < y)\\n,\\n(x >= y)\\n,\\n(x <= y)\\nx == y\\n,\\nx != y\\n(x === y)\\n,\\n(x !== y)\\nx in list\\n,\\nx not in list\\nlist.includes(x)\\n,\\n!list.includes(x)\\nx = value\\nlet x = value\\nif condition: ... elif: ... else: ...\\nif (condition) { ... } else if { ... } else { ... }\\nfor x in range(n)\\nfor (let x of Array.from({length: n}, (_, i) => i))\\nfor x in list\\nfor (let x of list)\\nwhile condition\\nwhile (condition)\\n[x for x in list if condition]\\nlist.filter(x => condition).map(x => x)\\nlen(list)\\nlist.length\\nlen(dict)\\nObject.keys(dict).length\\n[1, 2, 3]\\n[1, 2, 3]\\n(1, 2, 3)\\n[1, 2, 3]\\n{\\\"key\\\": \\\"value\\\"}\\n{\\\"key\\\": \\\"value\\\"}\\nx and y\\n,\\nx or y\\n(x && y)\\n, `(x\\nNone\\nnull\\nGradio-specific syntax\\nPython Syntax\\nJavaScript Equivalent\\ngr.Component(param=value)\\n{\\\"param\\\": \\\"value\\\", \\\"__type__\\\": \\\"update\\\"}\\n\\ud83d\\udcdc License\\ngroovy is open-source under the\\nMIT License\\n.\\nContributions to increase coverage of the Python library that groovy can transpile are welcome! We welcome AI-generated PRs if the rationale is clear to follow, PRs are not too large in scope, and tests are included.\"}, {\"name\": \"plumbum\", \"description\": \".. image:: https://readthedocs.org/projects/plumbum/badge/\\n   :target: https://plumbum.readthedocs.io/en/latest/\\n   :alt: Documentation Status\\n.. image:: https://github.com/tomerfiliba/plumbum/workflows/CI/badge.svg\\n   :target: https://github.com/tomerfiliba/plumbum/actions\\n   :alt: Build Status\\n.. image:: https://coveralls.io/repos/tomerfiliba/plumbum/badge.svg?branch=master&service=github\\n   :target: https://coveralls.io/github/tomerfiliba/plumbum?branch=master\\n   :alt: Coverage Status\\n.. image:: https://img.shields.io/pypi/v/plumbum.svg\\n   :target: https://pypi.python.org/pypi/plumbum/\\n   :alt: PyPI Status\\n.. image:: https://img.shields.io/pypi/pyversions/plumbum.svg\\n   :target: https://pypi.python.org/pypi/plumbum/\\n   :alt: PyPI Versions\\n.. image:: https://img.shields.io/conda/vn/conda-forge/plumbum.svg\\n   :target: https://github.com/conda-forge/plumbum-feedstock\\n   :alt: Conda-Forge Badge\\n.. image:: https://img.shields.io/pypi/l/plumbum.svg\\n   :target: https://pypi.python.org/pypi/plumbum/\\n   :alt: PyPI License\\n.. image:: https://badges.gitter.im/plumbumpy/Lobby.svg\\n   :alt: Join the chat at https://gitter.im/plumbumpy/Lobby\\n   :target: https://gitter.im/plumbumpy/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\\n   :alt: Code styled with Black\\n   :target: https://github.com/psf/black\\n\\n\\nPlumbum: Shell Combinators\\n==========================\\n\\nEver wished the compactness of shell scripts be put into a **real** programming language?\\nSay hello to *Plumbum Shell Combinators*. Plumbum (Latin for *lead*, which was used to create\\npipes back in the day) is a small yet feature-rich library for shell script-like programs in Python.\\nThe motto of the library is **\\\"Never write shell scripts again\\\"**, and thus it attempts to mimic\\nthe **shell syntax** (\\\"shell combinators\\\") where it makes sense, while keeping it all **Pythonic\\nand cross-platform**.\\n\\nApart from shell-like syntax and handy shortcuts, the library provides local and remote command\\nexecution (over SSH), local and remote file-system paths, easy working-directory and environment\\nmanipulation, and a programmatic Command-Line Interface (CLI) application toolkit.\\nNow let's see some code!\\n\\n*This is only a teaser; the full documentation can be found at*\\n`Read the Docs <https://plumbum.readthedocs.io>`_\\n\\nCheat Sheet\\n-----------\\n\\nBasics\\n******\\n\\n.. code-block:: python\\n\\n    >>> from plumbum import local\\n    >>> local.cmd.ls\\n    LocalCommand(/bin/ls)\\n    >>> local.cmd.ls()\\n    'build.py\\\\nCHANGELOG.rst\\\\nconda.recipe\\\\nCONTRIBUTING.rst\\\\ndocs\\\\nexamples\\\\nexperiments\\\\nLICENSE\\\\nMANIFEST.in\\\\nPipfile\\\\nplumbum\\\\nplumbum.egg-info\\\\npytest.ini\\\\nREADME.rst\\\\nsetup.cfg\\\\nsetup.py\\\\ntests\\\\ntranslations.py\\\\n'\\n    >>> notepad = local[\\\"c:\\\\\\\\windows\\\\\\\\notepad.exe\\\"]\\n    >>> notepad()                                   # Notepad window pops up\\n    ''                                              # Notepad window is closed by user, command returns\\n\\nIn the example above, you can use ``local[\\\"ls\\\"]`` if you have an unusually named executable or a full path to an executable. The ``local`` object represents your local machine. As you'll see, Plumbum also provides remote machines that use the same API!\\nYou can also use ``from plumbum.cmd import ls`` as well for accessing programs in the ``PATH``.\\n\\nPiping\\n******\\n\\n.. code-block:: python\\n\\n    >>> from plumbum.cmd import ls, grep, wc\\n    >>> chain = ls[\\\"-a\\\"] | grep[\\\"-v\\\", r\\\"\\\\.py\\\"] | wc[\\\"-l\\\"]\\n    >>> print(chain)\\n    /bin/ls -a | /bin/grep -v '\\\\.py' | /usr/bin/wc -l\\n    >>> chain()\\n    '27\\\\n'\\n\\nRedirection\\n***********\\n\\n.. code-block:: python\\n\\n    >>> from plumbum.cmd import cat, head\\n    >>> ((cat < \\\"setup.py\\\") | head[\\\"-n\\\", 4])()\\n    '#!/usr/bin/env python3\\\\nimport os\\\\n\\\\ntry:\\\\n'\\n    >>> (ls[\\\"-a\\\"] > \\\"file.list\\\")()\\n    ''\\n    >>> (cat[\\\"file.list\\\"] | wc[\\\"-l\\\"])()\\n    '31\\\\n'\\n\\nWorking-directory manipulation\\n******************************\\n\\n.. code-block:: python\\n\\n    >>> local.cwd\\n    <LocalWorkdir /home/tomer/workspace/plumbum>\\n    >>> with local.cwd(local.cwd / \\\"docs\\\"):\\n    ...     chain()\\n    ...\\n    '22\\\\n'\\n\\nForeground and background execution\\n***********************************\\n\\n.. code-block:: python\\n\\n    >>> from plumbum import FG, BG\\n    >>> (ls[\\\"-a\\\"] | grep[r\\\"\\\\.py\\\"]) & FG         # The output is printed to stdout directly\\n    build.py\\n    setup.py\\n    translations.py\\n    >>> (ls[\\\"-a\\\"] | grep[r\\\"\\\\.py\\\"]) & BG         # The process runs \\\"in the background\\\"\\n    <Future ['/bin/grep', '\\\\\\\\.py'] (running)>\\n\\nCommand nesting\\n***************\\n\\n.. code-block:: python\\n\\n    >>> from plumbum.cmd import sudo, ifconfig\\n    >>> print(sudo[ifconfig[\\\"-a\\\"]])\\n    /usr/bin/sudo /sbin/ifconfig -a\\n    >>> (sudo[ifconfig[\\\"-a\\\"]] | grep[\\\"-i\\\", \\\"loop\\\"]) & FG\\n    lo        Link encap:Local Loopback\\n              UP LOOPBACK RUNNING  MTU:16436  Metric:1\\n\\nRemote commands (over SSH)\\n**************************\\n\\nSupports `openSSH <http://www.openssh.org/>`_-compatible clients,\\n`PuTTY <http://www.chiark.greenend.org.uk/~sgtatham/putty/>`_ (on Windows)\\nand `Paramiko <https://github.com/paramiko/paramiko/>`_ (a pure-Python implementation of SSH2)\\n\\n.. code-block:: python\\n\\n    >>> from plumbum import SshMachine\\n    >>> remote = SshMachine(\\\"somehost\\\", user = \\\"john\\\", keyfile = \\\"/path/to/idrsa\\\")\\n    >>> r_ls = remote[\\\"ls\\\"]\\n    >>> with remote.cwd(\\\"/lib\\\"):\\n    ...     (r_ls | grep[\\\"0.so.0\\\"])()\\n    ...\\n    'libusb-1.0.so.0\\\\nlibusb-1.0.so.0.0.0\\\\n'\\n\\nCLI applications\\n****************\\n\\n.. code-block:: python\\n\\n    import logging\\n    from plumbum import cli\\n\\n    class MyCompiler(cli.Application):\\n        verbose = cli.Flag([\\\"-v\\\", \\\"--verbose\\\"], help = \\\"Enable verbose mode\\\")\\n        include_dirs = cli.SwitchAttr(\\\"-I\\\", list = True, help = \\\"Specify include directories\\\")\\n\\n        @cli.switch(\\\"--loglevel\\\", int)\\n        def set_log_level(self, level):\\n            \\\"\\\"\\\"Sets the log-level of the logger\\\"\\\"\\\"\\n            logging.root.setLevel(level)\\n\\n        def main(self, *srcfiles):\\n            print(\\\"Verbose:\\\", self.verbose)\\n            print(\\\"Include dirs:\\\", self.include_dirs)\\n            print(\\\"Compiling:\\\", srcfiles)\\n\\n    if __name__ == \\\"__main__\\\":\\n        MyCompiler.run()\\n\\nSample output\\n+++++++++++++\\n\\n::\\n\\n    $ python3 simple_cli.py -v -I foo/bar -Ispam/eggs x.cpp y.cpp z.cpp\\n    Verbose: True\\n    Include dirs: ['foo/bar', 'spam/eggs']\\n    Compiling: ('x.cpp', 'y.cpp', 'z.cpp')\\n\\nColors and Styles\\n-----------------\\n\\n.. code-block:: python\\n\\n    from plumbum import colors\\n    with colors.red:\\n        print(\\\"This library provides safe, flexible color access.\\\")\\n        print(colors.bold | \\\"(and styles in general)\\\", \\\"are easy!\\\")\\n    print(\\\"The simple 16 colors or\\\",\\n          colors.orchid & colors.underline | '256 named colors,',\\n          colors.rgb(18, 146, 64) | \\\"or full rgb colors\\\",\\n          'can be used.')\\n    print(\\\"Unsafe \\\" + colors.bg.dark_khaki + \\\"color access\\\" + colors.bg.reset + \\\" is available too.\\\")\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"Plumbum: shell combinators library\", \"latest_version\": \"1.10.0\", \"weekly_downloads\": 809568, \"description_cleaned\": \"Plumbum: Shell Combinators\\nEver wished the compactness of shell scripts be put into a\\nreal\\nprogramming language?\\nSay hello to\\nPlumbum Shell Combinators\\n. Plumbum (Latin for\\nlead\\n, which was used to create\\npipes back in the day) is a small yet feature-rich library for shell script-like programs in Python.\\nThe motto of the library is\\n\\u201cNever write shell scripts again\\u201d\\n, and thus it attempts to mimic\\nthe\\nshell syntax\\n(\\u201cshell combinators\\u201d) where it makes sense, while keeping it all\\nPythonic\\nand cross-platform\\n.\\nApart from shell-like syntax and handy shortcuts, the library provides local and remote command\\nexecution (over SSH), local and remote file-system paths, easy working-directory and environment\\nmanipulation, and a programmatic Command-Line Interface (CLI) application toolkit.\\nNow let\\u2019s see some code!\\nThis is only a teaser; the full documentation can be found at\\nRead the Docs\\nCheat Sheet\\nBasics\\n>>>\\nfrom\\nplumbum\\nimport\\nlocal\\n>>>\\nlocal\\n.\\ncmd\\n.\\nls\\nLocalCommand\\n(\\n/\\nbin\\n/\\nls\\n)\\n>>>\\nlocal\\n.\\ncmd\\n.\\nls\\n()\\n'build.py\\n\\\\n\\nCHANGELOG.rst\\n\\\\n\\nconda.recipe\\n\\\\n\\nCONTRIBUTING.rst\\n\\\\n\\ndocs\\n\\\\n\\nexamples\\n\\\\n\\nexperiments\\n\\\\n\\nLICENSE\\n\\\\n\\nMANIFEST.in\\n\\\\n\\nPipfile\\n\\\\n\\nplumbum\\n\\\\n\\nplumbum.egg-info\\n\\\\n\\npytest.ini\\n\\\\n\\nREADME.rst\\n\\\\n\\nsetup.cfg\\n\\\\n\\nsetup.py\\n\\\\n\\ntests\\n\\\\n\\ntranslations.py\\n\\\\n\\n'\\n>>>\\nnotepad\\n=\\nlocal\\n[\\n\\\"c:\\n\\\\\\\\\\nwindows\\n\\\\\\\\\\nnotepad.exe\\\"\\n]\\n>>>\\nnotepad\\n()\\n# Notepad window pops up\\n''\\n# Notepad window is closed by user, command returns\\nIn the example above, you can use\\nlocal[\\\"ls\\\"]\\nif you have an unusually named executable or a full path to an executable. The\\nlocal\\nobject represents your local machine. As you\\u2019ll see, Plumbum also provides remote machines that use the same API!\\nYou can also use\\nfrom plumbum.cmd import ls\\nas well for accessing programs in the\\nPATH\\n.\\nPiping\\n>>>\\nfrom\\nplumbum.cmd\\nimport\\nls\\n,\\ngrep\\n,\\nwc\\n>>>\\nchain\\n=\\nls\\n[\\n\\\"-a\\\"\\n]\\n|\\ngrep\\n[\\n\\\"-v\\\"\\n,\\nr\\n\\\"\\\\.py\\\"\\n]\\n|\\nwc\\n[\\n\\\"-l\\\"\\n]\\n>>>\\nprint\\n(\\nchain\\n)\\n/\\nbin\\n/\\nls\\n-\\na\\n|\\n/\\nbin\\n/\\ngrep\\n-\\nv\\n'\\\\.py'\\n|\\n/\\nusr\\n/\\nbin\\n/\\nwc\\n-\\nl\\n>>>\\nchain\\n()\\n'27\\n\\\\n\\n'\\nRedirection\\n>>>\\nfrom\\nplumbum.cmd\\nimport\\ncat\\n,\\nhead\\n>>>\\n((\\ncat\\n<\\n\\\"setup.py\\\"\\n)\\n|\\nhead\\n[\\n\\\"-n\\\"\\n,\\n4\\n])()\\n'#!/usr/bin/env python3\\n\\\\n\\nimport os\\n\\\\n\\\\n\\ntry:\\n\\\\n\\n'\\n>>>\\n(\\nls\\n[\\n\\\"-a\\\"\\n]\\n>\\n\\\"file.list\\\"\\n)()\\n''\\n>>>\\n(\\ncat\\n[\\n\\\"file.list\\\"\\n]\\n|\\nwc\\n[\\n\\\"-l\\\"\\n])()\\n'31\\n\\\\n\\n'\\nWorking-directory manipulation\\n>>>\\nlocal\\n.\\ncwd\\n<\\nLocalWorkdir\\n/\\nhome\\n/\\ntomer\\n/\\nworkspace\\n/\\nplumbum\\n>\\n>>>\\nwith\\nlocal\\n.\\ncwd\\n(\\nlocal\\n.\\ncwd\\n/\\n\\\"docs\\\"\\n):\\n...\\nchain\\n()\\n...\\n'22\\n\\\\n\\n'\\nForeground and background execution\\n>>>\\nfrom\\nplumbum\\nimport\\nFG\\n,\\nBG\\n>>>\\n(\\nls\\n[\\n\\\"-a\\\"\\n]\\n|\\ngrep\\n[\\nr\\n\\\"\\\\.py\\\"\\n])\\n&\\nFG\\n# The output is printed to stdout directly\\nbuild\\n.\\npy\\nsetup\\n.\\npy\\ntranslations\\n.\\npy\\n>>>\\n(\\nls\\n[\\n\\\"-a\\\"\\n]\\n|\\ngrep\\n[\\nr\\n\\\"\\\\.py\\\"\\n])\\n&\\nBG\\n# The process runs \\\"in the background\\\"\\n<\\nFuture\\n[\\n'/bin/grep'\\n,\\n'\\n\\\\\\\\\\n.py'\\n]\\n(\\nrunning\\n)\\n>\\nCommand nesting\\n>>>\\nfrom\\nplumbum.cmd\\nimport\\nsudo\\n,\\nifconfig\\n>>>\\nprint\\n(\\nsudo\\n[\\nifconfig\\n[\\n\\\"-a\\\"\\n]])\\n/\\nusr\\n/\\nbin\\n/\\nsudo\\n/\\nsbin\\n/\\nifconfig\\n-\\na\\n>>>\\n(\\nsudo\\n[\\nifconfig\\n[\\n\\\"-a\\\"\\n]]\\n|\\ngrep\\n[\\n\\\"-i\\\"\\n,\\n\\\"loop\\\"\\n])\\n&\\nFG\\nlo\\nLink\\nencap\\n:\\nLocal\\nLoopback\\nUP\\nLOOPBACK\\nRUNNING\\nMTU\\n:\\n16436\\nMetric\\n:\\n1\\nRemote commands (over SSH)\\nSupports\\nopenSSH\\n-compatible clients,\\nPuTTY\\n(on Windows)\\nand\\nParamiko\\n(a pure-Python implementation of SSH2)\\n>>>\\nfrom\\nplumbum\\nimport\\nSshMachine\\n>>>\\nremote\\n=\\nSshMachine\\n(\\n\\\"somehost\\\"\\n,\\nuser\\n=\\n\\\"john\\\"\\n,\\nkeyfile\\n=\\n\\\"/path/to/idrsa\\\"\\n)\\n>>>\\nr_ls\\n=\\nremote\\n[\\n\\\"ls\\\"\\n]\\n>>>\\nwith\\nremote\\n.\\ncwd\\n(\\n\\\"/lib\\\"\\n):\\n...\\n(\\nr_ls\\n|\\ngrep\\n[\\n\\\"0.so.0\\\"\\n])()\\n...\\n'libusb-1.0.so.0\\n\\\\n\\nlibusb-1.0.so.0.0.0\\n\\\\n\\n'\\nCLI applications\\nimport\\nlogging\\nfrom\\nplumbum\\nimport\\ncli\\nclass\\nMyCompiler\\n(\\ncli\\n.\\nApplication\\n):\\nverbose\\n=\\ncli\\n.\\nFlag\\n([\\n\\\"-v\\\"\\n,\\n\\\"--verbose\\\"\\n],\\nhelp\\n=\\n\\\"Enable verbose mode\\\"\\n)\\ninclude_dirs\\n=\\ncli\\n.\\nSwitchAttr\\n(\\n\\\"-I\\\"\\n,\\nlist\\n=\\nTrue\\n,\\nhelp\\n=\\n\\\"Specify include directories\\\"\\n)\\n@cli\\n.\\nswitch\\n(\\n\\\"--loglevel\\\"\\n,\\nint\\n)\\ndef\\nset_log_level\\n(\\nself\\n,\\nlevel\\n):\\n\\\"\\\"\\\"Sets the log-level of the logger\\\"\\\"\\\"\\nlogging\\n.\\nroot\\n.\\nsetLevel\\n(\\nlevel\\n)\\ndef\\nmain\\n(\\nself\\n,\\n*\\nsrcfiles\\n):\\nprint\\n(\\n\\\"Verbose:\\\"\\n,\\nself\\n.\\nverbose\\n)\\nprint\\n(\\n\\\"Include dirs:\\\"\\n,\\nself\\n.\\ninclude_dirs\\n)\\nprint\\n(\\n\\\"Compiling:\\\"\\n,\\nsrcfiles\\n)\\nif\\n__name__\\n==\\n\\\"__main__\\\"\\n:\\nMyCompiler\\n.\\nrun\\n()\\nSample output\\n$ python3 simple_cli.py -v -I foo/bar -Ispam/eggs x.cpp y.cpp z.cpp\\nVerbose: True\\nInclude dirs: ['foo/bar', 'spam/eggs']\\nCompiling: ('x.cpp', 'y.cpp', 'z.cpp')\\nColors and Styles\\nfrom\\nplumbum\\nimport\\ncolors\\nwith\\ncolors\\n.\\nred\\n:\\nprint\\n(\\n\\\"This library provides safe, flexible color access.\\\"\\n)\\nprint\\n(\\ncolors\\n.\\nbold\\n|\\n\\\"(and styles in general)\\\"\\n,\\n\\\"are easy!\\\"\\n)\\nprint\\n(\\n\\\"The simple 16 colors or\\\"\\n,\\ncolors\\n.\\norchid\\n&\\ncolors\\n.\\nunderline\\n|\\n'256 named colors,'\\n,\\ncolors\\n.\\nrgb\\n(\\n18\\n,\\n146\\n,\\n64\\n)\\n|\\n\\\"or full rgb colors\\\"\\n,\\n'can be used.'\\n)\\nprint\\n(\\n\\\"Unsafe \\\"\\n+\\ncolors\\n.\\nbg\\n.\\ndark_khaki\\n+\\n\\\"color access\\\"\\n+\\ncolors\\n.\\nbg\\n.\\nreset\\n+\\n\\\" is available too.\\\"\\n)\"}, {\"name\": \"outlines-core\", \"description\": \"<div align=\\\"center\\\" style=\\\"margin-bottom: 1em;\\\">\\n\\n<img src=\\\"./docs/assets/images/logo.png\\\" alt=\\\"Outlines-core Logo\\\" width=500></img>\\n\\n[![Latest Version]][crates.io] [![License]][github] ![MSRV][version]\\n\\n[Latest Version]: https://img.shields.io/crates/v/outlines-core.svg\\n[crates.io]: https://crates.io/crates/outlines-core\\n[License]: https://img.shields.io/github/license/dottxt-ai/outlines-core.svg?color=blue&cachedrop\\n[github]: https://github.com/dottxt-ai/outlines-core/blob/main/LICENSE\\n[MSRV]: MSRV\\n[version]: https://img.shields.io/crates/msrv/outlines-core.svg?label=msrv&color=lightgrayy\\n\\n*Structured generation (in Rust).*\\n\\n</div>\\n\\n## Outlines-core\\n\\nThis package provides the core functionality for structured generation, formerly implemented in [Outlines][outlines],\\nwith a focus on performance and portability, it offers a convenient way to:\\n\\n- build regular expressions from JSON schemas\\n\\n- construct an `Index` object by combining a `Vocabulary` and regular expression to efficiently map tokens from a given vocabulary to state transitions in a finite-state automation\\n\\n### Example\\n\\nBasic example of how it all fits together.\\n\\n```rust\\nuse outlines_core::prelude::*;\\n\\n// Define a JSON schema\\nlet schema = r#\\\"{\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"properties\\\": {\\n        \\\"name\\\": { \\\"type\\\": \\\"string\\\" },\\n        \\\"age\\\": { \\\"type\\\": \\\"integer\\\" }\\n    },\\n    \\\"required\\\": [\\\"name\\\", \\\"age\\\"]\\n}\\\"#;\\n\\n// Generate a regular expression from it\\nlet regex = json_schema::regex_from_str(&schema, None)?;\\n\\n// Create `Vocabulary` from pretrained large language model (but manually is also possible)\\nlet vocabulary = Vocabulary::from_pretrained(\\\"openai-community/gpt2\\\", None)?;\\n\\n// Create new `Index` from regex and a given `Vocabulary`\\nlet index = Index::new(&regex, &vocabulary)?;\\n\\nlet initial_state = index.initial_state();\\nlet allowed_tokens = index.allowed_tokens(&initial_state).expect(\\\"Some allowed token ids\\\");\\nlet token_id = allowed_tokens.first().expect(\\\"First token id\\\");\\nlet next_state = index.next_state(&initial_state, token_id);\\nlet final_states = index.final_states();\\n```\\n\\n## Python Bindings\\n\\nAdditionally, project provides interfaces to integrate the crate's functionality with Python.\\n\\n``` python\\nimport json\\n\\nfrom outlines_core.json_schema import build_regex_from_schema\\nfrom outlines_core.guide import Guide, Index, Vocabulary\\n\\nschema =  {\\n  \\\"title\\\": \\\"Foo\\\",\\n  \\\"type\\\": \\\"object\\\",\\n  \\\"properties\\\": {\\\"date\\\": {\\\"type\\\": \\\"string\\\", \\\"format\\\": \\\"date\\\"}}\\n}\\nregex = build_regex_from_schema(json.dumps(schema))\\n\\nvocabulary = Vocabulary.from_pretrained(\\\"openai-community/gpt2\\\")\\nindex = Index(regex, vocabulary)\\nguide = Guide(index)\\n\\n# Get current state of the Guide:\\ncurrent_state = guide.get_state()\\n\\n# Get allowed tokens for the current state of the Guide:\\nallowed_tokens = guide.get_tokens()\\n\\n# Advance Guide to the next state via some token_id and return allowed tokens for that new state:\\nnext_allowed_tokens = guide.advance(allowed_tokens[-1])\\n\\n# To check if Guide is finished:\\nguide.is_finished()\\n\\n# If it's finished then this assertion holds:\\nassert guide.get_tokens() == [vocabulary.get_eos_token_id()]\\n```\\n\\n## How to contribute?\\n\\n### Setup\\n\\nFork the repository on GitHub and clone the fork locally:\\n\\n```bash\\ngit clone git@github.com/YourUserName/outlines-core.git\\ncd outlines-core\\n```\\n\\nCreate a new virtual environment and install the dependencies in editable mode:\\n\\n``` bash\\npython -m venv .venv\\nsource .venv/bin/activate\\npip install -e \\\".[test]\\\"\\npre-commit install\\n```\\n\\n### Before pushing your code\\n\\nIf working with Python bindings don't forget to build Rust extension before testing, for example, in debug mode:\\n\\n```bash\\nmake build-extension-debug\\n```\\n\\nRun Python tests:\\n\\n``` bash\\npytest\\n```\\n\\nRun Rust tests:\\n\\n``` bash\\ncargo test\\n```\\n\\nOr alternatively using Makefile for both:\\n\\n``` bash\\nmake test\\n```\\n\\nFinally, run the code style checks:\\n\\n``` bash\\npre-commit run --all-files\\n```\\n\\nOr using Makefile:\\n\\n``` bash\\nmake pcc\\n```\\n\\nIf necessary you can run benchmarks locally:\\n\\n``` bash\\nmake pybench\\n```\\n\\n## Join us\\n\\n- \\ud83d\\udca1 **Have an idea?** Come chat with us on [Discord][discord]\\n-  **Found a bug?** Open an [issue](https://github.com/dottxt-ai/outlines-core/issues)\\n\\n[outlines]: https://github.com/dottxt-ai/outlines\\n[discord]: https://discord.gg/R9DSu34mGd\\n\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Structured Text Generation in Rust\", \"latest_version\": \"0.2.13\", \"weekly_downloads\": 809539, \"description_cleaned\": \"Structured generation (in Rust).\\nOutlines-core\\nThis package provides the core functionality for structured generation, formerly implemented in\\nOutlines\\n,\\nwith a focus on performance and portability, it offers a convenient way to:\\nbuild regular expressions from JSON schemas\\nconstruct an\\nIndex\\nobject by combining a\\nVocabulary\\nand regular expression to efficiently map tokens from a given vocabulary to state transitions in a finite-state automation\\nExample\\nBasic example of how it all fits together.\\nuse\\noutlines_core\\n::\\nprelude\\n::\\n*\\n;\\n// Define a JSON schema\\nlet\\nschema\\n=\\nr#\\\"{\\n\\\"type\\\": \\\"object\\\",\\n\\\"properties\\\": {\\n\\\"name\\\": { \\\"type\\\": \\\"string\\\" },\\n\\\"age\\\": { \\\"type\\\": \\\"integer\\\" }\\n},\\n\\\"required\\\": [\\\"name\\\", \\\"age\\\"]\\n}\\\"#\\n;\\n// Generate a regular expression from it\\nlet\\nregex\\n=\\njson_schema\\n::\\nregex_from_str\\n(\\n&\\nschema\\n,\\nNone\\n)\\n?\\n;\\n// Create `Vocabulary` from pretrained large language model (but manually is also possible)\\nlet\\nvocabulary\\n=\\nVocabulary\\n::\\nfrom_pretrained\\n(\\n\\\"openai-community/gpt2\\\"\\n,\\nNone\\n)\\n?\\n;\\n// Create new `Index` from regex and a given `Vocabulary`\\nlet\\nindex\\n=\\nIndex\\n::\\nnew\\n(\\n&\\nregex\\n,\\n&\\nvocabulary\\n)\\n?\\n;\\nlet\\ninitial_state\\n=\\nindex\\n.\\ninitial_state\\n();\\nlet\\nallowed_tokens\\n=\\nindex\\n.\\nallowed_tokens\\n(\\n&\\ninitial_state\\n).\\nexpect\\n(\\n\\\"Some allowed token ids\\\"\\n);\\nlet\\ntoken_id\\n=\\nallowed_tokens\\n.\\nfirst\\n().\\nexpect\\n(\\n\\\"First token id\\\"\\n);\\nlet\\nnext_state\\n=\\nindex\\n.\\nnext_state\\n(\\n&\\ninitial_state\\n,\\ntoken_id\\n);\\nlet\\nfinal_states\\n=\\nindex\\n.\\nfinal_states\\n();\\nPython Bindings\\nAdditionally, project provides interfaces to integrate the crate's functionality with Python.\\nimport\\njson\\nfrom\\noutlines_core.json_schema\\nimport\\nbuild_regex_from_schema\\nfrom\\noutlines_core.guide\\nimport\\nGuide\\n,\\nIndex\\n,\\nVocabulary\\nschema\\n=\\n{\\n\\\"title\\\"\\n:\\n\\\"Foo\\\"\\n,\\n\\\"type\\\"\\n:\\n\\\"object\\\"\\n,\\n\\\"properties\\\"\\n:\\n{\\n\\\"date\\\"\\n:\\n{\\n\\\"type\\\"\\n:\\n\\\"string\\\"\\n,\\n\\\"format\\\"\\n:\\n\\\"date\\\"\\n}}\\n}\\nregex\\n=\\nbuild_regex_from_schema\\n(\\njson\\n.\\ndumps\\n(\\nschema\\n))\\nvocabulary\\n=\\nVocabulary\\n.\\nfrom_pretrained\\n(\\n\\\"openai-community/gpt2\\\"\\n)\\nindex\\n=\\nIndex\\n(\\nregex\\n,\\nvocabulary\\n)\\nguide\\n=\\nGuide\\n(\\nindex\\n)\\n# Get current state of the Guide:\\ncurrent_state\\n=\\nguide\\n.\\nget_state\\n()\\n# Get allowed tokens for the current state of the Guide:\\nallowed_tokens\\n=\\nguide\\n.\\nget_tokens\\n()\\n# Advance Guide to the next state via some token_id and return allowed tokens for that new state:\\nnext_allowed_tokens\\n=\\nguide\\n.\\nadvance\\n(\\nallowed_tokens\\n[\\n-\\n1\\n])\\n# To check if Guide is finished:\\nguide\\n.\\nis_finished\\n()\\n# If it's finished then this assertion holds:\\nassert\\nguide\\n.\\nget_tokens\\n()\\n==\\n[\\nvocabulary\\n.\\nget_eos_token_id\\n()]\\nHow to contribute?\\nSetup\\nFork the repository on GitHub and clone the fork locally:\\ngit\\nclone\\ngit@github.com/YourUserName/outlines-core.git\\ncd\\noutlines-core\\nCreate a new virtual environment and install the dependencies in editable mode:\\npython\\n-m\\nvenv\\n.venv\\nsource\\n.venv/bin/activate\\npip\\ninstall\\n-e\\n\\\".[test]\\\"\\npre-commit\\ninstall\\nBefore pushing your code\\nIf working with Python bindings don't forget to build Rust extension before testing, for example, in debug mode:\\nmake\\nbuild-extension-debug\\nRun Python tests:\\npytest\\nRun Rust tests:\\ncargo\\ntest\\nOr alternatively using Makefile for both:\\nmake\\ntest\\nFinally, run the code style checks:\\npre-commit\\nrun\\n--all-files\\nOr using Makefile:\\nmake\\npcc\\nIf necessary you can run benchmarks locally:\\nmake\\npybench\\nJoin us\\n\\ud83d\\udca1\\nHave an idea?\\nCome chat with us on\\nDiscord\\nFound a bug?\\nOpen an\\nissue\"}, {\"name\": \"mizani\", \"description\": \"# Mizani\\n\\n[![Release](https://img.shields.io/pypi/v/mizani.svg)](https://pypi.python.org/pypi/mizani)\\n[![License](https://img.shields.io/pypi/l/mizani.svg)](https://pypi.python.org/pypi/mizani)\\n[![Build Status](https://github.com/has2k1/mizani/workflows/build/badge.svg?branch=main)](https://github.com/has2k1/mizani/actions?query=branch%3Amain+workflow%3A%22build%22)\\n[![Documentation](https://readthedocs.org/projects/mizani/badge/?version=latest)](https://mizani.readthedocs.io/en/latest/)\\n[![Coverage](https://codecov.io/github/has2k1/mizani/coverage.svg?branch=main)](https://codecov.io/github/has2k1/mizani?branch=main)\\n\\nMizani is a scales package for graphics. It is written in Python and is\\nbased on Hadley Wickham's [Scales](https://github.com/r-lib/scales).\\nSee the [documentation](https://mizani.readthedocs.io/en/latest/)\\nfor how to use it in a graphics system.\\n\\n## Installation\\n\\nOfficial Release\\n\\n```console\\n$ pip install mizani\\n```\\n\\nDevelopment version\\n\\n```console\\n$ pip install git+https://github.com/has2k1/mizani.git@main\\n```\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Scales for Python\", \"latest_version\": \"0.14.3\", \"weekly_downloads\": 809342, \"description_cleaned\": \"Mizani\\nMizani is a scales package for graphics. It is written in Python and is\\nbased on Hadley Wickham's\\nScales\\n.\\nSee the\\ndocumentation\\nfor how to use it in a graphics system.\\nInstallation\\nOfficial Release\\n$\\npip\\ninstall\\nmizani\\nDevelopment version\\n$\\npip\\ninstall\\ngit+https://github.com/has2k1/mizani.git@main\"}, {\"name\": \"arize-phoenix\", \"description\": \"<p align=\\\"center\\\">\\n    <a target=\\\"_blank\\\" href=\\\"https://phoenix.arize.com\\\" style=\\\"background:none\\\">\\n        <img alt=\\\"phoenix banner\\\" src=\\\"https://github.com/Arize-ai/phoenix-assets/blob/main/images/socal/github-large-banner-phoenix-v2.jpg?raw=true\\\" width=\\\"auto\\\" height=\\\"auto\\\"></img>\\n    </a>\\n    <br/>\\n    <br/>\\n    <a href=\\\"https://arize.com/docs/phoenix/\\\">\\n        <img src=\\\"https://img.shields.io/static/v1?message=Docs&logo=data:image/png;base64,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&labelColor=grey&color=blue&logoColor=white&label=%20\\\"/>\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg?__hstc=259489365.a667dfafcfa0169c8aee4178d115dc81.1733501603539.1733501603539.1733501603539.1&__hssc=259489365.1.1733501603539&__hsfp=3822854628&submissionGuid=381a0676-8f38-437b-96f2-fc10875658df#/shared-invite/email\\\">\\n        <img src=\\\"https://img.shields.io/static/v1?message=Community&logo=slack&labelColor=grey&color=blue&logoColor=white&label=%20\\\"/>\\n    </a>\\n     <a target=\\\"_blank\\\" href=\\\"https://bsky.app/profile/arize-phoenix.bsky.social\\\">\\n        <img src=\\\"https://img.shields.io/badge/-phoenix-blue.svg?color=blue&labelColor=gray&logo=bluesky\\\">\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://x.com/ArizePhoenix\\\">\\n        <img src=\\\"https://img.shields.io/badge/-ArizePhoenix-blue.svg?color=blue&labelColor=gray&logo=x\\\">\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://pypi.org/project/arize-phoenix/\\\">\\n        <img src=\\\"https://img.shields.io/pypi/v/arize-phoenix?color=blue\\\">\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://anaconda.org/conda-forge/arize-phoenix\\\">\\n        <img src=\\\"https://img.shields.io/conda/vn/conda-forge/arize-phoenix.svg?color=blue\\\">\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://pypi.org/project/arize-phoenix/\\\">\\n        <img src=\\\"https://img.shields.io/pypi/pyversions/arize-phoenix\\\">\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://hub.docker.com/r/arizephoenix/phoenix/tags\\\">\\n        <img src=\\\"https://img.shields.io/docker/v/arizephoenix/phoenix?sort=semver&logo=docker&label=image&color=blue\\\">\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://hub.docker.com/r/arizephoenix/phoenix-helm\\\">\\n        <img src=\\\"https://img.shields.io/badge/Helm-blue?style=flat&logo=helm&labelColor=grey\\\"/>\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-mcp\\\">\\n        <img src=\\\"https://badge.mcpx.dev?status=on\\\" title=\\\"MCP Enabled\\\"/>\\n    </a>\\n    <a href=\\\"cursor://anysphere.cursor-deeplink/mcp/install?name=phoenix&config=eyJjb21tYW5kIjoibnB4IC15IEBhcml6ZWFpL3Bob2VuaXgtbWNwQGxhdGVzdCAtLWJhc2VVcmwgaHR0cHM6Ly9teS1waG9lbml4LmNvbSAtLWFwaUtleSB5b3VyLWFwaS1rZXkifQ%3D%3D\\\"><img src=\\\"https://cursor.com/deeplink/mcp-install-dark.svg\\\" alt=\\\"Add Arize Phoenix MCP server to Cursor\\\" height=20 /></a>\\n</p>\\n\\nPhoenix is an open-source AI observability platform designed for experimentation, evaluation, and troubleshooting. It provides:\\n\\n- [**_Tracing_**](https://arize.com/docs/phoenix/tracing/llm-traces) - Trace your LLM application's runtime using OpenTelemetry-based instrumentation.\\n- [**_Evaluation_**](https://arize.com/docs/phoenix/evaluation/llm-evals) - Leverage LLMs to benchmark your application's performance using response and retrieval evals.\\n- [**_Datasets_**](https://arize.com/docs/phoenix/datasets-and-experiments/overview-datasets) - Create versioned datasets of examples for experimentation, evaluation, and fine-tuning.\\n- [**_Experiments_**](https://arize.com/docs/phoenix/datasets-and-experiments/overview-datasets#experiments) - Track and evaluate changes to prompts, LLMs, and retrieval.\\n- [**_Playground_**](https://arize.com/docs/phoenix/prompt-engineering/overview-prompts)- Optimize prompts, compare models, adjust parameters, and replay traced LLM calls.\\n- [**_Prompt Management_**](https://arize.com/docs/phoenix/prompt-engineering/overview-prompts/prompt-management)- Manage and test prompt changes systematically using version control, tagging, and experimentation.\\n\\nPhoenix is vendor and language agnostic with out-of-the-box support for popular frameworks (\\ud83e\\udd99[LlamaIndex](https://arize.com/docs/phoenix/tracing/integrations-tracing/llamaindex), \\ud83e\\udd9c\\u26d3[LangChain](https://arize.com/docs/phoenix/tracing/integrations-tracing/langchain), [Haystack](https://arize.com/docs/phoenix/tracing/integrations-tracing/haystack), \\ud83e\\udde9[DSPy](https://arize.com/docs/phoenix/tracing/integrations-tracing/dspy), \\ud83e\\udd17[smolagents](https://arize.com/docs/phoenix/tracing/integrations-tracing/hfsmolagents)) and LLM providers ([OpenAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/openai), [Bedrock](https://arize.com/docs/phoenix/tracing/integrations-tracing/bedrock), [MistralAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/mistralai), [VertexAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/vertexai), [LiteLLM](https://arize.com/docs/phoenix/tracing/integrations-tracing/litellm), [Google GenAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/google-genai) and more). For details on auto-instrumentation, check out the [OpenInference](https://github.com/Arize-ai/openinference) project.\\n\\nPhoenix runs practically anywhere, including your local machine, a Jupyter notebook, a containerized deployment, or in the cloud.\\n\\n## Installation\\n\\nInstall Phoenix via `pip` or `conda`\\n\\n```shell\\npip install arize-phoenix\\n```\\n\\nPhoenix container images are available via [Docker Hub](https://hub.docker.com/r/arizephoenix/phoenix) and can be deployed using Docker or Kubernetes. Arize AI also provides cloud instances at [app.phoenix.arize.com](https://app.phoenix.arize.com/).\\n\\n## Packages\\n\\nThe `arize-phoenix` package includes the entire Phoenix platfom. However if you have deployed the Phoenix platform, there are light-weight Python sub-packages and TypeScript packages that can be used in conjunction with the platfrom.\\n\\n### Python Subpackages\\n\\n| Package                                                                                       | Version & Docs                                                                                                                                                                                                                                                                      | Description                                                                                |\\n| --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |\\n| [arize-phoenix-otel](https://github.com/Arize-ai/phoenix/tree/main/packages/phoenix-otel)     | [![PyPI Version](https://img.shields.io/pypi/v/arize-phoenix-otel)](https://pypi.org/project/arize-phoenix-otel/) [![Docs](https://img.shields.io/badge/docs-blue?logo=readthedocs&logoColor=white)](https://arize-phoenix.readthedocs.io/projects/otel/en/latest/index.html)       | Provides a lightweight wrapper around OpenTelemetry primitives with Phoenix-aware defaults |\\n| [arize-phoenix-client](https://github.com/Arize-ai/phoenix/tree/main/packages/phoenix-client) | [![PyPI Version](https://img.shields.io/pypi/v/arize-phoenix-client)](https://pypi.org/project/arize-phoenix-client/) [![Docs](https://img.shields.io/badge/docs-blue?logo=readthedocs&logoColor=white)](https://arize-phoenix.readthedocs.io/projects/client/en/latest/index.html) | Lightweight client for interacting with the Phoenix server via its OpenAPI REST interface  |\\n| [arize-phoenix-evals](https://github.com/Arize-ai/phoenix/tree/main/packages/phoenix-evals)   | [![PyPI Version](https://img.shields.io/pypi/v/arize-phoenix-evals)](https://pypi.org/project/arize-phoenix-evals/) [![Docs](https://img.shields.io/badge/docs-blue?logo=readthedocs&logoColor=white)](https://arize-phoenix.readthedocs.io/projects/evals/en/latest/index.html)    | Tooling to evaluate LLM applications including RAG relevance, answer relevance, and more   |\\n\\n### TypeScript Subpackages\\n\\n| Package                                                                                             | Version & Docs                                                                                                                                                                                                                                           | Description                                                                                       |\\n| --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |\\n| [@arizeai/phoenix-otel](https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-otel)     | [![NPM Version](https://img.shields.io/npm/v/%40arizeai%2Fphoenix-otel)](https://www.npmjs.com/package/@arizeai/phoenix-otel) [![Docs](https://img.shields.io/badge/docs-blue?logo=typescript&logoColor=white)](https://arize-ai.github.io/phoenix/)     | Provides a lightweight wrapper around OpenTelemetry primitives with Phoenix-aware defaults        |\\n| [@arizeai/phoenix-client](https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-client) | [![NPM Version](https://img.shields.io/npm/v/%40arizeai%2Fphoenix-client)](https://www.npmjs.com/package/@arizeai/phoenix-client) [![Docs](https://img.shields.io/badge/docs-blue?logo=typescript&logoColor=white)](https://arize-ai.github.io/phoenix/) | Client for the Arize Phoenix API                                                                  |\\n| [@arizeai/phoenix-evals](https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-evals)   | [![NPM Version](https://img.shields.io/npm/v/%40arizeai%2Fphoenix-evals)](https://www.npmjs.com/package/@arizeai/phoenix-evals) [![Docs](https://img.shields.io/badge/docs-blue?logo=typescript&logoColor=white)](https://arize-ai.github.io/phoenix/)   | TypeScript evaluation library for LLM applications (alpha release)                                |\\n| [@arizeai/phoenix-mcp](https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-mcp)       | [![NPM Version](https://img.shields.io/npm/v/%40arizeai%2Fphoenix-mcp)](https://www.npmjs.com/package/@arizeai/phoenix-mcp) [![Docs](https://img.shields.io/badge/docs-blue?logo=markdown&logoColor=white)](./js/packages/phoenix-mcp/README.md)         | MCP server implementation for Arize Phoenix providing unified interface to Phoenix's capabilities |\\n\\n## Tracing Integrations\\n\\nPhoenix is built on top of OpenTelemetry and is vendor, language, and framework agnostic. For details about tracing integrations and example applications, see the [OpenInference](https://github.com/Arize-ai/openinference) project.\\n\\n**Python Integrations**\\n| Integration | Package | Version Badge |\\n|------------------|-----------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|\\n| [OpenAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/openai) | `openinference-instrumentation-openai` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-openai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-openai) |\\n| [OpenAI Agents](https://arize.com/docs/phoenix/tracing/integrations-tracing/openai-agents-sdk) | `openinference-instrumentation-openai-agents` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-openai-agents.svg)](https://pypi.python.org/pypi/openinference-instrumentation-openai-agents) |\\n| [LlamaIndex](https://arize.com/docs/phoenix/tracing/integrations-tracing/llamaindex) | `openinference-instrumentation-llama-index` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-llama-index.svg)](https://pypi.python.org/pypi/openinference-instrumentation-llama-index) |\\n| [DSPy](https://arize.com/docs/phoenix/tracing/integrations-tracing/dspy) | `openinference-instrumentation-dspy` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-dspy.svg)](https://pypi.python.org/pypi/openinference-instrumentation-dspy) |\\n| [AWS Bedrock](https://arize.com/docs/phoenix/tracing/integrations-tracing/bedrock) | `openinference-instrumentation-bedrock` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-bedrock.svg)](https://pypi.python.org/pypi/openinference-instrumentation-bedrock) |\\n| [LangChain](https://arize.com/docs/phoenix/tracing/integrations-tracing/langchain) | `openinference-instrumentation-langchain` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-langchain.svg)](https://pypi.python.org/pypi/openinference-instrumentation-langchain) |\\n| [MistralAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/mistralai) | `openinference-instrumentation-mistralai` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-mistralai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-mistralai) |\\n| [Google GenAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/google-gen-ai) | `openinference-instrumentation-google-genai` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-google-genai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-google-genai) |\\n| [Google ADK](https://arize.com/docs/phoenix/integrations/llm-providers/google-gen-ai/google-adk-tracing) | `openinference-instrumentation-google-adk` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-google-adk.svg)](https://pypi.python.org/pypi/openinference-instrumentation-google-adk) |\\n| [Guardrails](https://arize.com/docs/phoenix/tracing/integrations-tracing/guardrails) | `openinference-instrumentation-guardrails` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-guardrails.svg)](https://pypi.python.org/pypi/openinference-instrumentation-guardrails) |\\n| [VertexAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/vertexai) | `openinference-instrumentation-vertexai` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-vertexai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-vertexai) |\\n| [CrewAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/crewai) | `openinference-instrumentation-crewai` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-crewai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-crewai) |\\n| [Haystack](https://arize.com/docs/phoenix/tracing/integrations-tracing/haystack) | `openinference-instrumentation-haystack` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-haystack.svg)](https://pypi.python.org/pypi/openinference-instrumentation-haystack) |\\n| [LiteLLM](https://arize.com/docs/phoenix/tracing/integrations-tracing/litellm) | `openinference-instrumentation-litellm` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-litellm.svg)](https://pypi.python.org/pypi/openinference-instrumentation-litellm) |\\n| [Groq](https://arize.com/docs/phoenix/tracing/integrations-tracing/groq) | `openinference-instrumentation-groq` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-groq.svg)](https://pypi.python.org/pypi/openinference-instrumentation-groq) |\\n| [Instructor](https://arize.com/docs/phoenix/tracing/integrations-tracing/instructor) | `openinference-instrumentation-instructor` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-instructor.svg)](https://pypi.python.org/pypi/openinference-instrumentation-instructor) |\\n| [Anthropic](https://arize.com/docs/phoenix/tracing/integrations-tracing/anthropic) | `openinference-instrumentation-anthropic` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-anthropic.svg)](https://pypi.python.org/pypi/openinference-instrumentation-anthropic) |\\n| [Smolagents](https://huggingface.co/docs/smolagents/en/tutorials/inspect_runs) | `openinference-instrumentation-smolagents` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-smolagents.svg)](https://pypi.python.org/pypi/openinference-instrumentation-smolagents) |\\n| [Agno](https://arize.com/docs/phoenix/tracing/integrations-tracing/agno) | `openinference-instrumentation-agno` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-agno.svg)](https://pypi.python.org/pypi/openinference-instrumentation-agno) |\\n| [MCP](https://arize.com/docs/phoenix/tracing/integrations-tracing/model-context-protocol-mcp) | `openinference-instrumentation-mcp` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-mcp.svg)](https://pypi.python.org/pypi/openinference-instrumentation-mcp) |\\n| [Pydantic AI](https://arize.com/docs/phoenix/integrations/pydantic) | `openinference-instrumentation-pydantic-ai` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-pydantic-ai.svg)](https://pypi.python.org/pypi/openinference-instrumentation-pydantic-ai) |\\n| [Autogen AgentChat](https://arize.com/docs/phoenix/integrations/frameworks/autogen/autogen-tracing) | `openinference-instrumentation-autogen-agentchat` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-autogen-agentchat.svg)](https://pypi.python.org/pypi/openinference-instrumentation-autogen-agentchat) |\\n| [Portkey](https://arize.com/docs/phoenix/integrations/portkey) | `openinference-instrumentation-portkey` | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-portkey.svg)](https://pypi.python.org/pypi/openinference-instrumentation-portkey) |\\n\\n## Span Processors\\n\\nNormalize and convert data across other instrumentation libraries by adding span processors that unify data.\\n\\n| Package                                                                                                           | Description                                                      | Version                                                                                                                                                                |\\n| ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [`openinference-instrumentation-openlit`](./python/instrumentation/openinference-instrumentation-openlit)         | OpenInference Span Processor for OpenLIT traces.                 | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-openlit.svg)](https://pypi.python.org/pypi/openinference-instrumentation-openlit)         |\\n| [`openinference-instrumentation-openllmetry`](./python/instrumentation/openinference-instrumentation-openllmetry) | OpenInference Span Processor for OpenLLMetry (Traceloop) traces. | [![PyPI Version](https://img.shields.io/pypi/v/openinference-instrumentation-openllmetry.svg)](https://pypi.python.org/pypi/openinference-instrumentation-openllmetry) |\\n\\n### JavaScript Integrations\\n\\n| Integration                                                                                | Package                                            | Version Badge                                                                                                                                                                       |\\n| ------------------------------------------------------------------------------------------ | -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [OpenAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/openai-node-sdk)      | `@arizeai/openinference-instrumentation-openai`    | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-openai.svg)](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-openai)       |\\n| [LangChain.js](https://arize.com/docs/phoenix/tracing/integrations-tracing/langchain)      | `@arizeai/openinference-instrumentation-langchain` | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-langchain.svg)](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-langchain) |\\n| [Vercel AI SDK](https://arize.com/docs/phoenix/tracing/integrations-tracing/vercel-ai-sdk) | `@arizeai/openinference-vercel`                    | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-vercel)](https://www.npmjs.com/package/@arizeai/openinference-vercel)                                           |\\n| [BeeAI](https://arize.com/docs/phoenix/tracing/integrations-tracing/beeai)                 | `@arizeai/openinference-instrumentation-beeai`     | [![NPM Version](https://img.shields.io/npm/v/@arizeai/openinference-vercel)](https://www.npmjs.com/package/@arizeai/openinference-instrumentation-beeai)                            |\\n| [Mastra](https://arize.com/docs/phoenix/integrations/typescript/mastra)                    | `@mastra/arize`                                     | [![NPM Version](https://img.shields.io/npm/v/@mastra/arize.svg)](https://www.npmjs.com/package/@mastra/arize)                                                                     |\\n\\n### Java Integrations\\n\\n| Integration                                                                                                                       | Package                                     | Version Badge                                                                                                                                                                                                 |\\n| --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [LangChain4j](https://github.com/Arize-ai/openinference/tree/main/java/instrumentation/openinference-instrumentation-langchain4j) | `openinference-instrumentation-langchain4j` | [![Maven Central](https://img.shields.io/maven-central/v/com.arize/openinference-instrumentation-langchain4j.svg)](https://central.sonatype.com/artifact/com.arize/openinference-instrumentation-langchain4j) |\\n| SpringAI                                                                                                                          | `openinference-instrumentation-springAI`    | [![Maven Central](https://img.shields.io/maven-central/v/com.arize/openinference-instrumentation-springAI.svg)](https://central.sonatype.com/artifact/com.arize/openinference-instrumentation-springAI)       |\\n\\n### Platforms\\n\\n| Platform                                                                                                 | Description                                                    | Docs                                                                                                              |\\n| -------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |\\n| [BeeAI](https://docs.beeai.dev/observability/agents-traceability)                                        | AI agent framework with built-in observability                 | [Integration Guide](https://docs.beeai.dev/observability/agents-traceability)                                     |\\n| [Dify](https://docs.dify.ai/en/guides/monitoring/integrate-external-ops-tools/integrate-phoenix)         | Open-source LLM app development platform                       | [Integration Guide](https://docs.dify.ai/en/guides/monitoring/integrate-external-ops-tools/integrate-phoenix)     |\\n| [Envoy AI Gateway](https://github.com/envoyproxy/ai-gateway)                                             | AI Gateway built on Envoy Proxy for AI workloads               | [Integration Guide](https://github.com/envoyproxy/ai-gateway/tree/main/cmd/aigw#opentelemetry-setup-with-phoenix) |\\n| [LangFlow](https://arize.com/docs/phoenix/tracing/integrations-tracing/langflow)                         | Visual framework for building multi-agent and RAG applications | [Integration Guide](https://arize.com/docs/phoenix/tracing/integrations-tracing/langflow)                         |\\n| [LiteLLM Proxy](https://docs.litellm.ai/docs/observability/phoenix_integration#using-with-litellm-proxy) | Proxy server for LLMs                                          | [Integration Guide](https://docs.litellm.ai/docs/observability/phoenix_integration#using-with-litellm-proxy)      |\\n\\n## Community\\n\\nJoin our community to connect with thousands of AI builders.\\n\\n- \\ud83c\\udf0d Join our [Slack community](https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg?__hstc=259489365.a667dfafcfa0169c8aee4178d115dc81.1733501603539.1733501603539.1733501603539.1&__hssc=259489365.1.1733501603539&__hsfp=3822854628&submissionGuid=381a0676-8f38-437b-96f2-fc10875658df#/shared-invite/email).\\n- \\ud83d\\udcda Read our [documentation](https://arize.com/docs/phoenix).\\n- \\ud83d\\udca1 Ask questions and provide feedback in the _#phoenix-support_ channel.\\n- \\ud83c\\udf1f Leave a star on our [GitHub](https://github.com/Arize-ai/phoenix).\\n- \\ud83d\\udc1e Report bugs with [GitHub Issues](https://github.com/Arize-ai/phoenix/issues).\\n- \\ud835\\udd4f Follow us on [\\ud835\\udd4f](https://twitter.com/ArizePhoenix).\\n- \\ud83d\\uddfa\\ufe0f Check out our [roadmap](https://github.com/orgs/Arize-ai/projects/45) to see where we're heading next.\\n- \\ud83e\\uddd1\\u200d\\ud83c\\udfeb Deep dive into everything [Agents](http://arize.com/ai-agents/) and [LLM Evaluations](https://arize.com/llm-evaluation) on Arize's Learning Hubs.\\n\\n## Breaking Changes\\n\\nSee the [migration guide](./MIGRATION.md) for a list of breaking changes.\\n\\n## Copyright, Patent, and License\\n\\nCopyright 2025 Arize AI, Inc. All Rights Reserved.\\n\\nPortions of this code are patent protected by one or more U.S. Patents. See the [IP_NOTICE](https://github.com/Arize-ai/phoenix/blob/main/IP_NOTICE).\\n\\nThis software is licensed under the terms of the Elastic License 2.0 (ELv2). See [LICENSE](https://github.com/Arize-ai/phoenix/blob/main/LICENSE).\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"AI Observability and Evaluation\", \"latest_version\": \"12.15.0\", \"weekly_downloads\": 809229, \"description_cleaned\": \"Phoenix is an open-source AI observability platform designed for experimentation, evaluation, and troubleshooting. It provides:\\nTracing\\n- Trace your LLM application's runtime using OpenTelemetry-based instrumentation.\\nEvaluation\\n- Leverage LLMs to benchmark your application's performance using response and retrieval evals.\\nDatasets\\n- Create versioned datasets of examples for experimentation, evaluation, and fine-tuning.\\nExperiments\\n- Track and evaluate changes to prompts, LLMs, and retrieval.\\nPlayground\\n- Optimize prompts, compare models, adjust parameters, and replay traced LLM calls.\\nPrompt Management\\n- Manage and test prompt changes systematically using version control, tagging, and experimentation.\\nPhoenix is vendor and language agnostic with out-of-the-box support for popular frameworks (\\ud83e\\udd99\\nLlamaIndex\\n, \\ud83e\\udd9c\\u26d3\\nLangChain\\n,\\nHaystack\\n, \\ud83e\\udde9\\nDSPy\\n, \\ud83e\\udd17\\nsmolagents\\n) and LLM providers (\\nOpenAI\\n,\\nBedrock\\n,\\nMistralAI\\n,\\nVertexAI\\n,\\nLiteLLM\\n,\\nGoogle GenAI\\nand more). For details on auto-instrumentation, check out the\\nOpenInference\\nproject.\\nPhoenix runs practically anywhere, including your local machine, a Jupyter notebook, a containerized deployment, or in the cloud.\\nInstallation\\nInstall Phoenix via\\npip\\nor\\nconda\\npip\\ninstall\\narize-phoenix\\nPhoenix container images are available via\\nDocker Hub\\nand can be deployed using Docker or Kubernetes. Arize AI also provides cloud instances at\\napp.phoenix.arize.com\\n.\\nPackages\\nThe\\narize-phoenix\\npackage includes the entire Phoenix platfom. However if you have deployed the Phoenix platform, there are light-weight Python sub-packages and TypeScript packages that can be used in conjunction with the platfrom.\\nPython Subpackages\\nPackage\\nVersion & Docs\\nDescription\\narize-phoenix-otel\\nProvides a lightweight wrapper around OpenTelemetry primitives with Phoenix-aware defaults\\narize-phoenix-client\\nLightweight client for interacting with the Phoenix server via its OpenAPI REST interface\\narize-phoenix-evals\\nTooling to evaluate LLM applications including RAG relevance, answer relevance, and more\\nTypeScript Subpackages\\nPackage\\nVersion & Docs\\nDescription\\n@arizeai/phoenix-otel\\nProvides a lightweight wrapper around OpenTelemetry primitives with Phoenix-aware defaults\\n@arizeai/phoenix-client\\nClient for the Arize Phoenix API\\n@arizeai/phoenix-evals\\nTypeScript evaluation library for LLM applications (alpha release)\\n@arizeai/phoenix-mcp\\nMCP server implementation for Arize Phoenix providing unified interface to Phoenix's capabilities\\nTracing Integrations\\nPhoenix is built on top of OpenTelemetry and is vendor, language, and framework agnostic. For details about tracing integrations and example applications, see the\\nOpenInference\\nproject.\\nPython Integrations\\nIntegration\\nPackage\\nVersion Badge\\nOpenAI\\nopeninference-instrumentation-openai\\nOpenAI Agents\\nopeninference-instrumentation-openai-agents\\nLlamaIndex\\nopeninference-instrumentation-llama-index\\nDSPy\\nopeninference-instrumentation-dspy\\nAWS Bedrock\\nopeninference-instrumentation-bedrock\\nLangChain\\nopeninference-instrumentation-langchain\\nMistralAI\\nopeninference-instrumentation-mistralai\\nGoogle GenAI\\nopeninference-instrumentation-google-genai\\nGoogle ADK\\nopeninference-instrumentation-google-adk\\nGuardrails\\nopeninference-instrumentation-guardrails\\nVertexAI\\nopeninference-instrumentation-vertexai\\nCrewAI\\nopeninference-instrumentation-crewai\\nHaystack\\nopeninference-instrumentation-haystack\\nLiteLLM\\nopeninference-instrumentation-litellm\\nGroq\\nopeninference-instrumentation-groq\\nInstructor\\nopeninference-instrumentation-instructor\\nAnthropic\\nopeninference-instrumentation-anthropic\\nSmolagents\\nopeninference-instrumentation-smolagents\\nAgno\\nopeninference-instrumentation-agno\\nMCP\\nopeninference-instrumentation-mcp\\nPydantic AI\\nopeninference-instrumentation-pydantic-ai\\nAutogen AgentChat\\nopeninference-instrumentation-autogen-agentchat\\nPortkey\\nopeninference-instrumentation-portkey\\nSpan Processors\\nNormalize and convert data across other instrumentation libraries by adding span processors that unify data.\\nPackage\\nDescription\\nVersion\\nopeninference-instrumentation-openlit\\nOpenInference Span Processor for OpenLIT traces.\\nopeninference-instrumentation-openllmetry\\nOpenInference Span Processor for OpenLLMetry (Traceloop) traces.\\nJavaScript Integrations\\nIntegration\\nPackage\\nVersion Badge\\nOpenAI\\n@arizeai/openinference-instrumentation-openai\\nLangChain.js\\n@arizeai/openinference-instrumentation-langchain\\nVercel AI SDK\\n@arizeai/openinference-vercel\\nBeeAI\\n@arizeai/openinference-instrumentation-beeai\\nMastra\\n@mastra/arize\\nJava Integrations\\nIntegration\\nPackage\\nVersion Badge\\nLangChain4j\\nopeninference-instrumentation-langchain4j\\nSpringAI\\nopeninference-instrumentation-springAI\\nPlatforms\\nPlatform\\nDescription\\nDocs\\nBeeAI\\nAI agent framework with built-in observability\\nIntegration Guide\\nDify\\nOpen-source LLM app development platform\\nIntegration Guide\\nEnvoy AI Gateway\\nAI Gateway built on Envoy Proxy for AI workloads\\nIntegration Guide\\nLangFlow\\nVisual framework for building multi-agent and RAG applications\\nIntegration Guide\\nLiteLLM Proxy\\nProxy server for LLMs\\nIntegration Guide\\nCommunity\\nJoin our community to connect with thousands of AI builders.\\n\\ud83c\\udf0d Join our\\nSlack community\\n.\\n\\ud83d\\udcda Read our\\ndocumentation\\n.\\n\\ud83d\\udca1 Ask questions and provide feedback in the\\n#phoenix-support\\nchannel.\\n\\ud83c\\udf1f Leave a star on our\\nGitHub\\n.\\n\\ud83d\\udc1e Report bugs with\\nGitHub Issues\\n.\\n\\ud835\\udd4f Follow us on\\n\\ud835\\udd4f\\n.\\n\\ud83d\\uddfa\\ufe0f Check out our\\nroadmap\\nto see where we're heading next.\\n\\ud83e\\uddd1\\u200d\\ud83c\\udfeb Deep dive into everything\\nAgents\\nand\\nLLM Evaluations\\non Arize's Learning Hubs.\\nBreaking Changes\\nSee the\\nmigration guide\\nfor a list of breaking changes.\\nCopyright, Patent, and License\\nCopyright 2025 Arize AI, Inc. All Rights Reserved.\\nPortions of this code are patent protected by one or more U.S. Patents. See the\\nIP_NOTICE\\n.\\nThis software is licensed under the terms of the Elastic License 2.0 (ELv2). See\\nLICENSE\\n.\"}, {\"name\": \"dagster-graphql\", \"description\": \"\", \"description_content_type\": null, \"summary\": \"The GraphQL frontend to python dagster.\", \"latest_version\": \"1.12.2\", \"weekly_downloads\": 807586, \"description_cleaned\": \"\"}, {\"name\": \"zict\", \"description\": \"\\nMutable Mapping tools. See `documentation`_.\\n\\n.. _documentation: http://zict.readthedocs.io/en/latest/\\n\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"Mutable mapping tools\", \"latest_version\": \"3.0.0\", \"weekly_downloads\": 807058, \"description_cleaned\": \"Mutable Mapping tools. See\\ndocumentation\\n.\"}, {\"name\": \"pip-api\", \"description\": \"\\n<!--- BADGES: START --->\\n[![GitHub - License](https://img.shields.io/github/license/di/pip-api?logo=github&style=flat&color=green)][#github-license]\\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pip-api?logo=pypi&style=flat&color=blue)][#pypi-package]\\n[![PyPI - Package Version](https://img.shields.io/pypi/v/pip-api?logo=pypi&style=flat&color=orange)][#pypi-package]\\n[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/pip-api?logo=anaconda&style=flat)][#conda-forge-package]\\n[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/pip-api?logo=anaconda&style=flat&color=orange)][#conda-forge-package]\\n[![Conda Recipe](https://img.shields.io/static/v1?logo=conda-forge&style=flat&color=green&label=recipe&message=pip-api)][#conda-forge-feedstock]\\n\\n[#github-license]: https://github.com/di/pip-api/blob/master/LICENSE\\n[#pypi-package]: https://pypi.org/project/pip-api/\\n[#conda-forge-package]: https://anaconda.org/conda-forge/pip-api\\n[#conda-forge-feedstock]: https://github.com/conda-forge/pip-api-feedstock\\n<!--- BADGES: END --->\\n\\nSince [`pip`](https://pypi.org/p/pip) is a command-line-tool, [it does not have\\nan official, supported, _importable_\\nAPI](https://pip.pypa.io/en/latest/user_guide/#using-pip-from-your-program).\\n\\nHowever, this does not mean that people haven't tried to `import pip`, usually\\nto end up with much headache when `pip`'s maintainers do routine refactoring.\\n\\n## Goal\\nThe goal of this project is to provide an importable `pip` API, which is _fully\\ncompliant_ with the recommended method of using `pip` from your program.\\n\\nHow? By providing an importable API that wraps command-line calls to `pip`,\\nthis library can be used as a drop-in replacement for existing uses of `pip`'s\\ninternal API.\\n\\n### Scope\\nThis goal means that any new API added here must have the following\\nequivalents:\\n\\n- some internal `pip` API (or combination of internal APIs)\\n- some CLI calls (or combination of CLI calls)\\n\\nAny functionality that is not currently possible from internal `pip` API or\\nCLI calls is out of scope.\\n\\n## Installation\\n\\nYou can install `pip-api` with either `pip` or with `conda`.\\n\\n**With pip**:\\n\\n```sh\\npython -m pip install pip-api\\n```\\n\\n**With conda**:\\n\\n```sh\\nconda install -c conda-forge pip-api\\n```\\n\\n## Supported Commands\\n\\nNot all commands are supported in all versions of `pip` and on all platforms.\\nIf the command you are trying to use is not compatible, `pip_api` will raise a\\n`pip_api.exceptions.Incompatible` exception for your program to catch.\\n\\n### Available with all `pip` versions:\\n* `pip_api.version()`\\n  > Returns the `pip` version as a string, e.g. `\\\"9.0.1\\\"`\\n\\n* `pip_api.installed_distributions(local=False)`\\n  > Returns a list of all installed distributions as a `Distribution` object with the following attributes:\\n  > * `Distribution.name` (`string`): The name of the installed distribution\\n  > * `Distribution.version` ([`packaging.version.Version`](https://packaging.pypa.io/en/latest/version/#packaging.version.Version)): The version of the installed distribution\\n  > * `Distribution.location` (`string`): The location of the installed distribution\\n  > * `Distribution.editable` (`bool`): Whether the distribution is editable or not\\n  > Optionally takes a `local` parameter to filter out globally-installed packages\\n\\n* `pip_api.parse_requirements(filename, options=None, include_invalid=False, strict_hashes=False)`\\n  > Takes a path to a filename of a Requirements file. Returns a mapping from package name to a `pip_api.Requirement` object (subclass of [`packaging.requirements.Requirement`](https://packaging.pypa.io/en/latest/requirements/#packaging.requirements.Requirement)) with the following attributes:\\n  > * `Requirement.name` (`string`): The name of the requirement.\\n  > * `Requirement.extras` (`set`): A set of extras that the requirement specifies.\\n  > * `Requirement.specifier` ([`packaging.specifiers.SpecifierSet`](https://packaging.pypa.io/en/latest/specifiers/#packaging.specifiers.SpecifierSet)): A `SpecifierSet` of the version specified by the requirement.\\n  > * `Requirement.marker` ([`packaging.markers.Marker`](https://packaging.pypa.io/en/latest/markers/#packaging.markers.Marker)): A `Marker` of the marker for the requirement. Can be `None`.\\n  > * `Requirement.hashes` (`dict`): A mapping of hashes for the requirement, corresponding to `--hash=...` options.\\n  > * `Requirement.editable` (`bool`): Whether the requirement is editable, corresponding to `-e ...`\\n  > * `Requirement.filename` (`str`): The filename that the requirement originates from.\\n  > * `Requirement.lineno` (`int`): The source line that the requirement was parsed from.\\n  >\\n  > Optionally takes an `options` parameter to override the regex used to skip requirements lines.\\n  > Optionally takes an `include_invalid` parameter to return an `UnparsedRequirement` in the event that a requirement cannot be parsed correctly.\\n  > Optionally takes a `strict_hashes` parameter to require that all requirements have associated hashes.\\n\\n### Available with `pip>=8.0.0`:\\n* `pip_api.hash(filename, algorithm='sha256')`\\n  > Returns the resulting as a string.\\n  > Valid `algorithm` parameters are `'sha256'`, `'sha384'`, and `'sha512'`\\n\\n### Available with `pip>=19.2`:\\n* `pip_api.installed_distributions(local=False, paths=[])`\\n  > As described above, but with an extra optional `paths` parameter to provide a list of locations to look for installed distributions. Attempting to use the `paths` parameter with `pip<19.2` will result in a `PipError`.\\n\\n## Use cases\\nThis library is in use by a number of other tools, including:\\n* [`pip-audit`](https://pypi.org/project/pip-audit/), to analyze dependencies for known vulnerabilities\\n* [`pytest-reqs`](https://pypi.org/project/pytest-reqs), to compare requirements files with test dependencies\\n* [`hashin`](https://pypi.org/project/hashin/), to add hash pinning to requirements files\\n* ...and many more.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"An unofficial, importable pip API\", \"latest_version\": \"0.0.34\", \"weekly_downloads\": 805357, \"description_cleaned\": \"Since\\npip\\nis a command-line-tool,\\nit does not have\\nan official, supported,\\nimportable\\nAPI\\n.\\nHowever, this does not mean that people haven't tried to\\nimport pip\\n, usually\\nto end up with much headache when\\npip\\n's maintainers do routine refactoring.\\nGoal\\nThe goal of this project is to provide an importable\\npip\\nAPI, which is\\nfully\\ncompliant\\nwith the recommended method of using\\npip\\nfrom your program.\\nHow? By providing an importable API that wraps command-line calls to\\npip\\n,\\nthis library can be used as a drop-in replacement for existing uses of\\npip\\n's\\ninternal API.\\nScope\\nThis goal means that any new API added here must have the following\\nequivalents:\\nsome internal\\npip\\nAPI (or combination of internal APIs)\\nsome CLI calls (or combination of CLI calls)\\nAny functionality that is not currently possible from internal\\npip\\nAPI or\\nCLI calls is out of scope.\\nInstallation\\nYou can install\\npip-api\\nwith either\\npip\\nor with\\nconda\\n.\\nWith pip\\n:\\npython\\n-m\\npip\\ninstall\\npip-api\\nWith conda\\n:\\nconda\\ninstall\\n-c\\nconda-forge\\npip-api\\nSupported Commands\\nNot all commands are supported in all versions of\\npip\\nand on all platforms.\\nIf the command you are trying to use is not compatible,\\npip_api\\nwill raise a\\npip_api.exceptions.Incompatible\\nexception for your program to catch.\\nAvailable with all\\npip\\nversions:\\npip_api.version()\\nReturns the\\npip\\nversion as a string, e.g.\\n\\\"9.0.1\\\"\\npip_api.installed_distributions(local=False)\\nReturns a list of all installed distributions as a\\nDistribution\\nobject with the following attributes:\\nDistribution.name\\n(\\nstring\\n): The name of the installed distribution\\nDistribution.version\\n(\\npackaging.version.Version\\n): The version of the installed distribution\\nDistribution.location\\n(\\nstring\\n): The location of the installed distribution\\nDistribution.editable\\n(\\nbool\\n): Whether the distribution is editable or not\\nOptionally takes a\\nlocal\\nparameter to filter out globally-installed packages\\npip_api.parse_requirements(filename, options=None, include_invalid=False, strict_hashes=False)\\nTakes a path to a filename of a Requirements file. Returns a mapping from package name to a\\npip_api.Requirement\\nobject (subclass of\\npackaging.requirements.Requirement\\n) with the following attributes:\\nRequirement.name\\n(\\nstring\\n): The name of the requirement.\\nRequirement.extras\\n(\\nset\\n): A set of extras that the requirement specifies.\\nRequirement.specifier\\n(\\npackaging.specifiers.SpecifierSet\\n): A\\nSpecifierSet\\nof the version specified by the requirement.\\nRequirement.marker\\n(\\npackaging.markers.Marker\\n): A\\nMarker\\nof the marker for the requirement. Can be\\nNone\\n.\\nRequirement.hashes\\n(\\ndict\\n): A mapping of hashes for the requirement, corresponding to\\n--hash=...\\noptions.\\nRequirement.editable\\n(\\nbool\\n): Whether the requirement is editable, corresponding to\\n-e ...\\nRequirement.filename\\n(\\nstr\\n): The filename that the requirement originates from.\\nRequirement.lineno\\n(\\nint\\n): The source line that the requirement was parsed from.\\nOptionally takes an\\noptions\\nparameter to override the regex used to skip requirements lines.\\nOptionally takes an\\ninclude_invalid\\nparameter to return an\\nUnparsedRequirement\\nin the event that a requirement cannot be parsed correctly.\\nOptionally takes a\\nstrict_hashes\\nparameter to require that all requirements have associated hashes.\\nAvailable with\\npip>=8.0.0\\n:\\npip_api.hash(filename, algorithm='sha256')\\nReturns the resulting as a string.\\nValid\\nalgorithm\\nparameters are\\n'sha256'\\n,\\n'sha384'\\n, and\\n'sha512'\\nAvailable with\\npip>=19.2\\n:\\npip_api.installed_distributions(local=False, paths=[])\\nAs described above, but with an extra optional\\npaths\\nparameter to provide a list of locations to look for installed distributions. Attempting to use the\\npaths\\nparameter with\\npip<19.2\\nwill result in a\\nPipError\\n.\\nUse cases\\nThis library is in use by a number of other tools, including:\\npip-audit\\n, to analyze dependencies for known vulnerabilities\\npytest-reqs\\n, to compare requirements files with test dependencies\\nhashin\\n, to add hash pinning to requirements files\\n...and many more.\"}, {\"name\": \"channels\", \"description\": \"Django Channels\\n===============\\n\\n.. image:: https://github.com/django/channels/workflows/Tests/badge.svg?branch=master\\n    :target: https://github.com/django/channels/actions\\n\\n.. image:: https://readthedocs.org/projects/channels/badge/?version=latest\\n    :target: https://channels.readthedocs.io/en/latest/?badge=latest\\n\\n.. image:: https://img.shields.io/pypi/v/channels.svg\\n    :target: https://pypi.python.org/pypi/channels\\n\\n.. image:: https://img.shields.io/pypi/l/channels.svg\\n    :target: https://pypi.python.org/pypi/channels\\n\\nChannels augments Django to bring WebSocket, long-poll HTTP,\\ntask offloading and other async support to your code, using familiar Django\\ndesign patterns and a flexible underlying framework that lets you not only\\ncustomize behaviours but also write support for your own protocols and needs.\\n\\nDocumentation, installation and getting started instructions are at\\nhttps://channels.readthedocs.io\\n\\nChannels is an official Django Project and as such has a deprecation policy.\\nDetails about what's deprecated or pending deprecation for each release is in\\nthe `release notes <https://channels.readthedocs.io/en/latest/releases/index.html>`_.\\n\\nSupport can be obtained through several locations - see our\\n`support docs <https://channels.readthedocs.io/en/latest/support.html>`_ for more.\\n\\nYou can install channels from PyPI as the ``channels`` package.\\nSee our `installation <https://channels.readthedocs.io/en/latest/installation.html>`_\\nand `tutorial <https://channels.readthedocs.io/en/latest/tutorial/index.html>`_ docs for more.\\n\\nDependencies\\n------------\\n\\nAll Channels projects currently support Python 3.9 and up. ``channels`` is\\ncompatible with Django 4.2+.\\n\\n\\nContributing\\n------------\\n\\nTo learn more about contributing, please `read our contributing docs <https://channels.readthedocs.io/en/latest/contributing.html>`_.\\n\\n\\nMaintenance and Security\\n------------------------\\n\\nTo report security issues, please contact security@djangoproject.com. For GPG\\nsignatures and more security process information, see\\nhttps://docs.djangoproject.com/en/dev/internals/security/.\\n\\nTo report bugs or request new features, please open a new GitHub issue. For\\nlarger discussions, please post to the\\n`django-developers mailing list <https://groups.google.com/d/forum/django-developers>`_.\\n\\nMaintenance is overseen by Carlton Gibson with help from others. It is a\\nbest-effort basis - we unfortunately can only dedicate guaranteed time to fixing\\nsecurity holes.\\n\\nIf you are interested in joining the maintenance team, please\\n`read more about contributing <https://channels.readthedocs.io/en/latest/contributing.html>`_\\nand get in touch!\\n\\n\\nOther Projects\\n--------------\\n\\nThe Channels project is made up of several packages; the others are:\\n\\n* `Daphne <https://github.com/django/daphne/>`_, the HTTP and Websocket termination server\\n* `channels_redis <https://github.com/django/channels_redis/>`_, the Redis channel backend\\n* `asgiref <https://github.com/django/asgiref/>`_, the base ASGI library/memory backend\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"Brings async, event-driven capabilities to Django.\", \"latest_version\": \"4.3.1\", \"weekly_downloads\": 804743, \"description_cleaned\": \"Channels augments Django to bring WebSocket, long-poll HTTP,\\ntask offloading and other async support to your code, using familiar Django\\ndesign patterns and a flexible underlying framework that lets you not only\\ncustomize behaviours but also write support for your own protocols and needs.\\nDocumentation, installation and getting started instructions are at\\nhttps://channels.readthedocs.io\\nChannels is an official Django Project and as such has a deprecation policy.\\nDetails about what\\u2019s deprecated or pending deprecation for each release is in\\nthe\\nrelease notes\\n.\\nSupport can be obtained through several locations - see our\\nsupport docs\\nfor more.\\nYou can install channels from PyPI as the\\nchannels\\npackage.\\nSee our\\ninstallation\\nand\\ntutorial\\ndocs for more.\\nDependencies\\nAll Channels projects currently support Python 3.9 and up.\\nchannels\\nis\\ncompatible with Django 4.2+.\\nContributing\\nTo learn more about contributing, please\\nread our contributing docs\\n.\\nMaintenance and Security\\nTo report security issues, please contact\\nsecurity\\n@\\ndjangoproject\\n.\\ncom\\n. For GPG\\nsignatures and more security process information, see\\nhttps://docs.djangoproject.com/en/dev/internals/security/\\n.\\nTo report bugs or request new features, please open a new GitHub issue. For\\nlarger discussions, please post to the\\ndjango-developers mailing list\\n.\\nMaintenance is overseen by Carlton Gibson with help from others. It is a\\nbest-effort basis - we unfortunately can only dedicate guaranteed time to fixing\\nsecurity holes.\\nIf you are interested in joining the maintenance team, please\\nread more about contributing\\nand get in touch!\\nOther Projects\\nThe Channels project is made up of several packages; the others are:\\nDaphne\\n, the HTTP and Websocket termination server\\nchannels_redis\\n, the Redis channel backend\\nasgiref\\n, the base ASGI library/memory backend\"}, {\"name\": \"kornia\", \"description\": \"<div align=\\\"center\\\">\\n<p align=\\\"center\\\">\\n  <img width=\\\"55%\\\" src=\\\"https://github.com/kornia/data/raw/main/kornia_banner_pixie.png\\\" />\\n</p>\\n\\n---\\n\\nEnglish | [\\u7b80\\u4f53\\u4e2d\\u6587](README_zh-CN.md)\\n\\n<!-- prettier-ignore -->\\n<a href=\\\"https://kornia.readthedocs.io\\\">Docs</a> \\u2022\\n<a href=\\\"https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/nbs/hello_world_tutorial.ipynb\\\">Try it Now</a> \\u2022\\n<a href=\\\"https://kornia.github.io/tutorials/\\\">Tutorials</a> \\u2022\\n<a href=\\\"https://github.com/kornia/kornia-examples\\\">Examples</a> \\u2022\\n<a href=\\\"https://kornia.github.io//kornia-blog\\\">Blog</a> \\u2022\\n<a href=\\\"https://discord.gg/HfnywwpBnD\\\">Community</a>\\n\\n[![PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/kornia)\\n[![Downloads](https://static.pepy.tech/badge/kornia)](https://pepy.tech/project/kornia)\\n[![star](https://gitcode.com/kornia/kornia/star/badge.svg)](https://gitcode.com/kornia/kornia)\\n[![Discord](https://img.shields.io/badge/Discord-5865F2?logo=discord&logoColor=white)](https://discord.gg/HfnywwpBnD)\\n[![Twitter](https://img.shields.io/twitter/follow/kornia_foss?style=social)](https://twitter.com/kornia_foss)\\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENCE)\\n\\n</p>\\n</div>\\n\\n**Kornia** is a differentiable computer vision library that provides a rich set of differentiable image processing and geometric vision algorithms. Built on top of [PyTorch](https://pytorch.org), Kornia integrates seamlessly into existing AI workflows, allowing you to leverage powerful [batch transformations](), [auto-differentiation]() and [GPU acceleration](). Whether you\\u2019re working on image transformations, augmentations, or AI-driven image processing, Kornia equips you with the tools you need to bring your ideas to life.\\n\\n## Key Components\\n1. **Differentiable Image Processing**<br>\\n  Kornia provides a comprehensive suite of image processing operators, all differentiable and ready to integrate into deep learning pipelines.\\n    - **Filters**: Gaussian, Sobel, Median, Box Blur, etc.\\n    - **Transformations**: Affine, Homography, Perspective, etc.\\n    - **Enhancements**: Histogram Equalization, CLAHE, Gamma Correction, etc.\\n    - **Edge Detection**: Canny, Laplacian, Sobel, etc.\\n    - ... check our [docs](https://kornia.readthedocs.io) for more.\\n2. **Advanced Augmentations**<br>\\nPerform powerful data augmentation with Kornia\\u2019s built-in functions, ideal for training AI models with complex augmentation pipelines.\\n    - **Augmentation Pipeline**: AugmentationSequential, PatchSequential, VideoSequential, etc.\\n    - **Automatic Augmentation**: AutoAugment, RandAugment, TrivialAugment.\\n3. **AI Models**<br>\\nLeverage pre-trained AI models optimized for a variety of vision tasks, all within the Kornia ecosystem.\\n    - **Face Detection**: YuNet\\n    - **Feature Matching**: LoFTR, LightGlue\\n    - **Feature Descriptor**: DISK, DeDoDe, SOLD2\\n    - **Segmentation**: SAM\\n    - **Classification**: MobileViT, VisionTransformer.\\n\\n<details>\\n<summary>See here for some of the methods that we support! (>500 ops in total !)</summary>\\n\\n| **Category**               | **Methods/Models**                                                                                                   |\\n|----------------------------|---------------------------------------------------------------------------------------------------------------------|\\n| **Image Processing**        | - Color conversions (RGB, Grayscale, HSV, etc.)<br>- Geometric transformations (Affine, Homography, Resizing, etc.)<br>- Filtering (Gaussian blur, Median blur, etc.)<br>- Edge detection (Sobel, Canny, etc.)<br>- Morphological operations (Erosion, Dilation, etc.)                                 |\\n| **Augmentation**            | - Random cropping, Erasing<br> - Random geometric transformations (Affine, flipping, Fish Eye, Perspecive, Thin plate spline, Elastic)<br>- Random noises (Gaussian, Median, Motion, Box, Rain, Snow, Salt and Pepper)<br>- Random color jittering (Contrast, Brightness, CLAHE, Equalize, Gamma, Hue, Invert, JPEG, Plasma, Posterize, Saturation, Sharpness, Solarize)<br> - Random MixUp, CutMix, Mosaic, Transplantation, etc.                  |\\n| **Feature Detection**       | - Detector (Harris, GFTT, Hessian, DoG, KeyNet, DISK and DeDoDe)<br> - Descriptor (SIFT, HardNet, TFeat, HyNet, SOSNet, and LAFDescriptor)<br>- Matching (nearest neighbor, mutual nearest neighbor, geometrically aware matching, AdaLAM LightGlue, and LoFTR)                    |\\n| **Geometry**                | - Camera models and calibration<br>- Stereo vision (epipolar geometry, disparity, etc.)<br>- Homography estimation<br>- Depth estimation from disparity<br>- 3D transformations                |\\n| **Deep Learning Layers**    | - Custom convolution layers<br>- Recurrent layers for vision tasks<br>- Loss functions (e.g., SSIM, PSNR, etc.)<br>- Vision-specific optimizers                                        |\\n| **Photometric Functions**   | - Photometric loss functions<br>- Photometric augmentations                                                                                           |\\n| **Filtering**               | - Bilateral filtering<br>- DexiNed<br>- Dissolving<br>- Guided Blur<br>- Laplacian<br>- Gaussian<br>- Non-local means<br>- Sobel<br>- Unsharp masking                                                                                            |\\n| **Color**                   | - Color space conversions<br>- Brightness/contrast adjustment<br>- Gamma correction                                                                       |\\n| **Stereo Vision**           | - Disparity estimation<br>- Depth estimation<br>- Rectification                                                                                           |\\n| **Image Registration**      | - Affine and homography-based registration<br>- Image alignment using feature matching                                                                     |\\n| **Pose Estimation**         | - Essential and Fundamental matrix estimation<br>- PnP problem solvers<br>- Pose refinement                                                                |\\n| **Optical Flow**            | - Farneback optical flow<br>- Dense optical flow<br>- Sparse optical flow                                                                                  |\\n| **3D Vision**               | - Depth estimation<br>- Point cloud operations<br>- Nerf<br>                                                                |\\n| **Image Denoising**         | - Gaussian noise removal<br>- Poisson noise removal                                                                                                        |\\n| **Edge Detection**          | - Sobel operator<br>- Canny edge detection                                                                                                                 |                                               |\\n| **Transformations**         | - Rotation<br>- Translation<br>- Scaling<br>- Shearing                                                                                                     |\\n| **Loss Functions**          | - SSIM (Structural Similarity Index Measure)<br>- PSNR (Peak Signal-to-Noise Ratio)<br>- Cauchy<br>- Charbonnier<br>- Depth Smooth<br>- Dice<br>- Hausdorff<br>- Tversky<br>- Welsch<br>                                   |                                                                                             |\\n| **Morphological Operations**| - Dilation<br>- Erosion<br>- Opening<br>- Closing                                                                                                          |\\n\\n</details>\\n\\n## Sponsorship\\n\\nKornia is an open-source project that is developed and maintained by volunteers. Whether you're using it for research or commercial purposes, consider sponsoring or collaborating with us. Your support will help ensure Kornia's growth and ongoing innovation. Reach out to us today and be a part of shaping the future of this exciting initiative!\\n\\n<a href=\\\"https://opencollective.com/kornia/donate\\\" target=\\\"_blank\\\">\\n  <img src=\\\"https://opencollective.com/webpack/donate/button@2x.png?color=blue\\\" width=300 />\\n</a>\\n\\n## Installation\\n\\n[![PyPI python](https://img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia)\\n[![pytorch](https://img.shields.io/badge/PyTorch_1.9.1+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)\\n\\n### From pip\\n\\n  ```bash\\n  pip install kornia\\n  ```\\n\\n<details>\\n  <summary>Other installation options</summary>\\n\\n#### From source with editable mode\\n\\n  ```bash\\n  pip install -e .\\n  ```\\n\\n#### For development with UV (Recommended)\\n\\nFor development, Kornia uses [uv](https://github.com/astral-sh/uv) for fast Python package management and virtual environment creation. The project includes a `uv.lock` file for reproducible dependency management.\\n\\n  ```bash\\n  ./setup_dev_env.sh\\n  ```\\n\\nThis will set up a complete development environment with all dependencies using the lock file for reproducibility. For more details on dependency management and lock file usage, see [CONTRIBUTING.md](CONTRIBUTING.md).\\n\\n#### From Github url (latest version)\\n\\n  ```bash\\n  pip install git+https://github.com/kornia/kornia\\n  ```\\n\\n</details>\\n\\n## Quick Start\\n\\nKornia is not just another computer vision library \\u2014 it's your gateway to effortless Computer Vision and AI.\\n\\n<details>\\n<summary>Get started with Kornia image transformation and augmentation!</summary>\\n\\n```python\\nimport numpy as np\\nimport kornia_rs as kr\\n\\nfrom kornia.augmentation import AugmentationSequential, RandomAffine, RandomBrightness\\nfrom kornia.filters import StableDiffusionDissolving\\n\\n# Load and prepare your image\\nimg: np.ndarray = kr.read_image_any(\\\"img.jpeg\\\")\\nimg = kr.resize(img, (256, 256), interpolation=\\\"bilinear\\\")\\n\\n# alternatively, load image with PIL\\n# img = Image.open(\\\"img.jpeg\\\").resize((256, 256))\\n# img = np.array(img)\\n\\nimg = np.stack([img] * 2)  # batch images\\n\\n# Define an augmentation pipeline\\naugmentation_pipeline = AugmentationSequential(\\n    RandomAffine((-45., 45.), p=1.),\\n    RandomBrightness((0.,1.), p=1.)\\n)\\n\\n# Leveraging StableDiffusion models\\ndslv_op = StableDiffusionDissolving()\\n\\nimg = augmentation_pipeline(img)\\ndslv_op(img, step_number=500)\\n\\ndslv_op.save(\\\"Kornia-enhanced.jpg\\\")\\n```\\n\\n</details>\\n\\n<details>\\n<summary>Find out Kornia ONNX models with ONNXSequential!</summary>\\n\\n```python\\nimport numpy as np\\nfrom kornia.onnx import ONNXSequential\\n# Chain ONNX models from HuggingFace repo and your own local model together\\nonnx_seq = ONNXSequential(\\n    \\\"hf://operators/kornia.geometry.transform.flips.Hflip\\\",\\n    \\\"hf://models/kornia.models.detection.rtdetr_r18vd_640x640\\\",  # Or you may use \\\"YOUR_OWN_MODEL.onnx\\\"\\n)\\n# Prepare some input data\\ninput_data = np.random.randn(1, 3, 384, 512).astype(np.float32)\\n# Perform inference\\noutputs = onnx_seq(input_data)\\n# Print the model outputs\\nprint(outputs)\\n\\n# Export a new ONNX model that chains up all three models together!\\nonnx_seq.export(\\\"chained_model.onnx\\\")\\n```\\n</details>\\n\\n## Multi-framework support\\n\\nYou can now use Kornia with [TensorFlow](https://www.tensorflow.org/), [JAX](https://jax.readthedocs.io/en/latest/index.html), and [NumPy](https://numpy.org/). See [Multi-Framework Support](docs/source/get-started/multi-framework-support.rst) for more details.\\n\\n```python\\nimport kornia\\ntf_kornia = kornia.to_tensorflow()\\n```\\n\\n<p align=\\\"center\\\">\\n  Powered by\\n  <a href=\\\"https://github.com/ivy-llc/ivy\\\" target=\\\"_blank\\\">\\n    <div class=\\\"dark-light\\\" style=\\\"display: block;\\\" align=\\\"center\\\">\\n      <img class=\\\"dark-light\\\" width=\\\"15%\\\" src=\\\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/ivy-long.svg\\\"/>\\n    </div>\\n  </a>\\n</p>\\n\\n## Call For Contributors\\n\\nAre you passionate about computer vision, AI, and open-source development? Join us in shaping the future of Kornia! We are actively seeking contributors to help expand and enhance our library, making it even more powerful, accessible, and versatile. Whether you're an experienced developer or just starting, there's a place for you in our community.\\n\\n### Accessible AI Models\\n\\nWe are excited to announce our latest advancement: a new initiative designed to seamlessly integrate lightweight AI models into Kornia.\\nWe aim to run any models as smooth as big models such as StableDiffusion, to support them well in many perspectives.\\nWe have already included a selection of lightweight AI models like [YuNet (Face Detection)](), [Loftr (Feature Matching)](), and [SAM (Segmentation)](). Now, we're looking for contributors to help us:\\n\\n- Expand the Model Selection: Import decent models into our library. If you are a researcher, Kornia is an excellent place for you to promote your model!\\n- Model Optimization: Work on optimizing models to reduce their computational footprint while maintaining accuracy and performance. You may start from offering ONNX support!\\n- Model Documentation: Create detailed guides and examples to help users get the most out of these models in their projects.\\n\\n\\n### Documentation And Tutorial Optimization\\n\\nKornia's foundation lies in its extensive collection of classic computer vision operators, providing robust tools for image processing, feature extraction, and geometric transformations. We continuously seek for contributors to help us improve our documentation and present nice tutorials to our users.\\n\\n\\n## Cite\\n\\nIf you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](./CITATION.md).\\n\\n  ```bibtex\\n  @inproceedings{eriba2019kornia,\\n    author    = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},\\n    title     = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},\\n    booktitle = {Winter Conference on Applications of Computer Vision},\\n    year      = {2020},\\n    url       = {https://arxiv.org/pdf/1910.02190.pdf}\\n  }\\n  ```\\n\\n## Contributing\\n\\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes. The participation in this open source project is subject to [Code of Conduct](./CODE_OF_CONDUCT.md).\\n\\n## Community\\n- **Discord:** Join our workspace to keep in touch with our core contributors, get latest updates on the industry and  be part of our community. [JOIN HERE](https://discord.gg/HfnywwpBnD)\\n- **GitHub Issues:** bug reports, feature requests, install issues, RFCs, thoughts, etc. [OPEN](https://github.com/kornia/kornia/issues/new/choose)\\n- **Forums:** discuss implementations, research, etc. [GitHub Forums](https://github.com/kornia/kornia/discussions)\\n\\n<a href=\\\"https://github.com/Kornia/kornia/graphs/contributors\\\">\\n  <img src=\\\"https://contrib.rocks/image?repo=Kornia/kornia\\\" width=\\\"60%\\\" />\\n</a>\\n\\nMade with [contrib.rocks](https://contrib.rocks).\\n\\n## License\\n\\nKornia is released under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for more information.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Open Source Differentiable Computer Vision Library for PyTorch\", \"latest_version\": \"0.8.2\", \"weekly_downloads\": 802921, \"description_cleaned\": \"English |\\n\\u7b80\\u4f53\\u4e2d\\u6587\\nDocs\\n\\u2022\\nTry it Now\\n\\u2022\\nTutorials\\n\\u2022\\nExamples\\n\\u2022\\nBlog\\n\\u2022\\nCommunity\\nKornia\\nis a differentiable computer vision library that provides a rich set of differentiable image processing and geometric vision algorithms. Built on top of\\nPyTorch\\n, Kornia integrates seamlessly into existing AI workflows, allowing you to leverage powerful\\nbatch transformations\\n,\\nauto-differentiation\\nand\\nGPU acceleration\\n. Whether you\\u2019re working on image transformations, augmentations, or AI-driven image processing, Kornia equips you with the tools you need to bring your ideas to life.\\nKey Components\\nDifferentiable Image Processing\\nKornia provides a comprehensive suite of image processing operators, all differentiable and ready to integrate into deep learning pipelines.\\nFilters\\n: Gaussian, Sobel, Median, Box Blur, etc.\\nTransformations\\n: Affine, Homography, Perspective, etc.\\nEnhancements\\n: Histogram Equalization, CLAHE, Gamma Correction, etc.\\nEdge Detection\\n: Canny, Laplacian, Sobel, etc.\\n... check our\\ndocs\\nfor more.\\nAdvanced Augmentations\\nPerform powerful data augmentation with Kornia\\u2019s built-in functions, ideal for training AI models with complex augmentation pipelines.\\nAugmentation Pipeline\\n: AugmentationSequential, PatchSequential, VideoSequential, etc.\\nAutomatic Augmentation\\n: AutoAugment, RandAugment, TrivialAugment.\\nAI Models\\nLeverage pre-trained AI models optimized for a variety of vision tasks, all within the Kornia ecosystem.\\nFace Detection\\n: YuNet\\nFeature Matching\\n: LoFTR, LightGlue\\nFeature Descriptor\\n: DISK, DeDoDe, SOLD2\\nSegmentation\\n: SAM\\nClassification\\n: MobileViT, VisionTransformer.\\nSee here for some of the methods that we support! (>500 ops in total !)\\nCategory\\nMethods/Models\\nImage Processing\\n- Color conversions (RGB, Grayscale, HSV, etc.)\\n- Geometric transformations (Affine, Homography, Resizing, etc.)\\n- Filtering (Gaussian blur, Median blur, etc.)\\n- Edge detection (Sobel, Canny, etc.)\\n- Morphological operations (Erosion, Dilation, etc.)\\nAugmentation\\n- Random cropping, Erasing\\n- Random geometric transformations (Affine, flipping, Fish Eye, Perspecive, Thin plate spline, Elastic)\\n- Random noises (Gaussian, Median, Motion, Box, Rain, Snow, Salt and Pepper)\\n- Random color jittering (Contrast, Brightness, CLAHE, Equalize, Gamma, Hue, Invert, JPEG, Plasma, Posterize, Saturation, Sharpness, Solarize)\\n- Random MixUp, CutMix, Mosaic, Transplantation, etc.\\nFeature Detection\\n- Detector (Harris, GFTT, Hessian, DoG, KeyNet, DISK and DeDoDe)\\n- Descriptor (SIFT, HardNet, TFeat, HyNet, SOSNet, and LAFDescriptor)\\n- Matching (nearest neighbor, mutual nearest neighbor, geometrically aware matching, AdaLAM LightGlue, and LoFTR)\\nGeometry\\n- Camera models and calibration\\n- Stereo vision (epipolar geometry, disparity, etc.)\\n- Homography estimation\\n- Depth estimation from disparity\\n- 3D transformations\\nDeep Learning Layers\\n- Custom convolution layers\\n- Recurrent layers for vision tasks\\n- Loss functions (e.g., SSIM, PSNR, etc.)\\n- Vision-specific optimizers\\nPhotometric Functions\\n- Photometric loss functions\\n- Photometric augmentations\\nFiltering\\n- Bilateral filtering\\n- DexiNed\\n- Dissolving\\n- Guided Blur\\n- Laplacian\\n- Gaussian\\n- Non-local means\\n- Sobel\\n- Unsharp masking\\nColor\\n- Color space conversions\\n- Brightness/contrast adjustment\\n- Gamma correction\\nStereo Vision\\n- Disparity estimation\\n- Depth estimation\\n- Rectification\\nImage Registration\\n- Affine and homography-based registration\\n- Image alignment using feature matching\\nPose Estimation\\n- Essential and Fundamental matrix estimation\\n- PnP problem solvers\\n- Pose refinement\\nOptical Flow\\n- Farneback optical flow\\n- Dense optical flow\\n- Sparse optical flow\\n3D Vision\\n- Depth estimation\\n- Point cloud operations\\n- Nerf\\nImage Denoising\\n- Gaussian noise removal\\n- Poisson noise removal\\nEdge Detection\\n- Sobel operator\\n- Canny edge detection\\nTransformations\\n- Rotation\\n- Translation\\n- Scaling\\n- Shearing\\nLoss Functions\\n- SSIM (Structural Similarity Index Measure)\\n- PSNR (Peak Signal-to-Noise Ratio)\\n- Cauchy\\n- Charbonnier\\n- Depth Smooth\\n- Dice\\n- Hausdorff\\n- Tversky\\n- Welsch\\nMorphological Operations\\n- Dilation\\n- Erosion\\n- Opening\\n- Closing\\nSponsorship\\nKornia is an open-source project that is developed and maintained by volunteers. Whether you're using it for research or commercial purposes, consider sponsoring or collaborating with us. Your support will help ensure Kornia's growth and ongoing innovation. Reach out to us today and be a part of shaping the future of this exciting initiative!\\nInstallation\\nFrom pip\\npip\\ninstall\\nkornia\\nOther installation options\\nFrom source with editable mode\\npip\\ninstall\\n-e\\n.\\nFor development with UV (Recommended)\\nFor development, Kornia uses\\nuv\\nfor fast Python package management and virtual environment creation. The project includes a\\nuv.lock\\nfile for reproducible dependency management.\\n./setup_dev_env.sh\\nThis will set up a complete development environment with all dependencies using the lock file for reproducibility. For more details on dependency management and lock file usage, see\\nCONTRIBUTING.md\\n.\\nFrom Github url (latest version)\\npip\\ninstall\\ngit+https://github.com/kornia/kornia\\nQuick Start\\nKornia is not just another computer vision library \\u2014 it's your gateway to effortless Computer Vision and AI.\\nGet started with Kornia image transformation and augmentation!\\nimport\\nnumpy\\nas\\nnp\\nimport\\nkornia_rs\\nas\\nkr\\nfrom\\nkornia.augmentation\\nimport\\nAugmentationSequential\\n,\\nRandomAffine\\n,\\nRandomBrightness\\nfrom\\nkornia.filters\\nimport\\nStableDiffusionDissolving\\n# Load and prepare your image\\nimg\\n:\\nnp\\n.\\nndarray\\n=\\nkr\\n.\\nread_image_any\\n(\\n\\\"img.jpeg\\\"\\n)\\nimg\\n=\\nkr\\n.\\nresize\\n(\\nimg\\n,\\n(\\n256\\n,\\n256\\n),\\ninterpolation\\n=\\n\\\"bilinear\\\"\\n)\\n# alternatively, load image with PIL\\n# img = Image.open(\\\"img.jpeg\\\").resize((256, 256))\\n# img = np.array(img)\\nimg\\n=\\nnp\\n.\\nstack\\n([\\nimg\\n]\\n*\\n2\\n)\\n# batch images\\n# Define an augmentation pipeline\\naugmentation_pipeline\\n=\\nAugmentationSequential\\n(\\nRandomAffine\\n((\\n-\\n45.\\n,\\n45.\\n),\\np\\n=\\n1.\\n),\\nRandomBrightness\\n((\\n0.\\n,\\n1.\\n),\\np\\n=\\n1.\\n)\\n)\\n# Leveraging StableDiffusion models\\ndslv_op\\n=\\nStableDiffusionDissolving\\n()\\nimg\\n=\\naugmentation_pipeline\\n(\\nimg\\n)\\ndslv_op\\n(\\nimg\\n,\\nstep_number\\n=\\n500\\n)\\ndslv_op\\n.\\nsave\\n(\\n\\\"Kornia-enhanced.jpg\\\"\\n)\\nFind out Kornia ONNX models with ONNXSequential!\\nimport\\nnumpy\\nas\\nnp\\nfrom\\nkornia.onnx\\nimport\\nONNXSequential\\n# Chain ONNX models from HuggingFace repo and your own local model together\\nonnx_seq\\n=\\nONNXSequential\\n(\\n\\\"hf://operators/kornia.geometry.transform.flips.Hflip\\\"\\n,\\n\\\"hf://models/kornia.models.detection.rtdetr_r18vd_640x640\\\"\\n,\\n# Or you may use \\\"YOUR_OWN_MODEL.onnx\\\"\\n)\\n# Prepare some input data\\ninput_data\\n=\\nnp\\n.\\nrandom\\n.\\nrandn\\n(\\n1\\n,\\n3\\n,\\n384\\n,\\n512\\n)\\n.\\nastype\\n(\\nnp\\n.\\nfloat32\\n)\\n# Perform inference\\noutputs\\n=\\nonnx_seq\\n(\\ninput_data\\n)\\n# Print the model outputs\\nprint\\n(\\noutputs\\n)\\n# Export a new ONNX model that chains up all three models together!\\nonnx_seq\\n.\\nexport\\n(\\n\\\"chained_model.onnx\\\"\\n)\\nMulti-framework support\\nYou can now use Kornia with\\nTensorFlow\\n,\\nJAX\\n, and\\nNumPy\\n. See\\nMulti-Framework Support\\nfor more details.\\nimport\\nkornia\\ntf_kornia\\n=\\nkornia\\n.\\nto_tensorflow\\n()\\nPowered by\\nCall For Contributors\\nAre you passionate about computer vision, AI, and open-source development? Join us in shaping the future of Kornia! We are actively seeking contributors to help expand and enhance our library, making it even more powerful, accessible, and versatile. Whether you're an experienced developer or just starting, there's a place for you in our community.\\nAccessible AI Models\\nWe are excited to announce our latest advancement: a new initiative designed to seamlessly integrate lightweight AI models into Kornia.\\nWe aim to run any models as smooth as big models such as StableDiffusion, to support them well in many perspectives.\\nWe have already included a selection of lightweight AI models like\\nYuNet (Face Detection)\\n,\\nLoftr (Feature Matching)\\n, and\\nSAM (Segmentation)\\n. Now, we're looking for contributors to help us:\\nExpand the Model Selection: Import decent models into our library. If you are a researcher, Kornia is an excellent place for you to promote your model!\\nModel Optimization: Work on optimizing models to reduce their computational footprint while maintaining accuracy and performance. You may start from offering ONNX support!\\nModel Documentation: Create detailed guides and examples to help users get the most out of these models in their projects.\\nDocumentation And Tutorial Optimization\\nKornia's foundation lies in its extensive collection of classic computer vision operators, providing robust tools for image processing, feature extraction, and geometric transformations. We continuously seek for contributors to help us improve our documentation and present nice tutorials to our users.\\nCite\\nIf you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in\\nCITATION\\n.\\n@inproceedings\\n{\\neriba2019kornia\\n,\\nauthor\\n=\\n{E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski}\\n,\\ntitle\\n=\\n{Kornia: an Open Source Differentiable Computer Vision Library for PyTorch}\\n,\\nbooktitle\\n=\\n{Winter Conference on Applications of Computer Vision}\\n,\\nyear\\n=\\n{2020}\\n,\\nurl\\n=\\n{https://arxiv.org/pdf/1910.02190.pdf}\\n}\\nContributing\\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the\\nCONTRIBUTING\\nnotes. The participation in this open source project is subject to\\nCode of Conduct\\n.\\nCommunity\\nDiscord:\\nJoin our workspace to keep in touch with our core contributors, get latest updates on the industry and  be part of our community.\\nJOIN HERE\\nGitHub Issues:\\nbug reports, feature requests, install issues, RFCs, thoughts, etc.\\nOPEN\\nForums:\\ndiscuss implementations, research, etc.\\nGitHub Forums\\nMade with\\ncontrib.rocks\\n.\\nLicense\\nKornia is released under the Apache 2.0 license. See the\\nLICENSE\\nfile for more information.\"}, {\"name\": \"Scrapy\", \"description\": \"|logo|\\n\\n.. |logo| image:: https://raw.githubusercontent.com/scrapy/scrapy/master/docs/_static/logo.svg\\n   :target: https://scrapy.org\\n   :alt: Scrapy\\n   :width: 480px\\n\\n|version| |python_version| |ubuntu| |macos| |windows| |coverage| |conda| |deepwiki|\\n\\n.. |version| image:: https://img.shields.io/pypi/v/Scrapy.svg\\n   :target: https://pypi.org/pypi/Scrapy\\n   :alt: PyPI Version\\n\\n.. |python_version| image:: https://img.shields.io/pypi/pyversions/Scrapy.svg\\n   :target: https://pypi.org/pypi/Scrapy\\n   :alt: Supported Python Versions\\n\\n.. |ubuntu| image:: https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg\\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu\\n   :alt: Ubuntu\\n\\n.. |macos| image:: https://github.com/scrapy/scrapy/workflows/macOS/badge.svg\\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS\\n   :alt: macOS\\n\\n.. |windows| image:: https://github.com/scrapy/scrapy/workflows/Windows/badge.svg\\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows\\n   :alt: Windows\\n\\n.. |coverage| image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\\n   :target: https://codecov.io/github/scrapy/scrapy?branch=master\\n   :alt: Coverage report\\n\\n.. |conda| image:: https://anaconda.org/conda-forge/scrapy/badges/version.svg\\n   :target: https://anaconda.org/conda-forge/scrapy\\n   :alt: Conda Version\\n\\n.. |deepwiki| image:: https://deepwiki.com/badge.svg\\n   :target: https://deepwiki.com/scrapy/scrapy\\n   :alt: Ask DeepWiki\\n\\nScrapy_ is a web scraping framework to extract structured data from websites.\\nIt is cross-platform, and requires Python 3.9+. It is maintained by Zyte_\\n(formerly Scrapinghub) and `many other contributors`_.\\n\\n.. _many other contributors: https://github.com/scrapy/scrapy/graphs/contributors\\n.. _Scrapy: https://scrapy.org/\\n.. _Zyte: https://www.zyte.com/\\n\\nInstall with:\\n\\n.. code:: bash\\n\\n    pip install scrapy\\n\\nAnd follow the documentation_ to learn how to use it.\\n\\n.. _documentation: https://docs.scrapy.org/en/latest/\\n\\nIf you wish to contribute, see Contributing_.\\n\\n.. _Contributing: https://docs.scrapy.org/en/master/contributing.html\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"A high-level Web Crawling and Web Scraping framework\", \"latest_version\": \"2.13.3\", \"weekly_downloads\": 801743, \"description_cleaned\": \"Scrapy\\nis a web scraping framework to extract structured data from websites.\\nIt is cross-platform, and requires Python 3.9+. It is maintained by\\nZyte\\n(formerly Scrapinghub) and\\nmany other contributors\\n.\\nInstall with:\\npip\\ninstall\\nscrapy\\nAnd follow the\\ndocumentation\\nto learn how to use it.\\nIf you wish to contribute, see\\nContributing\\n.\"}, {\"name\": \"azure-cosmosdb-table\", \"description\": \"Microsoft Azure CosmosDB Table SDK for Python\\r\\n=============================================\\r\\n\\r\\nThis project provides a client library in Python that makes it easy to\\r\\nconsume Microsoft Azure CosmosDB Table services. For documentation please see\\r\\nthe Microsoft Azure `Python Developer Center`_ and our `API Reference`_ Page.\\r\\n\\r\\n    If you are looking for the Service Bus or Azure Management\\r\\n    libraries, please visit\\r\\n    https://github.com/Azure/azure-sdk-for-python.\\r\\n\\r\\n\\r\\nCompatibility\\r\\n=============\\r\\n\\r\\n**IMPORTANT**: If you have an earlier version of the azure package\\r\\n(version < 1.0), you should uninstall it before installing this package.\\r\\n\\r\\nYou can check the version using pip:\\r\\n\\r\\n.. code:: shell\\r\\n\\r\\n    pip freeze\\r\\n\\r\\nIf you see azure==0.11.0 (or any version below 1.0), uninstall it first then install it again:\\r\\n\\r\\n.. code:: shell\\r\\n\\r\\n    pip uninstall azure\\r\\n    pip install azure\\r\\n\\r\\nFeatures\\r\\n========\\r\\n\\r\\n-  Table\\r\\n\\r\\n   -  Create/Read/Update/Delete Tables\\r\\n   -  Create/Read/Update/Delete Entities\\r\\n   -  Batch operations\\r\\n   -  Advanced Table Operations\\r\\n\\r\\n\\r\\nGetting Started\\r\\n===============\\r\\n\\r\\nDownload\\r\\n--------\\r\\n\\r\\nOption 1: Via PyPi\\r\\n~~~~~~~~~~~~~~~~~~\\r\\n\\r\\nTo install via the Python Package Index (PyPI), type:\\r\\n\\r\\n::\\r\\n\\r\\n    pip install azure-cosmosdb-table\\r\\n\\r\\nOption 2: Source Via Git\\r\\n~~~~~~~~~~~~~~~~~~~~~~~~\\r\\n\\r\\nTo get the source code of the SDK via git just type:\\r\\n\\r\\n::\\r\\n\\r\\n    git clone git://github.com/Azure/azure-cosmosdb-python.git\\r\\n    cd ./azure-cosmosdb-table\\r\\n    python setup.py install\\r\\n\\r\\nOption 3: Source Zip\\r\\n~~~~~~~~~~~~~~~~~~~~\\r\\n\\r\\nDownload a zip of the code via GitHub or PyPi. Then, type:\\r\\n\\r\\n::\\r\\n\\r\\n    cd ./azure-cosmosdb-table\\r\\n    python setup.py install\\r\\n\\r\\n\\r\\nMinimum Requirements\\r\\n--------------------\\r\\n\\r\\n-  Python 2.7, 3.3, 3.4, 3.5, or 3.6.\\r\\n-  See setup.py for dependencies\\r\\n\\r\\nUsage\\r\\n-----\\r\\n\\r\\nTo use this SDK to call Microsoft Azure storage services, you need to\\r\\nfirst `create an account`_.\\r\\n\\r\\nCode Sample\\r\\n-----------\\r\\n\\r\\nSee the samples directory for table usage samples.\\r\\n\\r\\nNeed Help?\\r\\n==========\\r\\n\\r\\nBe sure to check out the Microsoft Azure `Developer Forums on MSDN`_ or\\r\\nthe `Developer Forums on Stack Overflow`_ if you have trouble with the\\r\\nprovided code.\\r\\n\\r\\nContribute Code or Provide Feedback\\r\\n===================================\\r\\n\\r\\nIf you would like to become an active contributor to this project, please\\r\\nfollow the instructions provided in `Azure Projects Contribution\\r\\nGuidelines`_. You can find more details for contributing in the `CONTRIBUTING.md doc`_.\\r\\n\\r\\nIf you encounter any bugs with the library, please file an issue in the\\r\\n`Issues`_ section of the project.\\r\\n\\r\\nLearn More\\r\\n==========\\r\\n\\r\\n-  `Python Developer Center`_\\r\\n-  `Azure Storage Service`_\\r\\n-  `Azure Storage Team Blog`_\\r\\n-  `API Reference`_\\r\\n\\r\\n.. _Python Developer Center: http://azure.microsoft.com/en-us/develop/python/\\r\\n.. _API Reference: https://azure.github.io/azure-cosmosdb-python/\\r\\n.. _create an account: https://account.windowsazure.com/signup\\r\\n.. _Developer Forums on MSDN: http://social.msdn.microsoft.com/Forums/windowsazure/en-US/home?forum=windowsazuredata\\r\\n.. _Developer Forums on Stack Overflow: http://stackoverflow.com/questions/tagged/azure+windows-azure-storage\\r\\n.. _Azure Projects Contribution Guidelines: http://azure.github.io/guidelines.html\\r\\n.. _Issues: https://github.com/Azure/azure-cosmosdb-python/issues\\r\\n.. _Azure Storage Service: http://azure.microsoft.com/en-us/documentation/services/storage/\\r\\n.. _Azure Storage Team Blog: http://blogs.msdn.com/b/windowsazurestorage/\\r\\n.. _CONTRIBUTING.md doc: CONTRIBUTING.md\\r\\n\\r\\n\\r\\n\", \"description_content_type\": null, \"summary\": \"Microsoft Azure CosmosDB Table Client Library for Python\", \"latest_version\": \"1.0.6\", \"weekly_downloads\": 801415, \"description_cleaned\": \"Microsoft Azure CosmosDB Table SDK for Python\\nThis project provides a client library in Python that makes it easy to\\nconsume Microsoft Azure CosmosDB Table services. For documentation please see\\nthe Microsoft Azure\\nPython Developer Center\\nand our\\nAPI Reference\\nPage.\\nIf you are looking for the Service Bus or Azure Management\\nlibraries, please visit\\nhttps://github.com/Azure/azure-sdk-for-python\\n.\\nCompatibility\\nIMPORTANT\\n: If you have an earlier version of the azure package\\n(version < 1.0), you should uninstall it before installing this package.\\nYou can check the version using pip:\\npip\\nfreeze\\nIf you see azure==0.11.0 (or any version below 1.0), uninstall it first then install it again:\\npip\\nuninstall\\nazure\\npip\\ninstall\\nazure\\nFeatures\\nTable\\nCreate/Read/Update/Delete Tables\\nCreate/Read/Update/Delete Entities\\nBatch operations\\nAdvanced Table Operations\\nGetting Started\\nDownload\\nOption 1: Via PyPi\\nTo install via the Python Package Index (PyPI), type:\\npip install azure-cosmosdb-table\\nOption 2: Source Via Git\\nTo get the source code of the SDK via git just type:\\ngit clone git://github.com/Azure/azure-cosmosdb-python.git\\ncd ./azure-cosmosdb-table\\npython setup.py install\\nOption 3: Source Zip\\nDownload a zip of the code via GitHub or PyPi. Then, type:\\ncd ./azure-cosmosdb-table\\npython setup.py install\\nMinimum Requirements\\nPython 2.7, 3.3, 3.4, 3.5, or 3.6.\\nSee setup.py for dependencies\\nUsage\\nTo use this SDK to call Microsoft Azure storage services, you need to\\nfirst\\ncreate an account\\n.\\nCode Sample\\nSee the samples directory for table usage samples.\\nNeed Help?\\nBe sure to check out the Microsoft Azure\\nDeveloper Forums on MSDN\\nor\\nthe\\nDeveloper Forums on Stack Overflow\\nif you have trouble with the\\nprovided code.\\nContribute Code or Provide Feedback\\nIf you would like to become an active contributor to this project, please\\nfollow the instructions provided in\\nAzure Projects Contribution\\nGuidelines\\n. You can find more details for contributing in the\\nCONTRIBUTING.md doc\\n.\\nIf you encounter any bugs with the library, please file an issue in the\\nIssues\\nsection of the project.\\nLearn More\\nPython Developer Center\\nAzure Storage Service\\nAzure Storage Team Blog\\nAPI Reference\"}, {\"name\": \"databricks-agents\", \"description\": \"# Limitations\\n\\nThe software and other materials included in this library are subject to the [DB license](https://www.databricks.com/legal/db-license).\\n\\n\\n# License\\n\\n**DB license**\\n\\nCopyright (2024) Databricks, Inc.\\n\\nThis library (the \\\"Software\\\") may not be used except in connection with the Licensee's use of the Databricks Platform Services\\npursuant to an Agreement (defined below) between Licensee (defined below) and Databricks, Inc. (\\\"Databricks\\\"). This Software\\nshall be deemed part of the Downloadable Services under the Agreement, or if the Agreement does not define Downloadable Services,\\nSubscription Services, or if neither are defined then the term in such Agreement that refers to the applicable Databricks Platform\\nServices (as defined below) shall be substituted herein for \\\"Downloadable Services\\\". Licensee's use of the Software must comply at\\nall times with any restrictions applicable to the Downlodable Services and Subscription Services, generally, and must be used in\\naccordance with any applicable documentation.\\n\\nAdditionally, and notwithstanding anything in the Agreement to the contrary:\\n* THE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR\\nIN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\nIf you have not agreed to an Agreement or otherwise do not agree to these terms, you may not use the Software.\\n\\nThis license terminates automatically upon the termination of the Agreement or Licensee's breach of these terms.\\n\\nAgreement: the agreement between Databricks and Licensee governing the use of the Databricks Platform Services, which shall be, with\\nrespect to Databricks, the Databricks Terms of Service located at www.databricks.com/termsofservice, and with respect to Databricks\\nCommunity Edition, the Community Edition Terms of Service located at www.databricks.com/ce-termsofuse, in each case unless Licensee\\nhas entered into a separate written agreement with Databricks governing the use of the applicable Databricks Platform Services.\\n\\nDatabricks Platform Services: the Databricks services or the Databricks Community Edition services, according to where the Software is used.\\n\\nLicensee: the user of the Software, or, if the Software is being used on behalf of a company, the company.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Mosaic AI Agent Framework SDK\", \"latest_version\": \"1.8.2\", \"weekly_downloads\": 800410, \"description_cleaned\": \"Limitations\\nThe software and other materials included in this library are subject to the\\nDB license\\n.\\nLicense\\nDB license\\nCopyright (2024) Databricks, Inc.\\nThis library (the \\\"Software\\\") may not be used except in connection with the Licensee's use of the Databricks Platform Services\\npursuant to an Agreement (defined below) between Licensee (defined below) and Databricks, Inc. (\\\"Databricks\\\"). This Software\\nshall be deemed part of the Downloadable Services under the Agreement, or if the Agreement does not define Downloadable Services,\\nSubscription Services, or if neither are defined then the term in such Agreement that refers to the applicable Databricks Platform\\nServices (as defined below) shall be substituted herein for \\\"Downloadable Services\\\". Licensee's use of the Software must comply at\\nall times with any restrictions applicable to the Downlodable Services and Subscription Services, generally, and must be used in\\naccordance with any applicable documentation.\\nAdditionally, and notwithstanding anything in the Agreement to the contrary:\\nTHE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR\\nIN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nIf you have not agreed to an Agreement or otherwise do not agree to these terms, you may not use the Software.\\nThis license terminates automatically upon the termination of the Agreement or Licensee's breach of these terms.\\nAgreement: the agreement between Databricks and Licensee governing the use of the Databricks Platform Services, which shall be, with\\nrespect to Databricks, the Databricks Terms of Service located at\\nwww.databricks.com/termsofservice\\n, and with respect to Databricks\\nCommunity Edition, the Community Edition Terms of Service located at\\nwww.databricks.com/ce-termsofuse\\n, in each case unless Licensee\\nhas entered into a separate written agreement with Databricks governing the use of the applicable Databricks Platform Services.\\nDatabricks Platform Services: the Databricks services or the Databricks Community Edition services, according to where the Software is used.\\nLicensee: the user of the Software, or, if the Software is being used on behalf of a company, the company.\"}, {\"name\": \"pyyaml-include\", \"description\": \"# pyyaml-include\\n\\n[![GitHub tag](https://img.shields.io/github/tag/tanbro/pyyaml-include.svg)](https://github.com/tanbro/pyyaml-include)\\n[![Python Package](https://github.com/tanbro/pyyaml-include/workflows/Python%20package/badge.svg)](https://github.com/tanbro/pyyaml-include/actions?query=workflow%3A%22Python+package%22)\\n[![Documentation Status](https://readthedocs.org/projects/pyyaml-include/badge/?version=latest)](https://pyyaml-include.readthedocs.io/en/latest/)\\n[![PyPI](https://img.shields.io/pypi/v/pyyaml-include.svg)](https://pypi.org/project/pyyaml-include/)\\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=tanbro_pyyaml-include&metric=alert_status)](https://sonarcloud.io/dashboard?id=tanbro_pyyaml-include)\\n\\nAn extending constructor of [PyYAML][]: include other [YAML][] files into current [YAML][] document.\\n\\nIn version `2.0`, [fsspec][] was introduced. With it, we can even include files by HTTP, SFTP, S3 ...\\n\\n> \\u26a0\\ufe0f **Warning** \\\\\\n> \\u201cpyyaml-include\\u201d `2.0` is **NOT compatible** with `1.0`\\n\\n## Install\\n\\n```bash\\npip install \\\"pyyaml-include\\\"\\n```\\n\\nBecause [fsspec][] was introduced to open the including files since v2.0, an installation can be performed like below, if want to open remote files:\\n\\n- for files on website:\\n\\n  ```bash\\n  pip install \\\"pyyaml-include\\\" fsspec[http]\\n  ```\\n\\n- for files on S3:\\n\\n  ```bash\\n  pip install \\\"pyyaml-include\\\" fsspec[s3]\\n  ```\\n\\n- see [fsspec][]'s documentation for more\\n\\n> \\ud83d\\udd16 **Tip** \\\\\\n> \\u201cpyyaml-include\\u201d depends on [fsspec][], it will be installed no matter including local or remote files.\\n\\n## Basic usages\\n\\nConsider we have such [YAML][] files:\\n\\n```\\n\\u251c\\u2500\\u2500 0.yml\\n\\u2514\\u2500\\u2500 include.d\\n    \\u251c\\u2500\\u2500 1.yml\\n    \\u2514\\u2500\\u2500 2.yml\\n```\\n\\n- `1.yml` 's content:\\n\\n  ```yaml\\n  name: \\\"1\\\"\\n  ```\\n\\n- `2.yml` 's content:\\n\\n  ```yaml\\n  name: \\\"2\\\"\\n  ```\\n\\nTo include `1.yml`, `2.yml` in `0.yml`, we shall:\\n\\n1. Register a `yaml_include.Constructor` to [PyYAML][]'s loader class, with `!inc`(or any other tags start with `!` character) as it's tag:\\n\\n   ```python\\n   import yaml\\n   import yaml_include\\n\\n   # add the tag\\n   yaml.add_constructor(\\\"!inc\\\", yaml_include.Constructor(base_dir='/your/conf/dir'))\\n   ```\\n\\n1. Use `!inc` tag(s) in `0.yaml`:\\n\\n   ```yaml\\n   file1: !inc include.d/1.yml\\n   file2: !inc include.d/2.yml\\n   ```\\n\\n1. Load `0.yaml` in your Python program\\n\\n   ```python\\n   with open('0.yml') as f:\\n      data = yaml.full_load(f)\\n   print(data)\\n   ```\\n\\n   we'll get:\\n\\n   ```python\\n   {'file1': {'name': '1'}, 'file2': {'name': '2'}}\\n   ```\\n\\n1. (optional) the constructor can be unregistered:\\n\\n   ```python\\n   del yaml.Loader.yaml_constructors[\\\"!inc\\\"]\\n   del yaml.UnSafeLoader.yaml_constructors[\\\"!inc\\\"]\\n   del yaml.FullLoader.yaml_constructors[\\\"!inc\\\"]\\n   ```\\n\\n### Include in Mapping\\n\\nIf `0.yml` was:\\n\\n```yaml\\nfile1: !inc include.d/1.yml\\nfile2: !inc include.d/2.yml\\n```\\n\\nWe'll get:\\n\\n```yaml\\nfile1:\\n  name: \\\"1\\\"\\nfile2:\\n  name: \\\"2\\\"\\n```\\n\\n### Include in Sequence\\n\\nIf `0.yml` was:\\n\\n```yaml\\nfiles:\\n  - !inc include.d/1.yml\\n  - !inc include.d/2.yml\\n```\\n\\nWe'll get:\\n\\n```yaml\\nfiles:\\n  - name: \\\"1\\\"\\n  - name: \\\"2\\\"\\n```\\n\\n## Advanced usages\\n\\n### Wildcards\\n\\nFile name can contain shell-style wildcards. Data loaded from the file(s) found by wildcards will be set in a sequence.\\n\\nThat is, a list will be returned when including file name contains wildcards.\\nLength of the returned list equals number of matched files:\\n\\nIf `0.yml` was:\\n\\n```yaml\\nfiles: !inc include.d/*.yml\\n```\\n\\nWe'll get:\\n\\n```yaml\\nfiles:\\n  - name: \\\"1\\\"\\n  - name: \\\"2\\\"\\n```\\n\\n- when only 1 file matched, length of list will be 1\\n- when there are no files matched, an empty list will be returned\\n\\nWe support `**`, `?` and `[..]`. We do not support `^` for pattern negation.\\nThe `maxdepth` option is applied on the first `**` found in the path.\\n\\n> \\u2757 **Important**\\n>\\n> - Using the `**` pattern in large directory trees or remote file system (S3, HTTP ...) may consume an inordinate amount of time.\\n> - There is no method like lazy-load or iteration, all data of found files returned to the YAML doc-tree are fully loaded in memory, large amount of memory may be needed if there were many or big files.\\n\\n### Work with fsspec\\n\\nIn `v2.0`, we use [fsspec][] to open including files, thus we can include files from many different sources, such as local file system, S3, HTTP, SFTP ...\\n\\nFor example, we can include a file from website in YAML:\\n\\n```yaml\\nconf:\\n  logging: !inc http://domain/etc/app/conf.d/logging.yml\\n```\\n\\nIn such situations, when creating a `Constructor` constructor, a [fsspec][] filesystem object shall be set to `fs` argument.\\n\\nFor example, if want to include files from website, we shall:\\n\\n1. create a `Constructor` with a [fsspec][] HTTP filesystem object as it's `fs`:\\n\\n   ```python\\n   import yaml\\n   import fsspec\\n   import yaml_include\\n\\n   http_fs = fsspec.filesystem(\\\"http\\\", client_kwargs={\\\"base_url\\\": f\\\"http://{HOST}:{PORT}\\\"})\\n\\n   ctor = yaml_include.Constructor(fs=http_fs, base_dir=\\\"/foo/baz\\\")\\n   yaml.add_constructor(\\\"!inc\\\", ctor, yaml.Loader)\\n   ```\\n\\n1. then, write a [YAML][] document to include files from `http://${HOST}:${PORT}`:\\n\\n   ```yaml\\n   key1: !inc doc1.yml    # relative path to \\\"base_dir\\\"\\n   key2: !inc ./doc2.yml  # relative path to \\\"base_dir\\\" also\\n   key3: !inc /doc3.yml   # absolute path, \\\"base_dir\\\" does not affect\\n   key3: !inc ../doc4.yml # relative path one level upper to \\\"base_dir\\\"\\n   ```\\n\\n1. load it with [PyYAML][]:\\n\\n   ```python\\n   yaml.load(yaml_string, yaml.Loader)\\n   ```\\n\\nAbove [YAML][] snippet will be loaded like:\\n\\n- `key1`: pared YAML of `http://${HOST}:${PORT}/foo/baz/doc1.yml`\\n- `key2`: pared YAML of `http://${HOST}:${PORT}/foo/baz/doc2.yml`\\n- `key3`: pared YAML of `http://${HOST}:${PORT}/doc3.yml`\\n- `key4`: pared YAML of `http://${HOST}:${PORT}/foo/doc4.yml`\\n\\n> \\ud83d\\udd16 **Tip** \\\\\\n> Check [fsspec][]'s documentation for more\\n\\n---\\n\\n> \\u2139\\ufe0f **Note** \\\\\\n> If `fs` argument is omitted, a `\\\"file\\\"`/`\\\"local\\\"` [fsspec][] filesystem object will be used automatically. That is to say:\\n>\\n> ```yaml\\n> data: !inc: foo/baz.yaml\\n> ```\\n>\\n> is equivalent to (if no `base_dir` was set in `Constructor()`):\\n>\\n> ```yaml\\n> data: !inc: file://foo/baz.yaml\\n> ```\\n>\\n> and\\n>\\n> ```python\\n> yaml.add_constructor(\\\"!inc\\\", Constructor())\\n> ```\\n>\\n> is equivalent to:\\n>\\n> ```python\\n> yaml.add_constructor(\\\"!inc\\\", Constructor(fs=fsspec.filesystem(\\\"file\\\")))\\n> ```\\n\\n### Parameters in YAML\\n\\nAs a callable object, `Constructor` passes YAML tag parameters to [fsspec][] for more detailed operations.\\n\\nThe first argument is `urlpath`, it's fixed and must-required, either positional or named.\\nNormally, we put it as a string after the tag(eg: `!inc`), just like examples above.\\n\\nHowever, there are more parameters.\\n\\n- in a sequence way, parameters will be passed to python as positional arguments, like `*args` in python function. eg:\\n\\n  ```yaml\\n  files: !inc [include.d/**/*.yaml, {maxdepth: 1}, {encoding: utf16}]\\n  ```\\n\\n- in a mapping way, parameters will be passed to python as named arguments, like `**kwargs` in python function. eg:\\n\\n  ```yaml\\n  files: !inc {urlpath: /foo/baz.yaml, encoding: utf16}\\n  ```\\n\\nBut the format of parameters has multiple cases, and differs variably in different [fsspec][] implementation backends.\\n\\n- If a scheme/protocol(\\u201c`http://`\\u201d, \\u201c`sftp://`\\u201d, \\u201c`file://`\\u201d, etc.) is defined, and there is no wildcard in `urlpath`, `Constructor` will invoke [`fsspec.open`](https://filesystem-spec.readthedocs.io/en/stable/api.html#fsspec.open) directly to open it. Which means `Constructor`'s `fs` will be ignored, and a new standalone `fs` will be created implicitly.\\n\\n  In this situation, `urlpath` will be passed to `fsspec.open`'s first argument, and all other parameters will also be passed to the function.\\n\\n  For example,\\n\\n  - the [YAML][] snippet\\n\\n    ```yaml\\n    files: !inc [file:///foo/baz.yaml, r]\\n    ```\\n\\n    will cause python code like\\n\\n    ```python\\n    with fsspec.open(\\\"file:///foo/baz.yaml\\\", \\\"r\\\") as f:\\n        yaml.load(f, Loader)\\n    ```\\n\\n  - and the [YAML][] snippet\\n\\n    ```yaml\\n    files: !inc {urlpath: file:///foo/baz.yaml, encoding: utf16}\\n    ```\\n\\n    will cause python code like\\n\\n    ```python\\n    with fsspec.open(\\\"file:///foo/baz.yaml\\\", encoding=\\\"utf16\\\") as f:\\n        yaml.load(f, Loader)\\n    ```\\n\\n- If `urlpath` has wildcard, and also scheme in it, `Constructor` will:\\n\\n  Invoke [fsspec][]'s [`open_files`](https://filesystem-spec.readthedocs.io/en/stable/api.html#fsspec.open_files) function to search, open and load files, and return the results in a list. [YAML][] include statement's parameters are passed to `open_files` function.\\n\\n- If `urlpath` has wildcard, and no scheme in it, `Constructor` will:\\n\\n  1. invoke corresponding [fsspec][] implementation backend's [`glob`](https://filesystem-spec.readthedocs.io/en/stable/api.html#fsspec.spec.AbstractFileSystem.glob) method to search files,\\n  1. then call [`open`](https://filesystem-spec.readthedocs.io/en/stable/api.html#fsspec.spec.AbstractFileSystem.open) method to open each found file(s).\\n\\n  `urlpath` will be passed as the first argument to both `glob` and `open` method of the corresponding [fsspec][] implementation backend, and other parameters will also be passed to `glob` and `open` method as their following arguments.\\n\\n  In the case of wildcards, what need to pay special attention to is that there are **two separated parameters** after `urlpath`, the first is for `glob` method, and the second is for `open` method. Each of them could be either sequence, mapping or scalar, corresponds single, positional and named argument(s) in python. For example:\\n\\n  - If we want to include every `.yml` file in directory `etc/app` recursively with max depth at 2, and open them in utf-16 codec, we shall write the [YAML][] as below:\\n\\n    ```yaml\\n    files: !inc [\\\"etc/app/**/*.yml\\\", {maxdepth: !!int \\\"2\\\"}, {encoding: utf16}]\\n    ```\\n\\n    it will cause python code like:\\n\\n    ```python\\n    for file in local_fs.glob(\\\"etc/app/**/*.yml\\\", maxdepth=2):\\n        with local_fs.open(file, encoding=\\\"utf16\\\") as f:\\n            yaml.load(f, Loader)\\n    ```\\n\\n  - Since `maxdepth` is the seconde argument after `path` in `glob` method, we can also write the [YAML][] like this:\\n\\n    ```yaml\\n    files: !inc [\\\"etc/app/**/*.yml\\\", [!!int \\\"2\\\"]]\\n    ```\\n\\n    The parameters for `open` is omitted, means no more arguments except `urlpath` is passed.\\n\\n    it will cause python code like:\\n\\n    ```python\\n    for file in local_fs.glob(\\\"etc/app/**/*.yml\\\", 2):\\n        with local_fs.open(file) as f:\\n            yaml.load(f, Loader)\\n    ```\\n\\n  - The two parameters can be in a mapping form, and name of the keys are `\\\"glob\\\"` and `\\\"open\\\"`. for example:\\n\\n    ```yaml\\n    files: !inc {urlpath: \\\"etc/app/**/*.yml\\\", glob: [!!int \\\"2\\\"], open: {encoding: utf16}}\\n    ```\\n\\n  > \\u2757 **Important** \\\\\\n  > [PyYAML][] sometimes takes scalar parameter of custom constructor as string, we can use a \\u2018Standard YAML tag\\u2019 to ensure non-string data type in the situation.\\n  >\\n  > For example, following [YAML][] snippet may cause an error:\\n  >\\n  > ```yaml\\n  > files: !inc [\\\"etc/app/**/*.yml\\\", open: {intParam: 1}]\\n  > ```\\n  >\\n  > Because [PyYAML][] treats `{\\\"intParam\\\": 1}` as `{\\\"intParam\\\": \\\"1\\\"}`, which makes python code like `fs.open(path, intParam=\\\"1\\\")`. To prevent this, we shall write the [YAML][] like:\\n  >\\n  > ```yaml\\n  > files: !inc [\\\"etc/app/**/*.yml\\\", open: {intParam: !!int 1}]\\n  > ```\\n  >\\n  > where `!!int` is a \\u2018Standard YAML tag\\u2019 to force integer type of `maxdepth` argument.\\n  >\\n  > > \\u2139\\ufe0f **Note** \\\\\\n  > > `BaseLoader`, `SafeLoader`, `CBaseLoader`, `CSafeLoader` do **NOT** support \\u2018Standard YAML tag\\u2019.\\n  > ---\\n  > > \\ud83d\\udd16 **Tip** \\\\\\n  > > `maxdepth` argument of [fsspec][] `glob` method is already force converted by `Constructor`, no need to write a `!!int` tag on it.\\n\\n- Else, `Constructor` will invoke corresponding [fsspec][] implementation backend's [`open`](https://filesystem-spec.readthedocs.io/en/stable/api.html#fsspec.spec.AbstractFileSystem.open) method to open the file, parameters beside `urlpath` will be passed to the method.\\n\\n### Absolute and Relative URL/Path\\n\\nWhen the path after include tag (eg: `!inc`) is not a full protocol/scheme URL and not starts with `\\\"/\\\"`, `Constructor` tries to join the path with `base_dir`, which is a argument of `Constructor.__init__()`.\\nIf `base_dir` is omitted or `None`, the actually including file path is the path in defined in [YAML][] without a change, and different [fsspec][] filesystem will treat them differently. In local filesystem, it will be `cwd`.\\n\\nFor remote filesystem, `HTTP` for example, the `base_dir` can not be `None` and usually be set to `\\\"/\\\"`.\\n\\nRelative path does not support full protocol/scheme URL format, `base_dir` does not effect for that.\\n\\nFor example, if we register such a `Constructor` to [PyYAML][]:\\n\\n```python\\nimport yaml\\nimport fsspec\\nimport yaml_include\\n\\nyaml.add_constructor(\\n    \\\"!http-include\\\",\\n    yaml_include.Constructor(\\n        fsspec.filesystem(\\\"http\\\", client_kwargs={\\\"base_url\\\": f\\\"http://{HOST}:{PORT}\\\"}),\\n        base_dir=\\\"/sub_1/sub_1_1\\\"\\n    )\\n)\\n```\\n\\nthen, load following [YAML][]:\\n\\n```yaml\\nxyz: !http-include xyz.yml\\n```\\n\\nthe actual URL to access is `http://$HOST:$PORT/sub_1/sub_1_1/xyz.yml`\\n\\n### Flatten sequence object in multiple matched files\\n\\nConsider we have such a YAML:\\n\\n```yaml\\nitems: !include \\\"*.yaml\\\"\\n```\\n\\nIf every file matches `*.yaml` contains a sequence object at the top level in it, what parsed and loaded will be:\\n\\n```yaml\\nitems: [\\n    [item 0 of 1st file, item 1 of 1st file, ... , item n of 1st file, ...],\\n    [item 0 of 2nd file, item 1 of 2nd file, ... , item n of 2nd file, ...],\\n    # ....\\n    [item 0 of nth file, item 1 of nth file, ... , item n of nth file, ...],\\n    # ...\\n]\\n```\\n\\nIt's a 2-dim array, because YAML content of each matched file is treated as a member of the list(sequence).\\n\\nBut if `flatten` parameter was set to `true`, like:\\n\\n```yaml\\nitems: !include {urlpath: \\\"*.yaml\\\", flatten: true}\\n```\\n\\nwe'll get:\\n\\n```yaml\\nitems: [\\n    item 0 of 1st file, item 1 of 1st file, ... , item n of 1st file,  # ...\\n    item 0 of 2nd file, item 1 of 2nd file, ... , item n of 2nd file,  # ...\\n    # ....\\n    item 0 of n-th file, item 1 of n-th file, ... , item n of n-th file,  # ...\\n    # ...\\n]\\n```\\n\\n> \\u2139\\ufe0f **Note**\\n>\\n> - Only available when multiple files were matched.\\n> - **Every matched file should have a Sequence object in its top level**, or a `TypeError` exception may be thrown.\\n\\n### Serialization\\n\\nWhen load [YAML][] string with include statement, the including files are parsed into python objects by default. That is, if we call `yaml.dump()` on the object, what dumped is the parsed python object, and can not serialize the include statement itself.\\n\\nTo serialize the statement, we shall first create an `yaml_include.Constructor` object whose **`autoload` attribute is `False`**:\\n\\n```python\\nimport yaml\\nimport yaml_include\\n\\nctor = yaml_include.Constructor(autoload=False)\\n```\\n\\nthen add both Constructor for Loader and Representer for Dumper:\\n\\n```python\\nyaml.add_constructor(\\\"!inc\\\", ctor)\\n\\nrpr = yaml_include.Representer(\\\"inc\\\")\\nyaml.add_representer(yaml_include.Data, rpr)\\n```\\n\\nNow, the including files will not be loaded when call `yaml.load()`, and `yaml_include.Data` objects will be placed at the positions where include statements are.\\n\\ncontinue above code:\\n\\n```python\\nyaml_str = \\\"\\\"\\\"\\n- !inc include.d/1.yaml\\n- !inc include.d/2.yaml\\n\\\"\\\"\\\"\\n\\nd0 = yaml.load(yaml_str, yaml.Loader)\\n# Here, \\\"include.d/1.yaml\\\" and \\\"include.d/2.yaml\\\" not be opened or loaded.\\n# d0 is like:\\n# [Data(urlpath=\\\"include.d/1.yaml\\\"), Data(urlpath=\\\"include.d/2.yaml\\\")]\\n\\n# serialize d0\\ns = yaml.dump(d0)\\nprint(s)\\n# \\u2018s\\u2019 will be:\\n# - !inc 'include.d/1.yaml'\\n# - !inc 'include.d/2.yaml'\\n\\n# de-serialization\\nctor.autoload = True # re-open auto load\\n# then load, the file \\\"include.d/1.yaml\\\" and \\\"include.d/2.yaml\\\" will be opened and loaded.\\nd1 = yaml.load(s, yaml.Loader)\\n\\n# Or perform a recursive opening / parsing on the object:\\nd2 = yaml_include.load(d0) # d2 is equal to d1\\n```\\n\\n`autoload` can be used in a `with` statement:\\n\\n```python\\nctor = yaml_include.Constructor()\\n# autoload is True here\\n\\nwith ctor.managed_autoload(False):\\n    # temporary set autoload to False\\n    yaml.full_load(YAML_TEXT)\\n# autoload restore True automatic\\n```\\n\\n### Include JSON or TOML\\n\\nWe can include files in different format other than [YAML][], like [JSON][] or [TOML][] -- ``custom_loader`` is for that.\\n\\n> \\ud83d\\udcd1 **Example** \\\\\\n> For example:\\n>\\n> ```python\\n> import json\\n> import tomllib as toml\\n> import yaml\\n> import yaml_include\\n>\\n> # Define loader function\\n> def my_loader(urlpath, file, Loader):\\n>     if urlpath.endswith(\\\".json\\\"):\\n>         return json.load(file)\\n>     if urlpath.endswith(\\\".toml\\\"):\\n>         return toml.load(file)\\n>     return yaml.load(file, Loader)\\n>\\n> # Create the include constructor, with the custom loader\\n> ctor = yaml_include.Constructor(custom_loader=my_loader)\\n>\\n> # Add the constructor to YAML Loader\\n> yaml.add_constructor(\\\"!inc\\\", ctor, yaml.Loader)\\n>\\n> # Then, json files will can be loaded by std-lib's json module, and the same to toml files.\\n> s = \\\"\\\"\\\"\\n> json: !inc \\\"*.json\\\"\\n> toml: !inc \\\"*.toml\\\"\\n> yaml: !inc \\\"*.yaml\\\"\\n> \\\"\\\"\\\"\\n>\\n> yaml.load(s, yaml.Loader)\\n> ```\\n\\n## Develop\\n\\n1. clone the repo:\\n\\n   ```bash\\n   git clone https://github.com/tanbro/pyyaml-include.git\\n   cd pyyaml-include\\n   ```\\n\\n1. create then activate a python virtual-env:\\n\\n   ```bash\\n   python -m venv .venv\\n   .venv/bin/activate\\n   ```\\n\\n1. install development requirements and the project itself in editable mode:\\n\\n   ```bash\\n   pip install -r requirements.txt\\n   ```\\n\\nNow you can work on it.\\n\\n## Test\\n\\nread: `tests/README.md`\\n\\n[YAML]: http://yaml.org/ \\\"YAML: YAML Ain't Markup Language\\u2122\\\"\\n[PyYaml]: https://pypi.org/project/PyYAML/ \\\"PyYAML is a full-featured YAML framework for the Python programming language.\\\"\\n[fsspec]: https://github.com/fsspec/filesystem_spec/ \\\"Filesystem Spec (fsspec) is a project to provide a unified pythonic interface to local, remote and embedded file systems and bytes storage.\\\"\\n[JSON]: https://json.io/ \\\"JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write\\\"\\n[TOML]: https://toml.io/ \\\"TOML aims to be a minimal configuration file format that's easy to read due to obvious semantics.\\\"\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"An extending constructor of PyYAML: include other YAML files into current YAML document\", \"latest_version\": \"2.2\", \"weekly_downloads\": 799504, \"description_cleaned\": \"pyyaml-include\\nAn extending constructor of\\nPyYAML\\n: include other\\nYAML\\nfiles into current\\nYAML\\ndocument.\\nIn version\\n2.0\\n,\\nfsspec\\nwas introduced. With it, we can even include files by HTTP, SFTP, S3 ...\\n\\u26a0\\ufe0f\\nWarning\\n\\u201cpyyaml-include\\u201d\\n2.0\\nis\\nNOT compatible\\nwith\\n1.0\\nInstall\\npip\\ninstall\\n\\\"pyyaml-include\\\"\\nBecause\\nfsspec\\nwas introduced to open the including files since v2.0, an installation can be performed like below, if want to open remote files:\\nfor files on website:\\npip\\ninstall\\n\\\"pyyaml-include\\\"\\nfsspec\\n[\\nhttp\\n]\\nfor files on S3:\\npip\\ninstall\\n\\\"pyyaml-include\\\"\\nfsspec\\n[\\ns3\\n]\\nsee\\nfsspec\\n's documentation for more\\n\\ud83d\\udd16\\nTip\\n\\u201cpyyaml-include\\u201d depends on\\nfsspec\\n, it will be installed no matter including local or remote files.\\nBasic usages\\nConsider we have such\\nYAML\\nfiles:\\n\\u251c\\u2500\\u2500 0.yml\\n\\u2514\\u2500\\u2500 include.d\\n    \\u251c\\u2500\\u2500 1.yml\\n    \\u2514\\u2500\\u2500 2.yml\\n1.yml\\n's content:\\nname\\n:\\n\\\"1\\\"\\n2.yml\\n's content:\\nname\\n:\\n\\\"2\\\"\\nTo include\\n1.yml\\n,\\n2.yml\\nin\\n0.yml\\n, we shall:\\nRegister a\\nyaml_include.Constructor\\nto\\nPyYAML\\n's loader class, with\\n!inc\\n(or any other tags start with\\n!\\ncharacter) as it's tag:\\nimport\\nyaml\\nimport\\nyaml_include\\n# add the tag\\nyaml\\n.\\nadd_constructor\\n(\\n\\\"!inc\\\"\\n,\\nyaml_include\\n.\\nConstructor\\n(\\nbase_dir\\n=\\n'/your/conf/dir'\\n))\\nUse\\n!inc\\ntag(s) in\\n0.yaml\\n:\\nfile1\\n:\\n!inc\\ninclude.d/1.yml\\nfile2\\n:\\n!inc\\ninclude.d/2.yml\\nLoad\\n0.yaml\\nin your Python program\\nwith\\nopen\\n(\\n'0.yml'\\n)\\nas\\nf\\n:\\ndata\\n=\\nyaml\\n.\\nfull_load\\n(\\nf\\n)\\nprint\\n(\\ndata\\n)\\nwe'll get:\\n{\\n'file1'\\n:\\n{\\n'name'\\n:\\n'1'\\n},\\n'file2'\\n:\\n{\\n'name'\\n:\\n'2'\\n}}\\n(optional) the constructor can be unregistered:\\ndel\\nyaml\\n.\\nLoader\\n.\\nyaml_constructors\\n[\\n\\\"!inc\\\"\\n]\\ndel\\nyaml\\n.\\nUnSafeLoader\\n.\\nyaml_constructors\\n[\\n\\\"!inc\\\"\\n]\\ndel\\nyaml\\n.\\nFullLoader\\n.\\nyaml_constructors\\n[\\n\\\"!inc\\\"\\n]\\nInclude in Mapping\\nIf\\n0.yml\\nwas:\\nfile1\\n:\\n!inc\\ninclude.d/1.yml\\nfile2\\n:\\n!inc\\ninclude.d/2.yml\\nWe'll get:\\nfile1\\n:\\nname\\n:\\n\\\"1\\\"\\nfile2\\n:\\nname\\n:\\n\\\"2\\\"\\nInclude in Sequence\\nIf\\n0.yml\\nwas:\\nfiles\\n:\\n-\\n!inc\\ninclude.d/1.yml\\n-\\n!inc\\ninclude.d/2.yml\\nWe'll get:\\nfiles\\n:\\n-\\nname\\n:\\n\\\"1\\\"\\n-\\nname\\n:\\n\\\"2\\\"\\nAdvanced usages\\nWildcards\\nFile name can contain shell-style wildcards. Data loaded from the file(s) found by wildcards will be set in a sequence.\\nThat is, a list will be returned when including file name contains wildcards.\\nLength of the returned list equals number of matched files:\\nIf\\n0.yml\\nwas:\\nfiles\\n:\\n!inc\\ninclude.d/*.yml\\nWe'll get:\\nfiles\\n:\\n-\\nname\\n:\\n\\\"1\\\"\\n-\\nname\\n:\\n\\\"2\\\"\\nwhen only 1 file matched, length of list will be 1\\nwhen there are no files matched, an empty list will be returned\\nWe support\\n**\\n,\\n?\\nand\\n[..]\\n. We do not support\\n^\\nfor pattern negation.\\nThe\\nmaxdepth\\noption is applied on the first\\n**\\nfound in the path.\\n\\u2757\\nImportant\\nUsing the\\n**\\npattern in large directory trees or remote file system (S3, HTTP ...) may consume an inordinate amount of time.\\nThere is no method like lazy-load or iteration, all data of found files returned to the YAML doc-tree are fully loaded in memory, large amount of memory may be needed if there were many or big files.\\nWork with fsspec\\nIn\\nv2.0\\n, we use\\nfsspec\\nto open including files, thus we can include files from many different sources, such as local file system, S3, HTTP, SFTP ...\\nFor example, we can include a file from website in YAML:\\nconf\\n:\\nlogging\\n:\\n!inc\\nhttp://domain/etc/app/conf.d/logging.yml\\nIn such situations, when creating a\\nConstructor\\nconstructor, a\\nfsspec\\nfilesystem object shall be set to\\nfs\\nargument.\\nFor example, if want to include files from website, we shall:\\ncreate a\\nConstructor\\nwith a\\nfsspec\\nHTTP filesystem object as it's\\nfs\\n:\\nimport\\nyaml\\nimport\\nfsspec\\nimport\\nyaml_include\\nhttp_fs\\n=\\nfsspec\\n.\\nfilesystem\\n(\\n\\\"http\\\"\\n,\\nclient_kwargs\\n=\\n{\\n\\\"base_url\\\"\\n:\\nf\\n\\\"http://\\n{\\nHOST\\n}\\n:\\n{\\nPORT\\n}\\n\\\"\\n})\\nctor\\n=\\nyaml_include\\n.\\nConstructor\\n(\\nfs\\n=\\nhttp_fs\\n,\\nbase_dir\\n=\\n\\\"/foo/baz\\\"\\n)\\nyaml\\n.\\nadd_constructor\\n(\\n\\\"!inc\\\"\\n,\\nctor\\n,\\nyaml\\n.\\nLoader\\n)\\nthen, write a\\nYAML\\ndocument to include files from\\nhttp://${HOST}:${PORT}\\n:\\nkey1\\n:\\n!inc\\ndoc1.yml\\n# relative path to \\\"base_dir\\\"\\nkey2\\n:\\n!inc\\n./doc2.yml\\n# relative path to \\\"base_dir\\\" also\\nkey3\\n:\\n!inc\\n/doc3.yml\\n# absolute path, \\\"base_dir\\\" does not affect\\nkey3\\n:\\n!inc\\n../doc4.yml\\n# relative path one level upper to \\\"base_dir\\\"\\nload it with\\nPyYAML\\n:\\nyaml\\n.\\nload\\n(\\nyaml_string\\n,\\nyaml\\n.\\nLoader\\n)\\nAbove\\nYAML\\nsnippet will be loaded like:\\nkey1\\n: pared YAML of\\nhttp://${HOST}:${PORT}/foo/baz/doc1.yml\\nkey2\\n: pared YAML of\\nhttp://${HOST}:${PORT}/foo/baz/doc2.yml\\nkey3\\n: pared YAML of\\nhttp://${HOST}:${PORT}/doc3.yml\\nkey4\\n: pared YAML of\\nhttp://${HOST}:${PORT}/foo/doc4.yml\\n\\ud83d\\udd16\\nTip\\nCheck\\nfsspec\\n's documentation for more\\n\\u2139\\ufe0f\\nNote\\nIf\\nfs\\nargument is omitted, a\\n\\\"file\\\"\\n/\\n\\\"local\\\"\\nfsspec\\nfilesystem object will be used automatically. That is to say:\\ndata: !inc\\n:\\nfoo/baz.yaml\\nis equivalent to (if no\\nbase_dir\\nwas set in\\nConstructor()\\n):\\ndata: !inc\\n:\\nfile://foo/baz.yaml\\nand\\nyaml\\n.\\nadd_constructor\\n(\\n\\\"!inc\\\"\\n,\\nConstructor\\n())\\nis equivalent to:\\nyaml\\n.\\nadd_constructor\\n(\\n\\\"!inc\\\"\\n,\\nConstructor\\n(\\nfs\\n=\\nfsspec\\n.\\nfilesystem\\n(\\n\\\"file\\\"\\n)))\\nParameters in YAML\\nAs a callable object,\\nConstructor\\npasses YAML tag parameters to\\nfsspec\\nfor more detailed operations.\\nThe first argument is\\nurlpath\\n, it's fixed and must-required, either positional or named.\\nNormally, we put it as a string after the tag(eg:\\n!inc\\n), just like examples above.\\nHowever, there are more parameters.\\nin a sequence way, parameters will be passed to python as positional arguments, like\\n*args\\nin python function. eg:\\nfiles\\n:\\n!inc\\n[\\ninclude.d/**/*.yaml\\n,\\n{\\nmaxdepth\\n:\\n1\\n},\\n{\\nencoding\\n:\\nutf16\\n}]\\nin a mapping way, parameters will be passed to python as named arguments, like\\n**kwargs\\nin python function. eg:\\nfiles\\n:\\n!inc\\n{\\nurlpath\\n:\\n/foo/baz.yaml\\n,\\nencoding\\n:\\nutf16\\n}\\nBut the format of parameters has multiple cases, and differs variably in different\\nfsspec\\nimplementation backends.\\nIf a scheme/protocol(\\u201c\\nhttp://\\n\\u201d, \\u201c\\nsftp://\\n\\u201d, \\u201c\\nfile://\\n\\u201d, etc.) is defined, and there is no wildcard in\\nurlpath\\n,\\nConstructor\\nwill invoke\\nfsspec.open\\ndirectly to open it. Which means\\nConstructor\\n's\\nfs\\nwill be ignored, and a new standalone\\nfs\\nwill be created implicitly.\\nIn this situation,\\nurlpath\\nwill be passed to\\nfsspec.open\\n's first argument, and all other parameters will also be passed to the function.\\nFor example,\\nthe\\nYAML\\nsnippet\\nfiles\\n:\\n!inc\\n[\\nfile\\n:\\n///foo/baz.yaml\\n,\\nr\\n]\\nwill cause python code like\\nwith\\nfsspec\\n.\\nopen\\n(\\n\\\"file:///foo/baz.yaml\\\"\\n,\\n\\\"r\\\"\\n)\\nas\\nf\\n:\\nyaml\\n.\\nload\\n(\\nf\\n,\\nLoader\\n)\\nand the\\nYAML\\nsnippet\\nfiles\\n:\\n!inc\\n{\\nurlpath\\n:\\nfile\\n:\\n///foo/baz.yaml\\n,\\nencoding\\n:\\nutf16\\n}\\nwill cause python code like\\nwith\\nfsspec\\n.\\nopen\\n(\\n\\\"file:///foo/baz.yaml\\\"\\n,\\nencoding\\n=\\n\\\"utf16\\\"\\n)\\nas\\nf\\n:\\nyaml\\n.\\nload\\n(\\nf\\n,\\nLoader\\n)\\nIf\\nurlpath\\nhas wildcard, and also scheme in it,\\nConstructor\\nwill:\\nInvoke\\nfsspec\\n's\\nopen_files\\nfunction to search, open and load files, and return the results in a list.\\nYAML\\ninclude statement's parameters are passed to\\nopen_files\\nfunction.\\nIf\\nurlpath\\nhas wildcard, and no scheme in it,\\nConstructor\\nwill:\\ninvoke corresponding\\nfsspec\\nimplementation backend's\\nglob\\nmethod to search files,\\nthen call\\nopen\\nmethod to open each found file(s).\\nurlpath\\nwill be passed as the first argument to both\\nglob\\nand\\nopen\\nmethod of the corresponding\\nfsspec\\nimplementation backend, and other parameters will also be passed to\\nglob\\nand\\nopen\\nmethod as their following arguments.\\nIn the case of wildcards, what need to pay special attention to is that there are\\ntwo separated parameters\\nafter\\nurlpath\\n, the first is for\\nglob\\nmethod, and the second is for\\nopen\\nmethod. Each of them could be either sequence, mapping or scalar, corresponds single, positional and named argument(s) in python. For example:\\nIf we want to include every\\n.yml\\nfile in directory\\netc/app\\nrecursively with max depth at 2, and open them in utf-16 codec, we shall write the\\nYAML\\nas below:\\nfiles\\n:\\n!inc\\n[\\n\\\"etc/app/**/*.yml\\\"\\n,\\n{\\nmaxdepth\\n:\\n!!int\\n\\\"2\\\"\\n},\\n{\\nencoding\\n:\\nutf16\\n}]\\nit will cause python code like:\\nfor\\nfile\\nin\\nlocal_fs\\n.\\nglob\\n(\\n\\\"etc/app/**/*.yml\\\"\\n,\\nmaxdepth\\n=\\n2\\n):\\nwith\\nlocal_fs\\n.\\nopen\\n(\\nfile\\n,\\nencoding\\n=\\n\\\"utf16\\\"\\n)\\nas\\nf\\n:\\nyaml\\n.\\nload\\n(\\nf\\n,\\nLoader\\n)\\nSince\\nmaxdepth\\nis the seconde argument after\\npath\\nin\\nglob\\nmethod, we can also write the\\nYAML\\nlike this:\\nfiles\\n:\\n!inc\\n[\\n\\\"etc/app/**/*.yml\\\"\\n,\\n[\\n!!int\\n\\\"2\\\"\\n]]\\nThe parameters for\\nopen\\nis omitted, means no more arguments except\\nurlpath\\nis passed.\\nit will cause python code like:\\nfor\\nfile\\nin\\nlocal_fs\\n.\\nglob\\n(\\n\\\"etc/app/**/*.yml\\\"\\n,\\n2\\n):\\nwith\\nlocal_fs\\n.\\nopen\\n(\\nfile\\n)\\nas\\nf\\n:\\nyaml\\n.\\nload\\n(\\nf\\n,\\nLoader\\n)\\nThe two parameters can be in a mapping form, and name of the keys are\\n\\\"glob\\\"\\nand\\n\\\"open\\\"\\n. for example:\\nfiles\\n:\\n!inc\\n{\\nurlpath\\n:\\n\\\"etc/app/**/*.yml\\\"\\n,\\nglob\\n:\\n[\\n!!int\\n\\\"2\\\"\\n],\\nopen\\n:\\n{\\nencoding\\n:\\nutf16\\n}}\\n\\u2757\\nImportant\\nPyYAML\\nsometimes takes scalar parameter of custom constructor as string, we can use a \\u2018Standard YAML tag\\u2019 to ensure non-string data type in the situation.\\nFor example, following\\nYAML\\nsnippet may cause an error:\\nfiles\\n:\\n!inc\\n[\\n\\\"etc/app/**/*.yml\\\"\\n,\\nopen\\n:\\n{\\nintParam\\n:\\n1\\n}]\\nBecause\\nPyYAML\\ntreats\\n{\\\"intParam\\\": 1}\\nas\\n{\\\"intParam\\\": \\\"1\\\"}\\n, which makes python code like\\nfs.open(path, intParam=\\\"1\\\")\\n. To prevent this, we shall write the\\nYAML\\nlike:\\nfiles\\n:\\n!inc\\n[\\n\\\"etc/app/**/*.yml\\\"\\n,\\nopen\\n:\\n{\\nintParam\\n:\\n!!int\\n1\\n}]\\nwhere\\n!!int\\nis a \\u2018Standard YAML tag\\u2019 to force integer type of\\nmaxdepth\\nargument.\\n\\u2139\\ufe0f\\nNote\\nBaseLoader\\n,\\nSafeLoader\\n,\\nCBaseLoader\\n,\\nCSafeLoader\\ndo\\nNOT\\nsupport \\u2018Standard YAML tag\\u2019.\\n\\ud83d\\udd16\\nTip\\nmaxdepth\\nargument of\\nfsspec\\nglob\\nmethod is already force converted by\\nConstructor\\n, no need to write a\\n!!int\\ntag on it.\\nElse,\\nConstructor\\nwill invoke corresponding\\nfsspec\\nimplementation backend's\\nopen\\nmethod to open the file, parameters beside\\nurlpath\\nwill be passed to the method.\\nAbsolute and Relative URL/Path\\nWhen the path after include tag (eg:\\n!inc\\n) is not a full protocol/scheme URL and not starts with\\n\\\"/\\\"\\n,\\nConstructor\\ntries to join the path with\\nbase_dir\\n, which is a argument of\\nConstructor.__init__()\\n.\\nIf\\nbase_dir\\nis omitted or\\nNone\\n, the actually including file path is the path in defined in\\nYAML\\nwithout a change, and different\\nfsspec\\nfilesystem will treat them differently. In local filesystem, it will be\\ncwd\\n.\\nFor remote filesystem,\\nHTTP\\nfor example, the\\nbase_dir\\ncan not be\\nNone\\nand usually be set to\\n\\\"/\\\"\\n.\\nRelative path does not support full protocol/scheme URL format,\\nbase_dir\\ndoes not effect for that.\\nFor example, if we register such a\\nConstructor\\nto\\nPyYAML\\n:\\nimport\\nyaml\\nimport\\nfsspec\\nimport\\nyaml_include\\nyaml\\n.\\nadd_constructor\\n(\\n\\\"!http-include\\\"\\n,\\nyaml_include\\n.\\nConstructor\\n(\\nfsspec\\n.\\nfilesystem\\n(\\n\\\"http\\\"\\n,\\nclient_kwargs\\n=\\n{\\n\\\"base_url\\\"\\n:\\nf\\n\\\"http://\\n{\\nHOST\\n}\\n:\\n{\\nPORT\\n}\\n\\\"\\n}),\\nbase_dir\\n=\\n\\\"/sub_1/sub_1_1\\\"\\n)\\n)\\nthen, load following\\nYAML\\n:\\nxyz\\n:\\n!http-include\\nxyz.yml\\nthe actual URL to access is\\nhttp://$HOST:$PORT/sub_1/sub_1_1/xyz.yml\\nFlatten sequence object in multiple matched files\\nConsider we have such a YAML:\\nitems\\n:\\n!include\\n\\\"*.yaml\\\"\\nIf every file matches\\n*.yaml\\ncontains a sequence object at the top level in it, what parsed and loaded will be:\\nitems\\n:\\n[\\n[\\nitem 0 of 1st file\\n,\\nitem 1 of 1st file\\n,\\n...\\n,\\nitem n of 1st file\\n,\\n...\\n],\\n[\\nitem 0 of 2nd file\\n,\\nitem 1 of 2nd file\\n,\\n...\\n,\\nitem n of 2nd file\\n,\\n...\\n],\\n# ....\\n[\\nitem 0 of nth file\\n,\\nitem 1 of nth file\\n,\\n...\\n,\\nitem n of nth file\\n,\\n...\\n],\\n# ...\\n]\\nIt's a 2-dim array, because YAML content of each matched file is treated as a member of the list(sequence).\\nBut if\\nflatten\\nparameter was set to\\ntrue\\n, like:\\nitems\\n:\\n!include\\n{\\nurlpath\\n:\\n\\\"*.yaml\\\"\\n,\\nflatten\\n:\\ntrue\\n}\\nwe'll get:\\nitems\\n:\\n[\\nitem 0 of 1st file\\n,\\nitem 1 of 1st file\\n,\\n...\\n,\\nitem n of 1st file\\n,\\n# ...\\nitem 0 of 2nd file\\n,\\nitem 1 of 2nd file\\n,\\n...\\n,\\nitem n of 2nd file\\n,\\n# ...\\n# ....\\nitem 0 of n-th file\\n,\\nitem 1 of n-th file\\n,\\n...\\n,\\nitem n of n-th file\\n,\\n# ...\\n# ...\\n]\\n\\u2139\\ufe0f\\nNote\\nOnly available when multiple files were matched.\\nEvery matched file should have a Sequence object in its top level\\n, or a\\nTypeError\\nexception may be thrown.\\nSerialization\\nWhen load\\nYAML\\nstring with include statement, the including files are parsed into python objects by default. That is, if we call\\nyaml.dump()\\non the object, what dumped is the parsed python object, and can not serialize the include statement itself.\\nTo serialize the statement, we shall first create an\\nyaml_include.Constructor\\nobject whose\\nautoload\\nattribute is\\nFalse\\n:\\nimport\\nyaml\\nimport\\nyaml_include\\nctor\\n=\\nyaml_include\\n.\\nConstructor\\n(\\nautoload\\n=\\nFalse\\n)\\nthen add both Constructor for Loader and Representer for Dumper:\\nyaml\\n.\\nadd_constructor\\n(\\n\\\"!inc\\\"\\n,\\nctor\\n)\\nrpr\\n=\\nyaml_include\\n.\\nRepresenter\\n(\\n\\\"inc\\\"\\n)\\nyaml\\n.\\nadd_representer\\n(\\nyaml_include\\n.\\nData\\n,\\nrpr\\n)\\nNow, the including files will not be loaded when call\\nyaml.load()\\n, and\\nyaml_include.Data\\nobjects will be placed at the positions where include statements are.\\ncontinue above code:\\nyaml_str\\n=\\n\\\"\\\"\\\"\\n- !inc include.d/1.yaml\\n- !inc include.d/2.yaml\\n\\\"\\\"\\\"\\nd0\\n=\\nyaml\\n.\\nload\\n(\\nyaml_str\\n,\\nyaml\\n.\\nLoader\\n)\\n# Here, \\\"include.d/1.yaml\\\" and \\\"include.d/2.yaml\\\" not be opened or loaded.\\n# d0 is like:\\n# [Data(urlpath=\\\"include.d/1.yaml\\\"), Data(urlpath=\\\"include.d/2.yaml\\\")]\\n# serialize d0\\ns\\n=\\nyaml\\n.\\ndump\\n(\\nd0\\n)\\nprint\\n(\\ns\\n)\\n# \\u2018s\\u2019 will be:\\n# - !inc 'include.d/1.yaml'\\n# - !inc 'include.d/2.yaml'\\n# de-serialization\\nctor\\n.\\nautoload\\n=\\nTrue\\n# re-open auto load\\n# then load, the file \\\"include.d/1.yaml\\\" and \\\"include.d/2.yaml\\\" will be opened and loaded.\\nd1\\n=\\nyaml\\n.\\nload\\n(\\ns\\n,\\nyaml\\n.\\nLoader\\n)\\n# Or perform a recursive opening / parsing on the object:\\nd2\\n=\\nyaml_include\\n.\\nload\\n(\\nd0\\n)\\n# d2 is equal to d1\\nautoload\\ncan be used in a\\nwith\\nstatement:\\nctor\\n=\\nyaml_include\\n.\\nConstructor\\n()\\n# autoload is True here\\nwith\\nctor\\n.\\nmanaged_autoload\\n(\\nFalse\\n):\\n# temporary set autoload to False\\nyaml\\n.\\nfull_load\\n(\\nYAML_TEXT\\n)\\n# autoload restore True automatic\\nInclude JSON or TOML\\nWe can include files in different format other than\\nYAML\\n, like\\nJSON\\nor\\nTOML\\n--\\ncustom_loader\\nis for that.\\n\\ud83d\\udcd1\\nExample\\nFor example:\\nimport\\njson\\nimport\\ntomllib\\nas\\ntoml\\nimport\\nyaml\\nimport\\nyaml_include\\n# Define loader function\\ndef\\nmy_loader\\n(\\nurlpath\\n,\\nfile\\n,\\nLoader\\n):\\nif\\nurlpath\\n.\\nendswith\\n(\\n\\\".json\\\"\\n):\\nreturn\\njson\\n.\\nload\\n(\\nfile\\n)\\nif\\nurlpath\\n.\\nendswith\\n(\\n\\\".toml\\\"\\n):\\nreturn\\ntoml\\n.\\nload\\n(\\nfile\\n)\\nreturn\\nyaml\\n.\\nload\\n(\\nfile\\n,\\nLoader\\n)\\n# Create the include constructor, with the custom loader\\nctor\\n=\\nyaml_include\\n.\\nConstructor\\n(\\ncustom_loader\\n=\\nmy_loader\\n)\\n# Add the constructor to YAML Loader\\nyaml\\n.\\nadd_constructor\\n(\\n\\\"!inc\\\"\\n,\\nctor\\n,\\nyaml\\n.\\nLoader\\n)\\n# Then, json files will can be loaded by std-lib's json module, and the same to toml files.\\ns\\n=\\n\\\"\\\"\\\"\\njson: !inc \\\"*.json\\\"\\ntoml: !inc \\\"*.toml\\\"\\nyaml: !inc \\\"*.yaml\\\"\\n\\\"\\\"\\\"\\nyaml\\n.\\nload\\n(\\ns\\n,\\nyaml\\n.\\nLoader\\n)\\nDevelop\\nclone the repo:\\ngit\\nclone\\nhttps://github.com/tanbro/pyyaml-include.git\\ncd\\npyyaml-include\\ncreate then activate a python virtual-env:\\npython\\n-m\\nvenv\\n.venv\\n.venv/bin/activate\\ninstall development requirements and the project itself in editable mode:\\npip\\ninstall\\n-r\\nrequirements.txt\\nNow you can work on it.\\nTest\\nread:\\ntests/README.md\"}, {\"name\": \"apache-airflow-core\", \"description\": \"<!--\\n Licensed to the Apache Software Foundation (ASF) under one\\n or more contributor license agreements.  See the NOTICE file\\n distributed with this work for additional information\\n regarding copyright ownership.  The ASF licenses this file\\n to you under the Apache License, Version 2.0 (the\\n \\\"License\\\"); you may not use this file except in compliance\\n with the License.  You may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\n Unless required by applicable law or agreed to in writing,\\n software distributed under the License is distributed on an\\n \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\n KIND, either express or implied.  See the License for the\\n specific language governing permissions and limitations\\n under the License.\\n -->\\n\\nThis is a core airflow package that contains the functionality of Apache Airflow components:\\nscheduler, API server, dag file processor and triggerer.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Core packages for Apache Airflow, schedule and API server\", \"latest_version\": \"3.1.3\", \"weekly_downloads\": 797299, \"description_cleaned\": \"This is a core airflow package that contains the functionality of Apache Airflow components:\\nscheduler, API server, dag file processor and triggerer.\"}, {\"name\": \"construct\", \"description\": \"Construct 2.10\\n===================\\n\\nConstruct is a powerful **declarative** and **symmetrical** parser and builder for binary data.\\n\\nInstead of writing *imperative code* to parse a piece of data, you declaratively define a *data structure* that describes your data. As this data structure is not code, you can use it in one direction to *parse* data into Pythonic objects, and in the other direction, to *build* objects into binary data.\\n\\nThe library provides both simple, atomic constructs (such as integers of various sizes), as well as composite ones which allow you form hierarchical and sequential structures of increasing complexity. Construct features **bit and byte granularity**, easy debugging and testing, an **easy-to-extend subclass system**, and lots of primitive constructs to make your work easier:\\n\\n* Fields: raw bytes or numerical types\\n* Structs and Sequences: combine simpler constructs into more complex ones\\n* Bitwise: splitting bytes into bit-grained fields\\n* Adapters: change how data is represented\\n* Arrays/Ranges: duplicate constructs\\n* Meta-constructs: use the context (history) to compute the size of data\\n* If/Switch: branch the computational path based on the context\\n* On-demand (lazy) parsing: read and parse only what you require\\n* Pointers: jump from here to there in the data stream\\n* Tunneling: prefix data with a byte count or compress it\\n\\n\\nExample\\n---------\\n\\nA ``Struct`` is a collection of ordered, named fields::\\n\\n    >>> format = Struct(\\n    ...     \\\"signature\\\" / Const(b\\\"BMP\\\"),\\n    ...     \\\"width\\\" / Int8ub,\\n    ...     \\\"height\\\" / Int8ub,\\n    ...     \\\"pixels\\\" / Array(this.width * this.height, Byte),\\n    ... )\\n    >>> format.build(dict(width=3,height=2,pixels=[7,8,9,11,12,13]))\\n    b'BMP\\\\x03\\\\x02\\\\x07\\\\x08\\\\t\\\\x0b\\\\x0c\\\\r'\\n    >>> format.parse(b'BMP\\\\x03\\\\x02\\\\x07\\\\x08\\\\t\\\\x0b\\\\x0c\\\\r')\\n    Container(signature=b'BMP')(width=3)(height=2)(pixels=[7, 8, 9, 11, 12, 13])\\n\\nA ``Sequence`` is a collection of ordered fields, and differs from ``Array`` and ``GreedyRange`` in that those two are homogenous::\\n\\n    >>> format = Sequence(PascalString(Byte, \\\"utf8\\\"), GreedyRange(Byte))\\n    >>> format.build([u\\\"lalaland\\\", [255,1,2]])\\n    b'\\\\nlalaland\\\\xff\\\\x01\\\\x02'\\n    >>> format.parse(b\\\"\\\\x004361789432197\\\")\\n    ['', [52, 51, 54, 49, 55, 56, 57, 52, 51, 50, 49, 57, 55]]\\n\", \"description_content_type\": null, \"summary\": \"A powerful declarative symmetric parser/builder for binary data\", \"latest_version\": \"2.10.70\", \"weekly_downloads\": 796264, \"description_cleaned\": \"Construct is a powerful\\ndeclarative\\nand\\nsymmetrical\\nparser and builder for binary data.\\nInstead of writing\\nimperative code\\nto parse a piece of data, you declaratively define a\\ndata structure\\nthat describes your data. As this data structure is not code, you can use it in one direction to\\nparse\\ndata into Pythonic objects, and in the other direction, to\\nbuild\\nobjects into binary data.\\nThe library provides both simple, atomic constructs (such as integers of various sizes), as well as composite ones which allow you form hierarchical and sequential structures of increasing complexity. Construct features\\nbit and byte granularity\\n, easy debugging and testing, an\\neasy-to-extend subclass system\\n, and lots of primitive constructs to make your work easier:\\nFields: raw bytes or numerical types\\nStructs and Sequences: combine simpler constructs into more complex ones\\nBitwise: splitting bytes into bit-grained fields\\nAdapters: change how data is represented\\nArrays/Ranges: duplicate constructs\\nMeta-constructs: use the context (history) to compute the size of data\\nIf/Switch: branch the computational path based on the context\\nOn-demand (lazy) parsing: read and parse only what you require\\nPointers: jump from here to there in the data stream\\nTunneling: prefix data with a byte count or compress it\\nExample\\nA\\nStruct\\nis a collection of ordered, named fields:\\n>>> format = Struct(\\n...     \\\"signature\\\" / Const(b\\\"BMP\\\"),\\n...     \\\"width\\\" / Int8ub,\\n...     \\\"height\\\" / Int8ub,\\n...     \\\"pixels\\\" / Array(this.width * this.height, Byte),\\n... )\\n>>> format.build(dict(width=3,height=2,pixels=[7,8,9,11,12,13]))\\nb'BMP\\\\x03\\\\x02\\\\x07\\\\x08\\\\t\\\\x0b\\\\x0c\\\\r'\\n>>> format.parse(b'BMP\\\\x03\\\\x02\\\\x07\\\\x08\\\\t\\\\x0b\\\\x0c\\\\r')\\nContainer(signature=b'BMP')(width=3)(height=2)(pixels=[7, 8, 9, 11, 12, 13])\\nA\\nSequence\\nis a collection of ordered fields, and differs from\\nArray\\nand\\nGreedyRange\\nin that those two are homogenous:\\n>>> format = Sequence(PascalString(Byte, \\\"utf8\\\"), GreedyRange(Byte))\\n>>> format.build([u\\\"lalaland\\\", [255,1,2]])\\nb'\\\\nlalaland\\\\xff\\\\x01\\\\x02'\\n>>> format.parse(b\\\"\\\\x004361789432197\\\")\\n['', [52, 51, 54, 49, 55, 56, 57, 52, 51, 50, 49, 57, 55]]\"}, {\"name\": \"pytest-order\", \"description\": \"_pytest-order_ - a pytest plugin to order test execution\\n========================================================\\n\\n[![PyPI version](https://badge.fury.io/py/pytest-order.svg)](https://pypi.org/project/pytest-order) [![Testsuite](https://github.com/pytest-dev/pytest-order/workflows/Testsuite/badge.svg)](https://github.com/pytest-dev/pytest-order/actions?query=workflow%3ATestsuite) [![DocBuild](https://readthedocs.org/projects/pytest-order/badge/?version=latest)](https://pytest-order.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/pytest-dev/pytest-order/branch/main/graph/badge.svg?token=M9PHWZSHUU)](https://codecov.io/gh/pytest-dev/pytest-order) [![Python version](https://img.shields.io/pypi/pyversions/pytest-order.svg)](https://pypi.org/project/pytest-order)\\n\\n`pytest-order` is a pytest plugin that allows you to customize the order in which\\nyour tests are run. It uses the marker `order` that defines when a specific\\ntest shall run, either by using an ordinal number, or by specifying the\\nrelationship to other tests.\\n\\n`pytest-order` is a fork of\\n[pytest-ordering](https://github.com/ftobia/pytest-ordering) that provides\\nadditional features like ordering relative to other tests.\\n\\n`pytest-order` works with Python 3.7 - 3.12, with pytest\\nversions >= 5.0.0 for all versions up to Python 3.9, and for pytest >=\\n6.2.4 for Python >= 3.10. `pytest-order` runs on Linux, macOS and Windows.\\n\\nDocumentation\\n-------------\\nApart from this overview, the following information is available:\\n- usage documentation for the [latest release](https://pytest-order.readthedocs.io/en/stable/)\\n- usage documentation for the [current main branch](https://pytest-order.readthedocs.io/en/latest/)\\n- most examples shown in the documentation can also be found in the\\n  [repository](https://github.com/pytest-dev/pytest-order/tree/main/example)\\n- the [Release Notes](https://github.com/pytest-dev/pytest-order/blob/main/CHANGELOG.md)\\n  with a list of changes in the latest versions\\n- a [list of open issues](https://github.com/pytest-dev/pytest-order/blob/main/old_issues.md)\\n  in the original project and their handling in `pytest-order`\\n\\nFeatures\\n--------\\n`pytest-order` provides the following features:\\n- ordering of tests [by index](https://pytest-order.readthedocs.io/en/stable/usage.html#ordering-by-numbers)\\n- ordering of tests both from the start and from the end (via negative\\n  index)\\n- ordering of tests [relative to each other](https://pytest-order.readthedocs.io/en/stable/usage.html#order-relative-to-other-tests)\\n  (via the `before` and `after` marker attributes)\\n- session-, module- and class-scope ordering via the\\n  [order-scope](https://pytest-order.readthedocs.io/en/stable/configuration.html#order-scope) option\\n- directory scope ordering via the\\n  [order-scope-level](https://pytest-order.readthedocs.io/en/stable/configuration.html#order-scope-level) option\\n- hierarchical module and class-level ordering via the\\n  [order-group-scope](https://pytest-order.readthedocs.io/en/stable/configuration.html#order-group-scope) option\\n- ordering tests with `pytest-dependency` markers if using the\\n  [order-dependencies](https://pytest-order.readthedocs.io/en/stable/configuration.html#order-dependencies) option,\\n  more information about `pytest-dependency` compatibility\\n  [here](https://pytest-order.readthedocs.io/en/stable/other_plugins.html#relationship-with-pytest-dependency)\\n- sparse ordering of tests via the\\n  [sparse-ordering](https://pytest-order.readthedocs.io/en/stable/configuration.html#sparse-ordering) option\\n- usage of custom markers for ordering using the\\n  [order-marker-prefix](https://pytest-order.readthedocs.io/en/stable/configuration.html#order-marker-prefix) option\\n\\nOverview\\n--------\\n_(adapted from the original project)_\\n\\nHave you ever wanted to easily run one of your tests before any others run?\\nOr run some tests last? Or run this one test before that other test? Or\\nmake sure that this group of tests runs after this other group of tests?\\n\\nNow you can.\\n\\nInstall with:\\n\\n    pip install pytest-order\\n\\nThis defines the ``order`` marker that you can use in your code with\\ndifferent attributes.\\n\\nFor example, this code:\\n\\n    import pytest\\n\\n    @pytest.mark.order(2)\\n    def test_foo():\\n        assert True\\n\\n    @pytest.mark.order(1)\\n    def test_bar():\\n        assert True\\n\\nyields the output:\\n\\n    $ pytest test_foo.py -vv\\n    ============================= test session starts ==============================\\n    platform darwin -- Python 3.7.1, pytest-5.4.3, py-1.8.1, pluggy-0.13.1 -- env/bin/python\\n    plugins: order\\n    collected 2 items\\n\\n    test_foo.py:7: test_bar PASSED\\n    test_foo.py:3: test_foo PASSED\\n\\n    =========================== 2 passed in 0.01 seconds ===========================\\n\\nContributing\\n------------\\nContributions are very welcome. Tests can be run with\\n[tox](https://tox.readthedocs.io/en/latest/), please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\nLicense\\n-------\\nDistributed under the terms of the [MIT](http://opensource.org/licenses/MIT)\\nlicense, `pytest-order` is free and open source software.\\n\\nHistory\\n-------\\nThis is a fork of [pytest-ordering](https://github.com/ftobia/pytest-ordering).\\nThat project is not maintained anymore, and there are several helpful PRs\\nthat are now integrated into `pytest-order`. The idea and most of the\\ninitial code has been created by Frank Tobia, the author of that plugin, and\\n[contributors](https://github.com/pytest-dev/pytest-order/blob/main/AUTHORS).\\n\\nWhile derived from `pytest_ordering`, `pytest-order` is **not** compatible\\nwith `pytest-ordering` due to the changed marker name (`order` instead of\\n`run`). Additional markers defined in `pytest_ordering` are all integrated\\ninto the `order` marker (for a rationale see also\\n[this issue](https://github.com/ftobia/pytest-ordering/issues/38)).\\n\\nOrdering relative to other tests and all the configuration options are not\\navailable in the released version of `pytest-ordering`.\\nHowever, most of these features are derived from or inspired by\\n[issues](https://github.com/pytest-dev/pytest-order/blob/main/old_issues.md)\\nand pull requests already existing in `pytest-ordering`.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"pytest plugin to run your tests in a specific order\", \"latest_version\": \"1.3.0\", \"weekly_downloads\": 795477, \"description_cleaned\": \"pytest-order\\n- a pytest plugin to order test execution\\npytest-order\\nis a pytest plugin that allows you to customize the order in which\\nyour tests are run. It uses the marker\\norder\\nthat defines when a specific\\ntest shall run, either by using an ordinal number, or by specifying the\\nrelationship to other tests.\\npytest-order\\nis a fork of\\npytest-ordering\\nthat provides\\nadditional features like ordering relative to other tests.\\npytest-order\\nworks with Python 3.7 - 3.12, with pytest\\nversions >= 5.0.0 for all versions up to Python 3.9, and for pytest >=\\n6.2.4 for Python >= 3.10.\\npytest-order\\nruns on Linux, macOS and Windows.\\nDocumentation\\nApart from this overview, the following information is available:\\nusage documentation for the\\nlatest release\\nusage documentation for the\\ncurrent main branch\\nmost examples shown in the documentation can also be found in the\\nrepository\\nthe\\nRelease Notes\\nwith a list of changes in the latest versions\\na\\nlist of open issues\\nin the original project and their handling in\\npytest-order\\nFeatures\\npytest-order\\nprovides the following features:\\nordering of tests\\nby index\\nordering of tests both from the start and from the end (via negative\\nindex)\\nordering of tests\\nrelative to each other\\n(via the\\nbefore\\nand\\nafter\\nmarker attributes)\\nsession-, module- and class-scope ordering via the\\norder-scope\\noption\\ndirectory scope ordering via the\\norder-scope-level\\noption\\nhierarchical module and class-level ordering via the\\norder-group-scope\\noption\\nordering tests with\\npytest-dependency\\nmarkers if using the\\norder-dependencies\\noption,\\nmore information about\\npytest-dependency\\ncompatibility\\nhere\\nsparse ordering of tests via the\\nsparse-ordering\\noption\\nusage of custom markers for ordering using the\\norder-marker-prefix\\noption\\nOverview\\n(adapted from the original project)\\nHave you ever wanted to easily run one of your tests before any others run?\\nOr run some tests last? Or run this one test before that other test? Or\\nmake sure that this group of tests runs after this other group of tests?\\nNow you can.\\nInstall with:\\npip install pytest-order\\nThis defines the\\norder\\nmarker that you can use in your code with\\ndifferent attributes.\\nFor example, this code:\\nimport pytest\\n\\n@pytest.mark.order(2)\\ndef test_foo():\\n    assert True\\n\\n@pytest.mark.order(1)\\ndef test_bar():\\n    assert True\\nyields the output:\\n$ pytest test_foo.py -vv\\n============================= test session starts ==============================\\nplatform darwin -- Python 3.7.1, pytest-5.4.3, py-1.8.1, pluggy-0.13.1 -- env/bin/python\\nplugins: order\\ncollected 2 items\\n\\ntest_foo.py:7: test_bar PASSED\\ntest_foo.py:3: test_foo PASSED\\n\\n=========================== 2 passed in 0.01 seconds ===========================\\nContributing\\nContributions are very welcome. Tests can be run with\\ntox\\n, please ensure\\nthe coverage at least stays the same before you submit a pull request.\\nLicense\\nDistributed under the terms of the\\nMIT\\nlicense,\\npytest-order\\nis free and open source software.\\nHistory\\nThis is a fork of\\npytest-ordering\\n.\\nThat project is not maintained anymore, and there are several helpful PRs\\nthat are now integrated into\\npytest-order\\n. The idea and most of the\\ninitial code has been created by Frank Tobia, the author of that plugin, and\\ncontributors\\n.\\nWhile derived from\\npytest_ordering\\n,\\npytest-order\\nis\\nnot\\ncompatible\\nwith\\npytest-ordering\\ndue to the changed marker name (\\norder\\ninstead of\\nrun\\n). Additional markers defined in\\npytest_ordering\\nare all integrated\\ninto the\\norder\\nmarker (for a rationale see also\\nthis issue\\n).\\nOrdering relative to other tests and all the configuration options are not\\navailable in the released version of\\npytest-ordering\\n.\\nHowever, most of these features are derived from or inspired by\\nissues\\nand pull requests already existing in\\npytest-ordering\\n.\"}, {\"name\": \"azure-cosmosdb-nspkg\", \"description\": \"Microsoft Azure CosmosDB SDK for Python\\r\\n=======================================\\r\\n\\r\\nThis is the Microsoft Azure CosmosDB namespace package.\\r\\n\\r\\nThis package is not intended to be installed directly by the end user.\\r\\n\\r\\nIt provides the necessary files for other packages to extend the azure.cosmosdb namespace.\\r\\n\\r\\nIf you are looking to install the Azure CosmosDB libraries, see the\\r\\n`azure <https://pypi.python.org/pypi/azure>`__ bundle package.\\r\\n\\r\\n\\r\\n\", \"description_content_type\": null, \"summary\": \"Microsoft Azure CosmosDB Namespace Package [Internal]\", \"latest_version\": \"2.0.2\", \"weekly_downloads\": 794406, \"description_cleaned\": \"This is the Microsoft Azure CosmosDB namespace package.\\nThis package is not intended to be installed directly by the end user.\\nIt provides the necessary files for other packages to extend the azure.cosmosdb namespace.\\nIf you are looking to install the Azure CosmosDB libraries, see the\\nazure\\nbundle package.\"}, {\"name\": \"valkey\", \"description\": \"# valkey-py\\n\\nThe Python interface to the Valkey key-value store.\\n\\n[![CI](https://github.com/valkey-io/valkey-py/workflows/CI/badge.svg)](https://github.com/valkey-io/valkey-py/actions?query=workflow%3ACI+branch%3Amain)\\n[![docs](https://readthedocs.org/projects/valkey-py/badge/?version=latest&style=flat)](https://valkey-py.readthedocs.io/en/latest/)\\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)\\n[![pypi](https://badge.fury.io/py/valkey.svg)](https://pypi.org/project/valkey/)\\n[![pre-release](https://img.shields.io/github/v/release/valkey-io/valkey-py?include_prereleases&label=latest-prerelease)](https://github.com/valkey-io/valkey-py/releases)\\n[![codecov](https://codecov.io/gh/valkey-io/valkey-py/branch/main/graph/badge.svg?token=yenl5fzxxr)](https://codecov.io/gh/valkey-io/valkey-py)\\n\\n[Installation](#installation) |  [Usage](#usage) | [Documentation](#documentation) | [Advanced Topics](#advanced-topics) | [Contributing](https://github.com/valkey-io/valkey-py/blob/main/CONTRIBUTING.md)\\n\\n---------------------------------------------\\n\\n## Installation\\n\\nStart a valkey via docker:\\n\\n``` bash\\ndocker run -p 6379:6379 -it valkey/valkey:latest\\n```\\n\\nTo install valkey-py, simply:\\n\\n``` bash\\n$ pip install valkey\\n```\\n\\nFor faster performance, install valkey with libvalkey support, this provides a compiled response parser, and *for most cases* requires zero code changes.\\nBy default, if libvalkey >= 2.3.2 is available, valkey-py will attempt to use it for response parsing.\\n\\n``` bash\\n$ pip install \\\"valkey[libvalkey]\\\"\\n```\\n\\n## Usage\\n\\n### Basic Example\\n\\n``` python\\n>>> import valkey\\n>>> r = valkey.Valkey(host='localhost', port=6379, db=0)\\n>>> r.set('foo', 'bar')\\nTrue\\n>>> r.get('foo')\\nb'bar'\\n```\\n\\nThe above code connects to localhost on port 6379, sets a value in Redis, and retrieves it. All responses are returned as bytes in Python, to receive decoded strings, set *decode_responses=True*.  For this, and more connection options, see [these examples](https://valkey-py.readthedocs.io/en/latest/examples.html).\\n\\n### Migration from redis-py\\n\\nYou are encouraged to use the new class names, but to allow for a smooth transition alias are available:\\n\\n``` python\\n>>> import valkey as redis\\n>>> r = redis.Redis(host='localhost', port=6379, db=0)\\n>>> r.set('foo', 'bar')\\nTrue\\n>>> r.get('foo')\\nb'bar'\\n```\\n\\n#### RESP3 Support\\nTo enable support for RESP3 change your connection object to include *protocol=3*\\n\\n``` python\\n>>> import valkey\\n>>> r = valkey.Valkey(host='localhost', port=6379, db=0, protocol=3)\\n```\\n\\n### Connection Pools\\n\\nBy default, valkey-py uses a connection pool to manage connections. Each instance of a Valkey class receives its own connection pool. You can however define your own [valkey.ConnectionPool](https://valkey-py.readthedocs.io/en/latest/connections.html#connection-pools).\\n\\n``` python\\n>>> pool = valkey.ConnectionPool(host='localhost', port=6379, db=0)\\n>>> r = valkey.Valkey(connection_pool=pool)\\n```\\n\\nAlternatively, you might want to look at [Async connections](https://valkey-py.readthedocs.io/en/latest/examples/asyncio_examples.html), or [Cluster connections](https://valkey-py.readthedocs.io/en/latest/connections.html#cluster-client), or even [Async Cluster connections](https://valkey-py.readthedocs.io/en/latest/connections.html#async-cluster-client).\\n\\n### Valkey Commands\\n\\nThere is built-in support for all of the [out-of-the-box Valkey commands](https://valkey.io/commands). They are exposed using the raw Redis command names (`HSET`, `HGETALL`, etc.) except where a word (i.e. del) is reserved by the language. See the [complete set of commands](https://github.com/valkey-io/valkey-py/tree/main/valkey/commands), or [the documentation](https://valkey-py.readthedocs.io/en/latest/commands.html).\\n\\n## Documentation\\n\\nCheck out the [documentation](https://valkey-py.readthedocs.io/en/latest/index.html)\\n\\n## Advanced Topics\\n\\nThe [official Valkey command documentation](https://valkey.io/commands)\\ndoes a great job of explaining each command in detail. valkey-py attempts\\nto adhere to the official command syntax. There are a few exceptions:\\n\\n-   **MULTI/EXEC**: These are implemented as part of the Pipeline class.\\n    The pipeline is wrapped with the MULTI and EXEC statements by\\n    default when it is executed, which can be disabled by specifying\\n    transaction=False. See more about Pipelines below.\\n\\n-   **SUBSCRIBE/LISTEN**: Similar to pipelines, PubSub is implemented as\\n    a separate class as it places the underlying connection in a state\\n    where it can\\\\'t execute non-pubsub commands. Calling the pubsub\\n    method from the Valkey client will return a PubSub instance where you\\n    can subscribe to channels and listen for messages. You can only call\\n    PUBLISH from the Valkey client (see [this comment on issue\\n    #151](https://github.com/redis/redis-py/issues/151#issuecomment-1545015)\\n    for details).\\n\\nFor more details, please see the documentation on [advanced topics page](https://valkey-py.readthedocs.io/en/latest/advanced_features.html).\\n\\n### Pipelines\\n\\nThe following is a basic example of a [Valkey pipeline](https://valkey.io/topics/pipelining/), a method to optimize round-trip calls, by batching Valkey commands, and receiving their results as a list.\\n\\n\\n``` python\\n>>> pipe = r.pipeline()\\n>>> pipe.set('foo', 5)\\n>>> pipe.set('bar', 18.5)\\n>>> pipe.set('blee', \\\"hello world!\\\")\\n>>> pipe.execute()\\n[True, True, True]\\n```\\n\\n### PubSub\\n\\nThe following example shows how to utilize [Valkey Pub/Sub](https://valkey.io/topics/pubsub/) to subscribe to specific channels.\\n\\n``` python\\n>>> r = valkey.Valkey(...)\\n>>> p = r.pubsub()\\n>>> p.subscribe('my-first-channel', 'my-second-channel', ...)\\n>>> p.get_message()\\n{'pattern': None, 'type': 'subscribe', 'channel': b'my-second-channel', 'data': 1}\\n```\\n\\n\\n--------------------------\\n\\n### Author\\n\\nYou can read valkey-py sources on [GitHub](https://github.com/valkey-io/valkey-py), or download it from [pypi](https://pypi.org/project/valkey/)\\nIt was created as a fork of [redis-py](https://github.com/redis/redis-py)\\n\\nSpecial thanks to:\\n\\n-   Andy McCurdy (<sedrik@gmail.com>) the original author of redis-py.\\n-   Ludovico Magnocavallo, author of the original Python Redis client,\\n    from which some of the socket code is still used.\\n-   Alexander Solovyov for ideas on the generic response callback\\n    system.\\n-   Paul Hubbard for initial packaging support in redis-py.\\n\\n[![Valkey](./docs/logo-valkey.png)](https://valkey.io/)\\n\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Python client for Valkey forked from redis-py\", \"latest_version\": \"6.1.1\", \"weekly_downloads\": 792918, \"description_cleaned\": \"valkey-py\\nThe Python interface to the Valkey key-value store.\\nInstallation\\n|\\nUsage\\n|\\nDocumentation\\n|\\nAdvanced Topics\\n|\\nContributing\\nInstallation\\nStart a valkey via docker:\\ndocker\\nrun\\n-p\\n6379\\n:6379\\n-it\\nvalkey/valkey:latest\\nTo install valkey-py, simply:\\n$\\npip\\ninstall\\nvalkey\\nFor faster performance, install valkey with libvalkey support, this provides a compiled response parser, and\\nfor most cases\\nrequires zero code changes.\\nBy default, if libvalkey >= 2.3.2 is available, valkey-py will attempt to use it for response parsing.\\n$\\npip\\ninstall\\n\\\"valkey[libvalkey]\\\"\\nUsage\\nBasic Example\\n>>>\\nimport\\nvalkey\\n>>>\\nr\\n=\\nvalkey\\n.\\nValkey\\n(\\nhost\\n=\\n'localhost'\\n,\\nport\\n=\\n6379\\n,\\ndb\\n=\\n0\\n)\\n>>>\\nr\\n.\\nset\\n(\\n'foo'\\n,\\n'bar'\\n)\\nTrue\\n>>>\\nr\\n.\\nget\\n(\\n'foo'\\n)\\nb\\n'bar'\\nThe above code connects to localhost on port 6379, sets a value in Redis, and retrieves it. All responses are returned as bytes in Python, to receive decoded strings, set\\ndecode_responses=True\\n.  For this, and more connection options, see\\nthese examples\\n.\\nMigration from redis-py\\nYou are encouraged to use the new class names, but to allow for a smooth transition alias are available:\\n>>>\\nimport\\nvalkey\\nas\\nredis\\n>>>\\nr\\n=\\nredis\\n.\\nRedis\\n(\\nhost\\n=\\n'localhost'\\n,\\nport\\n=\\n6379\\n,\\ndb\\n=\\n0\\n)\\n>>>\\nr\\n.\\nset\\n(\\n'foo'\\n,\\n'bar'\\n)\\nTrue\\n>>>\\nr\\n.\\nget\\n(\\n'foo'\\n)\\nb\\n'bar'\\nRESP3 Support\\nTo enable support for RESP3 change your connection object to include\\nprotocol=3\\n>>>\\nimport\\nvalkey\\n>>>\\nr\\n=\\nvalkey\\n.\\nValkey\\n(\\nhost\\n=\\n'localhost'\\n,\\nport\\n=\\n6379\\n,\\ndb\\n=\\n0\\n,\\nprotocol\\n=\\n3\\n)\\nConnection Pools\\nBy default, valkey-py uses a connection pool to manage connections. Each instance of a Valkey class receives its own connection pool. You can however define your own\\nvalkey.ConnectionPool\\n.\\n>>>\\npool\\n=\\nvalkey\\n.\\nConnectionPool\\n(\\nhost\\n=\\n'localhost'\\n,\\nport\\n=\\n6379\\n,\\ndb\\n=\\n0\\n)\\n>>>\\nr\\n=\\nvalkey\\n.\\nValkey\\n(\\nconnection_pool\\n=\\npool\\n)\\nAlternatively, you might want to look at\\nAsync connections\\n, or\\nCluster connections\\n, or even\\nAsync Cluster connections\\n.\\nValkey Commands\\nThere is built-in support for all of the\\nout-of-the-box Valkey commands\\n. They are exposed using the raw Redis command names (\\nHSET\\n,\\nHGETALL\\n, etc.) except where a word (i.e. del) is reserved by the language. See the\\ncomplete set of commands\\n, or\\nthe documentation\\n.\\nDocumentation\\nCheck out the\\ndocumentation\\nAdvanced Topics\\nThe\\nofficial Valkey command documentation\\ndoes a great job of explaining each command in detail. valkey-py attempts\\nto adhere to the official command syntax. There are a few exceptions:\\nMULTI/EXEC\\n: These are implemented as part of the Pipeline class.\\nThe pipeline is wrapped with the MULTI and EXEC statements by\\ndefault when it is executed, which can be disabled by specifying\\ntransaction=False. See more about Pipelines below.\\nSUBSCRIBE/LISTEN\\n: Similar to pipelines, PubSub is implemented as\\na separate class as it places the underlying connection in a state\\nwhere it can't execute non-pubsub commands. Calling the pubsub\\nmethod from the Valkey client will return a PubSub instance where you\\ncan subscribe to channels and listen for messages. You can only call\\nPUBLISH from the Valkey client (see\\nthis comment on issue\\n#151\\nfor details).\\nFor more details, please see the documentation on\\nadvanced topics page\\n.\\nPipelines\\nThe following is a basic example of a\\nValkey pipeline\\n, a method to optimize round-trip calls, by batching Valkey commands, and receiving their results as a list.\\n>>>\\npipe\\n=\\nr\\n.\\npipeline\\n()\\n>>>\\npipe\\n.\\nset\\n(\\n'foo'\\n,\\n5\\n)\\n>>>\\npipe\\n.\\nset\\n(\\n'bar'\\n,\\n18.5\\n)\\n>>>\\npipe\\n.\\nset\\n(\\n'blee'\\n,\\n\\\"hello world!\\\"\\n)\\n>>>\\npipe\\n.\\nexecute\\n()\\n[\\nTrue\\n,\\nTrue\\n,\\nTrue\\n]\\nPubSub\\nThe following example shows how to utilize\\nValkey Pub/Sub\\nto subscribe to specific channels.\\n>>>\\nr\\n=\\nvalkey\\n.\\nValkey\\n(\\n...\\n)\\n>>>\\np\\n=\\nr\\n.\\npubsub\\n()\\n>>>\\np\\n.\\nsubscribe\\n(\\n'my-first-channel'\\n,\\n'my-second-channel'\\n,\\n...\\n)\\n>>>\\np\\n.\\nget_message\\n()\\n{\\n'pattern'\\n:\\nNone\\n,\\n'type'\\n:\\n'subscribe'\\n,\\n'channel'\\n:\\nb\\n'my-second-channel'\\n,\\n'data'\\n:\\n1\\n}\\nAuthor\\nYou can read valkey-py sources on\\nGitHub\\n, or download it from\\npypi\\nIt was created as a fork of\\nredis-py\\nSpecial thanks to:\\nAndy McCurdy (\\nsedrik@gmail.com\\n) the original author of redis-py.\\nLudovico Magnocavallo, author of the original Python Redis client,\\nfrom which some of the socket code is still used.\\nAlexander Solovyov for ideas on the generic response callback\\nsystem.\\nPaul Hubbard for initial packaging support in redis-py.\"}, {\"name\": \"pystache\", \"description\": \"Pystache\\n========\\n\\n|ci| |conda| |coverage| |bandit| |release|\\n\\n|pre| |cov| |pylint|\\n\\n|tag| |license| |python|\\n\\n\\nThis updated fork of Pystache is currently tested on Python 3.8+ and in\\nConda, on Linux, Macos, and Windows.\\n\\n|logo|\\n\\n`Pystache <https://github.com/PennyDreadfulMTG/pystache>`__ is a Python\\nimplementation of `Mustache <https://github.com/mustache/mustache/>`__.\\nMustache is a framework-agnostic, logic-free templating system inspired\\nby `ctemplate <https://code.google.com/p/google-ctemplate/>`__ and\\net. Like ctemplate, Mustache \\\"emphasizes separating logic from presentation:\\nit is impossible to embed application logic in this template language.\\\"\\n\\nThe `mustache(5) <https://mustache.github.io/mustache.5.html>`__ man\\npage provides a good introduction to Mustache's syntax. For a more\\ncomplete (and more current) description of Mustache's behavior, see the\\nofficial `Mustache spec <https://github.com/mustache/spec>`__.\\n\\nPystache is `semantically versioned <https://semver.org>`__ and older\\nversions can still be found on `PyPI <https://pypi.python.org/pypi/pystache>`__.\\nThis version of Pystache now passes all tests in `version 1.1.3\\n<https://github.com/mustache/spec/tree/v1.1.3>`__ of the spec.\\n\\n\\nRequirements\\n============\\n\\nPystache is tested with:\\n\\n-  Python 3.8\\n-  Python 3.9\\n-  Python 3.10\\n-  Python 3.11\\n-  Python 3.12\\n-  Python 3.13\\n-  Conda (py38 and py310)\\n\\nJSON support is needed only for the command-line interface and to run\\nthe spec tests; PyYAML can still be used (see the Develop section).\\n\\nOfficial support for Python 2 has ended with Pystache version 0.6.0.\\n\\n\\n.. note:: This project uses setuptools_scm_ to generate and maintain the\\n          version file, which only gets included in the sdist/wheel\\n          packages. In a fresh clone, running any of the tox_ commands\\n          should generate the current version file.\\n\\n.. _setuptools_scm: https://github.com/pypa/setuptools_scm\\n.. _tox: https://github.com/tox-dev/tox\\n\\n\\nQuick Start\\n===========\\n\\nBe sure to get the latest release from either Pypi or Github.\\n\\nInstall It\\n----------\\n\\nFrom Pypi::\\n\\n  $ pip install pystache\\n\\nOr Github::\\n\\n  $ pip install -U pystache -f https://github.com/PennyDreadfulMTG/pystache/releases/\\n\\n\\nAnd test it::\\n\\n  $ pystache-test\\n\\nTo install and test from source (e.g. from GitHub), see the Develop\\nsection.\\n\\nUse It\\n------\\n\\nOpen a python console::\\n\\n  >>> import pystache\\n  >>> print(pystache.render('Hi {{person}}!', {'person': 'Mom'}))\\n  Hi Mom!\\n\\nYou can also create dedicated view classes to hold your view logic.\\n\\nHere's your view class (in ../pystache/tests/examples/readme.py):\\n\\n::\\n\\n  class SayHello(object):\\n      def to(self):\\n          return \\\"Pizza\\\"\\n\\nInstantiating like so:\\n\\n::\\n\\n  >>> from pystache.tests.examples.readme import SayHello\\n  >>> hello = SayHello()\\n\\nThen your template, say_hello.mustache (by default in the same directory\\nas your class definition):\\n\\n::\\n\\n  Hello, {{to}}!\\n\\nPull it together:\\n\\n::\\n\\n  >>> renderer = pystache.Renderer()\\n  >>> print(renderer.render(hello))\\n  Hello, Pizza!\\n\\nFor greater control over rendering (e.g. to specify a custom template\\ndirectory), use the ``Renderer`` class like above. One can pass\\nattributes to the Renderer class constructor or set them on a Renderer\\ninstance. To customize template loading on a per-view basis, subclass\\n``TemplateSpec``. See the docstrings of the\\n`Renderer <https://github.com/PennyDreadfulMTG/pystache/blob/master/pystache/renderer.py>`__\\nclass and\\n`TemplateSpec <https://github.com/PennyDreadfulMTG/pystache/blob/master/pystache/template_spec.py>`__\\nclass for more information.\\n\\nYou can also pre-parse a template:\\n\\n::\\n\\n  >>> parsed = pystache.parse(u\\\"Hey {{#who}}{{.}}!{{/who}}\\\")\\n  >>> print(parsed)\\n  ['Hey ', _SectionNode(key='who', index_begin=12, index_end=18, parsed=[_EscapeNode(key='.'), '!'])]\\n\\nAnd then:\\n\\n::\\n\\n  >>> print(renderer.render(parsed, {'who': 'Pops'}))\\n  Hey Pops!\\n  >>> print(renderer.render(parsed, {'who': 'you'}))\\n  Hey you!\\n\\n\\nUnicode\\n-------\\n\\nThis section describes how Pystache handles unicode, strings, and\\nencodings.\\n\\nInternally, Pystache uses `only unicode strings`_ (``str`` in Python 3).\\nFor input, Pystache accepts byte strings (``bytes`` in Python 3).\\nFor output, Pystache's template rendering methods return only unicode.\\n\\n.. _only unicode strings: https://docs.python.org/howto/unicode.html#tips-for-writing-unicode-aware-programs\\n\\nPystache's ``Renderer`` class supports a number of attributes to control\\nhow Pystache converts byte strings to unicode on input. These include\\nthe ``file_encoding``, ``string_encoding``, and ``decode_errors`` attributes.\\n\\nThe ``file_encoding`` attribute is the encoding the renderer uses to\\nconvert to unicode any files read from the file system. Similarly,\\n``string_encoding`` is the encoding the renderer uses to convert any other\\nbyte strings encountered during the rendering process into unicode (e.g.\\ncontext values that are encoded byte strings).\\n\\nThe ``decode_errors`` attribute is what the renderer passes as the\\n``errors`` argument to Python's built-in unicode-decoding function\\n(``str()`` in Python 3). The valid values for this argument are\\n``strict``, ``ignore``, and ``replace``.\\n\\nEach of these attributes can be set via the ``Renderer`` class's\\nconstructor using a keyword argument of the same name. See the Renderer\\nclass's docstrings for further details. In addition, the ``file_encoding``\\nattribute can be controlled on a per-view basis by subclassing the\\n``TemplateSpec`` class. When not specified explicitly, these attributes\\ndefault to values set in Pystache's ``defaults`` module.\\n\\n\\nDevelop\\n=======\\n\\nTo test from a source distribution (without installing)::\\n\\n  $ python test_pystache.py\\n\\nTo test Pystache with multiple versions of Python (with a single\\ncommand!) and different platforms, you can use [tox](https://pypi.python.org/pypi/tox)::\\n\\n  $ pip install tox\\n  $ tox -e py\\n\\nTo run tests on multiple versions with coverage, run::\\n\\n  $ tox -e py38-linux,py39-linux  # for example\\n\\n(substitute your platform above, eg, macos or windows)\\n\\nThe source distribution tests also include doctests and tests from the\\nMustache spec. To include tests from the Mustache spec in your test\\nruns::\\n\\n  $ git submodule update --init\\n\\nThe test harness parses the spec's (more human-readable) yaml files if\\n`PyYAML <http://pypi.python.org/pypi/PyYAML>`__ is present. Otherwise,\\nit parses the json files. To install PyYAML::\\n\\n  $ pip install pyyaml  # note this is installed automatically by tox\\n\\nOnce the submodule is available, you can run the full test set with::\\n\\n  $ tox -e setup -- ext/spec/specs\\n\\n\\nMaking Changes & Contributing\\n-----------------------------\\n\\nWe use the gitchangelog_ action to generate our github Release page, as\\nwell as the gitchangelog message format to help it categorize/filter\\ncommits for a tidier release page. Please use the appropriate ACTION\\nmodifiers in any Pull Requests.\\n\\nThis repo is also pre-commit_ enabled for various linting and format\\nchecks.  The checks run automatically on commit and will fail the\\ncommit (if not clean) with some checks performing simple file corrections.\\n\\nIf other checks fail on commit, the failure display should explain the error\\ntypes and line numbers. Note you must fix any fatal errors for the\\ncommit to succeed; some errors should be fixed automatically (use\\n``git status`` and ``git diff`` to review any changes).\\n\\nNote ``pylint`` is the primary check that requires your own input, as well\\nas a decision as to the appropriate fix action.  You must fix any ``pylint``\\nwarnings (relative to the baseline config score) for the commit to succeed.\\n\\nSee the following pages for more information on gitchangelog and pre-commit.\\n\\n.. inclusion-marker-1\\n\\n* generate-changelog_\\n* pre-commit-config_\\n* pre-commit-usage_\\n\\n.. _generate-changelog:  docs/source/dev/generate-changelog.rst\\n.. _pre-commit-config: docs/source/dev/pre-commit-config.rst\\n.. _pre-commit-usage: docs/source/dev/pre-commit-usage.rst\\n.. inclusion-marker-2\\n\\nYou will need to install pre-commit before contributing any changes;\\ninstalling it using your system's package manager is recommended,\\notherwise install with pip into your usual virtual environment using\\nsomething like::\\n\\n  $ sudo emerge pre-commit  --or--\\n  $ pip install pre-commit\\n\\nthen install it into the repo you just cloned::\\n\\n  $ git clone https://github.com/PennyDreadfulMTG/pystache\\n  $ cd pystache/\\n  $ pre-commit install\\n\\nIt's usually a good idea to update the hooks to the latest version::\\n\\n    pre-commit autoupdate\\n\\n.. _gitchangelog: https://github.com/sarnold/gitchangelog-action\\n.. _pre-commit: https://pre-commit.com/\\n\\n\\nCredits\\n=======\\n\\n  >>> import pystache\\n  >>> context = { 'author': 'Chris Wanstrath', 'maintainer': 'Chris Jerdonek','refurbisher': 'Steve Arnold', 'new_maintainer': 'Thomas David Baker' }\\n  >>> print(pystache.render(\\\"Author: {{author}}\\\\nMaintainer: {{maintainer}}\\\\nRefurbisher: {{refurbisher}}\\\\nNew maintainer: {{new_maintainer}}\\\", context))\\n  Author: Chris Wanstrath\\n  Maintainer: Chris Jerdonek\\n  Refurbisher: Steve Arnold\\n  New maintainer: Thomas David Baker\\n\\n\\nPystache logo by `David Phillips <http://davidphillips.us/>`__ is\\nlicensed under a `Creative Commons Attribution-ShareAlike 3.0 Unported\\nLicense <https://creativecommons.org/licenses/by-sa/3.0/deed.en_US>`__.\\n\\n|ccbysa|\\n\\n\\n.. |ci| image:: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/ci.yml/badge.svg\\n    :target: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/ci.yml\\n    :alt: CI Status\\n\\n.. |conda| image:: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/conda.yml/badge.svg\\n    :target: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/conda.yml\\n    :alt: Conda Status\\n\\n.. |coverage| image:: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/coverage.yml/badge.svg\\n    :target: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/coverage.yml\\n    :alt: Coverage workflow\\n\\n.. |bandit| image:: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/bandit.yml/badge.svg\\n    :target: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/bandit.yml\\n    :alt: Security check - Bandit\\n\\n.. |release| image:: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/release.yml/badge.svg\\n    :target: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/release.yml\\n    :alt: Release Status\\n\\n.. |cov| image:: https://raw.githubusercontent.com/PennyDreadfulMTG/pystache/badges/master/test-coverage.svg\\n    :target: https://github.com/PennyDreadfulMTG/pystache/\\n    :alt: Test coverage\\n\\n.. |pylint| image:: https://raw.githubusercontent.com/PennyDreadfulMTG/pystache/badges/master/pylint-score.svg\\n    :target: https://github.com/PennyDreadfulMTG/pystache/actions/workflows/pylint.yml\\n    :alt: Pylint Score\\n\\n.. |license| image:: https://img.shields.io/github/license/PennyDreadfulMTG/pystache\\n    :target: https://github.com/PennyDreadfulMTG/pystache/blob/master/LICENSE\\n    :alt: License\\n\\n.. |tag| image:: https://img.shields.io/github/v/tag/PennyDreadfulMTG/pystache?color=green&include_prereleases&label=latest%20release\\n    :target: https://github.com/PennyDreadfulMTG/pystache/releases\\n    :alt: GitHub tag\\n\\n.. |python| image:: https://img.shields.io/badge/python-3.6+-blue.svg\\n    :target: https://www.python.org/downloads/\\n    :alt: Python\\n\\n.. |pre| image:: https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white\\n   :target: https://github.com/pre-commit/pre-commit\\n   :alt: pre-commit\\n\\n.. |logo| image:: gh/images/logo_phillips_small.png\\n\\n.. |ccbysa| image:: https://i.creativecommons.org/l/by-sa/3.0/88x31.png\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"Mustache for Python\", \"latest_version\": \"0.6.8\", \"weekly_downloads\": 791904, \"description_cleaned\": \"Pystache\\nThis updated fork of Pystache is currently tested on Python 3.8+ and in\\nConda, on Linux, Macos, and Windows.\\nPystache\\nis a Python\\nimplementation of\\nMustache\\n.\\nMustache is a framework-agnostic, logic-free templating system inspired\\nby\\nctemplate\\nand\\net. Like ctemplate, Mustache \\u201cemphasizes separating logic from presentation:\\nit is impossible to embed application logic in this template language.\\u201d\\nThe\\nmustache(5)\\nman\\npage provides a good introduction to Mustache\\u2019s syntax. For a more\\ncomplete (and more current) description of Mustache\\u2019s behavior, see the\\nofficial\\nMustache spec\\n.\\nPystache is\\nsemantically versioned\\nand older\\nversions can still be found on\\nPyPI\\n.\\nThis version of Pystache now passes all tests in\\nversion 1.1.3\\nof the spec.\\nRequirements\\nPystache is tested with:\\nPython 3.8\\nPython 3.9\\nPython 3.10\\nPython 3.11\\nPython 3.12\\nPython 3.13\\nConda (py38 and py310)\\nJSON support is needed only for the command-line interface and to run\\nthe spec tests; PyYAML can still be used (see the Develop section).\\nOfficial support for Python 2 has ended with Pystache version 0.6.0.\\nNote\\nThis project uses\\nsetuptools_scm\\nto generate and maintain the\\nversion file, which only gets included in the sdist/wheel\\npackages. In a fresh clone, running any of the\\ntox\\ncommands\\nshould generate the current version file.\\nQuick Start\\nBe sure to get the latest release from either Pypi or Github.\\nInstall It\\nFrom Pypi:\\n$ pip install pystache\\nOr Github:\\n$ pip install -U pystache -f https://github.com/PennyDreadfulMTG/pystache/releases/\\nAnd test it:\\n$ pystache-test\\nTo install and test from source (e.g. from GitHub), see the Develop\\nsection.\\nUse It\\nOpen a python console:\\n>>> import pystache\\n>>> print(pystache.render('Hi {{person}}!', {'person': 'Mom'}))\\nHi Mom!\\nYou can also create dedicated view classes to hold your view logic.\\nHere\\u2019s your view class (in ../pystache/tests/examples/readme.py):\\nclass SayHello(object):\\n    def to(self):\\n        return \\\"Pizza\\\"\\nInstantiating like so:\\n>>> from pystache.tests.examples.readme import SayHello\\n>>> hello = SayHello()\\nThen your template, say_hello.mustache (by default in the same directory\\nas your class definition):\\nHello, {{to}}!\\nPull it together:\\n>>> renderer = pystache.Renderer()\\n>>> print(renderer.render(hello))\\nHello, Pizza!\\nFor greater control over rendering (e.g. to specify a custom template\\ndirectory), use the\\nRenderer\\nclass like above. One can pass\\nattributes to the Renderer class constructor or set them on a Renderer\\ninstance. To customize template loading on a per-view basis, subclass\\nTemplateSpec\\n. See the docstrings of the\\nRenderer\\nclass and\\nTemplateSpec\\nclass for more information.\\nYou can also pre-parse a template:\\n>>> parsed = pystache.parse(u\\\"Hey {{#who}}{{.}}!{{/who}}\\\")\\n>>> print(parsed)\\n['Hey ', _SectionNode(key='who', index_begin=12, index_end=18, parsed=[_EscapeNode(key='.'), '!'])]\\nAnd then:\\n>>> print(renderer.render(parsed, {'who': 'Pops'}))\\nHey Pops!\\n>>> print(renderer.render(parsed, {'who': 'you'}))\\nHey you!\\nUnicode\\nThis section describes how Pystache handles unicode, strings, and\\nencodings.\\nInternally, Pystache uses\\nonly unicode strings\\n(\\nstr\\nin Python 3).\\nFor input, Pystache accepts byte strings (\\nbytes\\nin Python 3).\\nFor output, Pystache\\u2019s template rendering methods return only unicode.\\nPystache\\u2019s\\nRenderer\\nclass supports a number of attributes to control\\nhow Pystache converts byte strings to unicode on input. These include\\nthe\\nfile_encoding\\n,\\nstring_encoding\\n, and\\ndecode_errors\\nattributes.\\nThe\\nfile_encoding\\nattribute is the encoding the renderer uses to\\nconvert to unicode any files read from the file system. Similarly,\\nstring_encoding\\nis the encoding the renderer uses to convert any other\\nbyte strings encountered during the rendering process into unicode (e.g.\\ncontext values that are encoded byte strings).\\nThe\\ndecode_errors\\nattribute is what the renderer passes as the\\nerrors\\nargument to Python\\u2019s built-in unicode-decoding function\\n(\\nstr()\\nin Python 3). The valid values for this argument are\\nstrict\\n,\\nignore\\n, and\\nreplace\\n.\\nEach of these attributes can be set via the\\nRenderer\\nclass\\u2019s\\nconstructor using a keyword argument of the same name. See the Renderer\\nclass\\u2019s docstrings for further details. In addition, the\\nfile_encoding\\nattribute can be controlled on a per-view basis by subclassing the\\nTemplateSpec\\nclass. When not specified explicitly, these attributes\\ndefault to values set in Pystache\\u2019s\\ndefaults\\nmodule.\\nDevelop\\nTo test from a source distribution (without installing):\\n$ python test_pystache.py\\nTo test Pystache with multiple versions of Python (with a single\\ncommand!) and different platforms, you can use [tox](\\nhttps://pypi.python.org/pypi/tox\\n):\\n$ pip install tox\\n$ tox -e py\\nTo run tests on multiple versions with coverage, run:\\n$ tox -e py38-linux,py39-linux  # for example\\n(substitute your platform above, eg, macos or windows)\\nThe source distribution tests also include doctests and tests from the\\nMustache spec. To include tests from the Mustache spec in your test\\nruns:\\n$ git submodule update --init\\nThe test harness parses the spec\\u2019s (more human-readable) yaml files if\\nPyYAML\\nis present. Otherwise,\\nit parses the json files. To install PyYAML:\\n$ pip install pyyaml  # note this is installed automatically by tox\\nOnce the submodule is available, you can run the full test set with:\\n$ tox -e setup -- ext/spec/specs\\nMaking Changes & Contributing\\nWe use the\\ngitchangelog\\naction to generate our github Release page, as\\nwell as the gitchangelog message format to help it categorize/filter\\ncommits for a tidier release page. Please use the appropriate ACTION\\nmodifiers in any Pull Requests.\\nThis repo is also\\npre-commit\\nenabled for various linting and format\\nchecks.  The checks run automatically on commit and will fail the\\ncommit (if not clean) with some checks performing simple file corrections.\\nIf other checks fail on commit, the failure display should explain the error\\ntypes and line numbers. Note you must fix any fatal errors for the\\ncommit to succeed; some errors should be fixed automatically (use\\ngit status\\nand\\ngit diff\\nto review any changes).\\nNote\\npylint\\nis the primary check that requires your own input, as well\\nas a decision as to the appropriate fix action.  You must fix any\\npylint\\nwarnings (relative to the baseline config score) for the commit to succeed.\\nSee the following pages for more information on gitchangelog and pre-commit.\\ngenerate-changelog\\npre-commit-config\\npre-commit-usage\\nYou will need to install pre-commit before contributing any changes;\\ninstalling it using your system\\u2019s package manager is recommended,\\notherwise install with pip into your usual virtual environment using\\nsomething like:\\n$ sudo emerge pre-commit  --or--\\n$ pip install pre-commit\\nthen install it into the repo you just cloned:\\n$ git clone https://github.com/PennyDreadfulMTG/pystache\\n$ cd pystache/\\n$ pre-commit install\\nIt\\u2019s usually a good idea to update the hooks to the latest version:\\npre-commit autoupdate\\nCredits\\n>>> import pystache\\n>>> context = { 'author': 'Chris Wanstrath', 'maintainer': 'Chris Jerdonek','refurbisher': 'Steve Arnold', 'new_maintainer': 'Thomas David Baker' }\\n>>> print(pystache.render(\\\"Author: {{author}}\\\\nMaintainer: {{maintainer}}\\\\nRefurbisher: {{refurbisher}}\\\\nNew maintainer: {{new_maintainer}}\\\", context))\\nAuthor: Chris Wanstrath\\nMaintainer: Chris Jerdonek\\nRefurbisher: Steve Arnold\\nNew maintainer: Thomas David Baker\\nPystache logo by\\nDavid Phillips\\nis\\nlicensed under a\\nCreative Commons Attribution-ShareAlike 3.0 Unported\\nLicense\\n.\"}, {\"name\": \"striprtf\", \"description\": \"# striprtf\\n![Build status](https://github.com/joshy/striprtf/workflows/striprtf%20build/badge.svg)\\n\\n## Purpose\\nThis is a library to convert Rich Text Format (RTF) files to plain text files. A lot of medical documents are written in RTF format which is not ideal for parsing and further processing. This library converts it to plain old text.\\n\\n## How to use it\\n```python\\nfrom striprtf.striprtf import rtf_to_text\\nrtf = \\\"some rtf encoded string\\\"\\ntext = rtf_to_text(rtf)\\nprint(text)\\n```\\n\\nIf you want to use a different encoding than `cp1252` you can pass it via the `encoding`\\nparameter. This is only taken into account if no explicit codepage has been set. \\n```python\\nfrom striprtf.striprtf import rtf_to_text\\nrtf = \\\"some rtf encoded string in latin1\\\"\\ntext = rtf_to_text(rtf, encoding=\\\"latin-1\\\")\\nprint(text)\\n```\\n\\nSometimes UnicodeDecodingErrors can happen because of various reasons.\\nIn this case you can try to relax the encoding process like this:\\n```python\\nfrom striprtf.striprtf import rtf_to_text\\nrtf = \\\"some rtf encoded string\\\"\\ntext = rtf_to_text(rtf, errors=\\\"ignore\\\")\\nprint(text)\\n```\\n\\n## Online version\\nIf you don't want to install or just try it out there is an [online version](https://striprtf.dev) available. \\n\\n## PostgreSQL \\nThere is also a [PostgreSQL version](https://github.com/MnhnL/pg_striprtf) available from [Raffael Mancini](https://github.com/raffael-mnhn).\\n\\n## History\\n[Pyth](https://github.com/brendonh/pyth) was not working for the rtf files I\\nhad. The next best thing was this gist:\\nhttps://gist.github.com/gilsondev/7c1d2d753ddb522e7bc22511cfb08676\\n\\n~~Very few additions where made, e.g. better formatting of tables. ~~\\n\\nIn the meantime some encodings bugs have been fixed. :-)\\n\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"A simple library to convert rtf to text\", \"latest_version\": \"0.0.29\", \"weekly_downloads\": 791354, \"description_cleaned\": \"striprtf\\nPurpose\\nThis is a library to convert Rich Text Format (RTF) files to plain text files. A lot of medical documents are written in RTF format which is not ideal for parsing and further processing. This library converts it to plain old text.\\nHow to use it\\nfrom\\nstriprtf.striprtf\\nimport\\nrtf_to_text\\nrtf\\n=\\n\\\"some rtf encoded string\\\"\\ntext\\n=\\nrtf_to_text\\n(\\nrtf\\n)\\nprint\\n(\\ntext\\n)\\nIf you want to use a different encoding than\\ncp1252\\nyou can pass it via the\\nencoding\\nparameter. This is only taken into account if no explicit codepage has been set.\\nfrom\\nstriprtf.striprtf\\nimport\\nrtf_to_text\\nrtf\\n=\\n\\\"some rtf encoded string in latin1\\\"\\ntext\\n=\\nrtf_to_text\\n(\\nrtf\\n,\\nencoding\\n=\\n\\\"latin-1\\\"\\n)\\nprint\\n(\\ntext\\n)\\nSometimes UnicodeDecodingErrors can happen because of various reasons.\\nIn this case you can try to relax the encoding process like this:\\nfrom\\nstriprtf.striprtf\\nimport\\nrtf_to_text\\nrtf\\n=\\n\\\"some rtf encoded string\\\"\\ntext\\n=\\nrtf_to_text\\n(\\nrtf\\n,\\nerrors\\n=\\n\\\"ignore\\\"\\n)\\nprint\\n(\\ntext\\n)\\nOnline version\\nIf you don't want to install or just try it out there is an\\nonline version\\navailable.\\nPostgreSQL\\nThere is also a\\nPostgreSQL version\\navailable from\\nRaffael Mancini\\n.\\nHistory\\nPyth\\nwas not working for the rtf files I\\nhad. The next best thing was this gist:\\nhttps://gist.github.com/gilsondev/7c1d2d753ddb522e7bc22511cfb08676\\n~~Very few additions where made, e.g. better formatting of tables. ~~\\nIn the meantime some encodings bugs have been fixed. :-)\"}, {\"name\": \"INITools\", \"description\": \"A set of tools for parsing and using ``.ini``-style files, including\\nan abstract parser and several tools built on that parser.\\n\\nRepository available at `http://bitbucket.org/ianb/initools\\n<http://bitbucket.org/ianb/initools>`_, or `download a tarball\\nof the development version \\n<http://bitbucket.org/ianb/initools/get/tip.gz#egg=INITools-dev>`_\\nusing ``easy_install INITools==dev``\", \"description_content_type\": null, \"summary\": \"Tools for parsing and using INI-style files\", \"latest_version\": \"0.3.1\", \"weekly_downloads\": 789852, \"description_cleaned\": \"A set of tools for parsing and using\\n.ini\\n-style files, including\\nan abstract parser and several tools built on that parser.\\nRepository available at\\nhttp://bitbucket.org/ianb/initools\\n, or\\ndownload a tarball\\nof the development version\\nusing\\neasy_install\\nINITools==dev\"}, {\"name\": \"tweepy\", \"description\": \"Tweepy: Twitter for Python!\\n======\\n\\n[![PyPI Version](https://img.shields.io/pypi/v/tweepy?label=PyPI)](https://pypi.org/project/tweepy/)\\n[![Python Versions](https://img.shields.io/pypi/pyversions/tweepy?label=Python)](https://pypi.org/project/tweepy/)\\n[![DOI](https://zenodo.org/badge/244025.svg)](https://zenodo.org/badge/latestdoi/244025)\\n\\n[![Documentation Status](https://readthedocs.org/projects/tweepy/badge/?version=latest)](https://tweepy.readthedocs.io/en/latest/)\\n[![Test Status](https://github.com/tweepy/tweepy/workflows/Test/badge.svg)](https://github.com/tweepy/tweepy/actions?query=workflow%3ATest)\\n[![Coverage Status](https://img.shields.io/coveralls/tweepy/tweepy/master.svg?style=flat)](https://coveralls.io/github/tweepy/tweepy?branch=master)\\n\\n[![Discord Server](https://discord.com/api/guilds/432685901596852224/embed.png)](https://discord.gg/bJvqnhg)\\n\\nInstallation\\n------------\\n\\nThe easiest way to install the latest version from PyPI is by using\\n[pip](https://pip.pypa.io/):\\n\\n    pip install tweepy\\n\\nTo use the `tweepy.asynchronous` subpackage, be sure to install with the\\n`async` extra:\\n\\n    pip install tweepy[async]\\n\\nYou can also use Git to clone the repository from GitHub to install the latest\\ndevelopment version:\\n\\n    git clone https://github.com/tweepy/tweepy.git\\n    cd tweepy\\n    pip install .\\n\\nAlternatively, install directly from the GitHub repository:\\n\\n    pip install git+https://github.com/tweepy/tweepy.git\\n\\nLatest version of Python and older versions not end of life (bugfix and security) are supported.\\n\\nLinks\\n-----\\n\\n- [Documentation](https://tweepy.readthedocs.io/en/latest/)\\n- [Official Discord Server](https://discord.gg/bJvqnhg)\\n- [Twitter API Documentation](https://developer.twitter.com/en/docs/twitter-api)\\n\\n\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Library for accessing the X API (Twitter)\", \"latest_version\": \"4.16.0\", \"weekly_downloads\": 788234, \"description_cleaned\": \"Tweepy: Twitter for Python!\\nInstallation\\nThe easiest way to install the latest version from PyPI is by using\\npip\\n:\\npip install tweepy\\nTo use the\\ntweepy.asynchronous\\nsubpackage, be sure to install with the\\nasync\\nextra:\\npip install tweepy[async]\\nYou can also use Git to clone the repository from GitHub to install the latest\\ndevelopment version:\\ngit clone https://github.com/tweepy/tweepy.git\\ncd tweepy\\npip install .\\nAlternatively, install directly from the GitHub repository:\\npip install git+https://github.com/tweepy/tweepy.git\\nLatest version of Python and older versions not end of life (bugfix and security) are supported.\\nLinks\\nDocumentation\\nOfficial Discord Server\\nTwitter API Documentation\"}, {\"name\": \"llama-index-readers-file\", \"description\": \"# LlamaIndex Readers Integration: File\\n\\n```bash\\npip install llama-index-readers-file\\n```\\n\\nThis is the default integration for different loaders that are used within `SimpleDirectoryReader`.\\n\\nProvides support for the following loaders:\\n\\n- DocxReader\\n- HWPReader\\n- PDFReader\\n- EpubReader\\n- FlatReader\\n- HTMLTagReader\\n- ImageCaptionReader\\n- ImageReader\\n- ImageVisionLLMReader\\n- IPYNBReader\\n- MarkdownReader\\n- MboxReader\\n- PptxReader\\n- PandasCSVReader\\n- VideoAudioReader\\n- UnstructuredReader\\n- PyMuPDFReader\\n- ImageTabularChartReader\\n- XMLReader\\n- PagedCSVReader\\n- CSVReader\\n- RTFReader\\n\\n## Installation\\n\\n```bash\\npip install llama-index-readers-file\\n```\\n\\n## Usage\\n\\nOnce installed, You can import any of the loader. Here's an example usage of one of the loader.\\n\\n```python\\nfrom llama_index.core import SimpleDirectoryReader\\nfrom llama_index.readers.file import (\\n    DocxReader,\\n    HWPReader,\\n    PDFReader,\\n    EpubReader,\\n    FlatReader,\\n    HTMLTagReader,\\n    ImageCaptionReader,\\n    ImageReader,\\n    ImageVisionLLMReader,\\n    IPYNBReader,\\n    MarkdownReader,\\n    MboxReader,\\n    PptxReader,\\n    PandasCSVReader,\\n    VideoAudioReader,\\n    UnstructuredReader,\\n    PyMuPDFReader,\\n    ImageTabularChartReader,\\n    XMLReader,\\n    PagedCSVReader,\\n    CSVReader,\\n    RTFReader,\\n)\\n\\n# PDF Reader with `SimpleDirectoryReader`\\nparser = PDFReader()\\nfile_extractor = {\\\".pdf\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Docx Reader example\\nparser = DocxReader()\\nfile_extractor = {\\\".docx\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# HWP Reader example\\nparser = HWPReader()\\nfile_extractor = {\\\".hwp\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Epub Reader example\\nparser = EpubReader()\\nfile_extractor = {\\\".epub\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Flat Reader example\\nparser = FlatReader()\\nfile_extractor = {\\\".txt\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# HTML Tag Reader example\\nparser = HTMLTagReader()\\nfile_extractor = {\\\".html\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Image Reader example\\nparser = ImageReader()\\nfile_extractor = {\\n    \\\".jpg\\\": parser,\\n    \\\".jpeg\\\": parser,\\n    \\\".png\\\": parser,\\n}  # Add other image formats as needed\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# IPYNB Reader example\\nparser = IPYNBReader()\\nfile_extractor = {\\\".ipynb\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Markdown Reader example\\nparser = MarkdownReader()\\nfile_extractor = {\\\".md\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Mbox Reader example\\nparser = MboxReader()\\nfile_extractor = {\\\".mbox\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Pptx Reader example\\n# Basic usage - extracts text, tables, charts, and speaker notes\\nparser = PptxReader()\\n\\n# Advanced usage - control parsing behavior\\nparser = PptxReader(\\n    extract_images=True,  # Enable image captioning\\n    context_consolidation_with_llm=True,  # Use LLM for content synthesis\\n    num_workers=4,  # Parallel processing\\n    batch_size=10,  # Slides processed per worker batch\\n    raise_on_error=True,  # Raise value error if file_parsing is not successful\\n)\\n\\nfile_extractor = {\\\".pptx\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n\\n# Pandas CSV Reader example\\nparser = PandasCSVReader()\\nfile_extractor = {\\\".csv\\\": parser}  # Add other CSV formats as needed\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# PyMuPDF Reader example\\nparser = PyMuPDFReader()\\nfile_extractor = {\\\".pdf\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# XML Reader example\\nparser = XMLReader()\\nfile_extractor = {\\\".xml\\\": parser}\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# Paged CSV Reader example\\nparser = PagedCSVReader()\\nfile_extractor = {\\\".csv\\\": parser}  # Add other CSV formats as needed\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n\\n# CSV Reader example\\nparser = CSVReader()\\nfile_extractor = {\\\".csv\\\": parser}  # Add other CSV formats as needed\\ndocuments = SimpleDirectoryReader(\\n    \\\"./data\\\", file_extractor=file_extractor\\n).load_data()\\n```\\n\\nThis loader is designed to be used as a way to load data into [LlamaIndex](https://github.com/run-llama/llama_index/).\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"llama-index readers file integration\", \"latest_version\": \"0.5.4\", \"weekly_downloads\": 787538, \"description_cleaned\": \"LlamaIndex Readers Integration: File\\npip\\ninstall\\nllama-index-readers-file\\nThis is the default integration for different loaders that are used within\\nSimpleDirectoryReader\\n.\\nProvides support for the following loaders:\\nDocxReader\\nHWPReader\\nPDFReader\\nEpubReader\\nFlatReader\\nHTMLTagReader\\nImageCaptionReader\\nImageReader\\nImageVisionLLMReader\\nIPYNBReader\\nMarkdownReader\\nMboxReader\\nPptxReader\\nPandasCSVReader\\nVideoAudioReader\\nUnstructuredReader\\nPyMuPDFReader\\nImageTabularChartReader\\nXMLReader\\nPagedCSVReader\\nCSVReader\\nRTFReader\\nInstallation\\npip\\ninstall\\nllama-index-readers-file\\nUsage\\nOnce installed, You can import any of the loader. Here's an example usage of one of the loader.\\nfrom\\nllama_index.core\\nimport\\nSimpleDirectoryReader\\nfrom\\nllama_index.readers.file\\nimport\\n(\\nDocxReader\\n,\\nHWPReader\\n,\\nPDFReader\\n,\\nEpubReader\\n,\\nFlatReader\\n,\\nHTMLTagReader\\n,\\nImageCaptionReader\\n,\\nImageReader\\n,\\nImageVisionLLMReader\\n,\\nIPYNBReader\\n,\\nMarkdownReader\\n,\\nMboxReader\\n,\\nPptxReader\\n,\\nPandasCSVReader\\n,\\nVideoAudioReader\\n,\\nUnstructuredReader\\n,\\nPyMuPDFReader\\n,\\nImageTabularChartReader\\n,\\nXMLReader\\n,\\nPagedCSVReader\\n,\\nCSVReader\\n,\\nRTFReader\\n,\\n)\\n# PDF Reader with `SimpleDirectoryReader`\\nparser\\n=\\nPDFReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".pdf\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Docx Reader example\\nparser\\n=\\nDocxReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".docx\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# HWP Reader example\\nparser\\n=\\nHWPReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".hwp\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Epub Reader example\\nparser\\n=\\nEpubReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".epub\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Flat Reader example\\nparser\\n=\\nFlatReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".txt\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# HTML Tag Reader example\\nparser\\n=\\nHTMLTagReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".html\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Image Reader example\\nparser\\n=\\nImageReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".jpg\\\"\\n:\\nparser\\n,\\n\\\".jpeg\\\"\\n:\\nparser\\n,\\n\\\".png\\\"\\n:\\nparser\\n,\\n}\\n# Add other image formats as needed\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# IPYNB Reader example\\nparser\\n=\\nIPYNBReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".ipynb\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Markdown Reader example\\nparser\\n=\\nMarkdownReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".md\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Mbox Reader example\\nparser\\n=\\nMboxReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".mbox\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Pptx Reader example\\n# Basic usage - extracts text, tables, charts, and speaker notes\\nparser\\n=\\nPptxReader\\n()\\n# Advanced usage - control parsing behavior\\nparser\\n=\\nPptxReader\\n(\\nextract_images\\n=\\nTrue\\n,\\n# Enable image captioning\\ncontext_consolidation_with_llm\\n=\\nTrue\\n,\\n# Use LLM for content synthesis\\nnum_workers\\n=\\n4\\n,\\n# Parallel processing\\nbatch_size\\n=\\n10\\n,\\n# Slides processed per worker batch\\nraise_on_error\\n=\\nTrue\\n,\\n# Raise value error if file_parsing is not successful\\n)\\nfile_extractor\\n=\\n{\\n\\\".pptx\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Pandas CSV Reader example\\nparser\\n=\\nPandasCSVReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".csv\\\"\\n:\\nparser\\n}\\n# Add other CSV formats as needed\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# PyMuPDF Reader example\\nparser\\n=\\nPyMuPDFReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".pdf\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# XML Reader example\\nparser\\n=\\nXMLReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".xml\\\"\\n:\\nparser\\n}\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# Paged CSV Reader example\\nparser\\n=\\nPagedCSVReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".csv\\\"\\n:\\nparser\\n}\\n# Add other CSV formats as needed\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\n# CSV Reader example\\nparser\\n=\\nCSVReader\\n()\\nfile_extractor\\n=\\n{\\n\\\".csv\\\"\\n:\\nparser\\n}\\n# Add other CSV formats as needed\\ndocuments\\n=\\nSimpleDirectoryReader\\n(\\n\\\"./data\\\"\\n,\\nfile_extractor\\n=\\nfile_extractor\\n)\\n.\\nload_data\\n()\\nThis loader is designed to be used as a way to load data into\\nLlamaIndex\\n.\"}, {\"name\": \"tink\", \"description\": \"# Tink Python\\n\\n<!-- GCP Ubuntu --->\\n\\n[bazel_badge_gcp_ubuntu]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-bazel-gcp-ubuntu.svg\\n[bazel_kms_badge_gcp_ubuntu]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-bazel-kms-gcp-ubuntu.svg\\n[pip_badge_gcp_ubuntu]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-pip-gcp-ubuntu.svg\\n[pip_kms_badge_gcp_ubuntu]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-pip-kms-gcp-ubuntu.svg\\n[bdist_badge_gcp_ubuntu]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-release-bdist-create-gcp-ubuntu.svg\\n[sdist_create_badge_gcp_ubuntu]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-release-sdist-create-gcp-ubuntu.svg\\n[sdist_test_badge_gcp_ubuntu]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-release-sdist-test-gcp-ubuntu.svg\\n\\n<!-- GCP Ubuntu (aarch64) --->\\n\\n[bdist_create_badge_gcp_ubuntu_aarch64]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-release-bdist-create-gcp_ubuntu-arm64-external.svg\\n\\n<!-- GCP Windows --->\\n\\n[bazel_badge_gcp_windows]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-bazel-gcp-windows.svg\\n[pip_badge_gcp_windows]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-pip-gcp-windows.svg\\n[bdist_badge_gcp_windows]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-release-bdist-create-gcp-windows.svg\\n\\n<!-- MacOS --->\\n\\n[bazel_badge_macos]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-bazel-macos-external.svg\\n[bazel_kms_badge_macos]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-bazel-kms-macos-external.svg\\n[pip_badge_macos]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-pip-macos-external.svg\\n[pip_kms_badge_macos]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-pip-kms-macos-external.svg\\n[bdist_create_badge_macos]: https://storage.googleapis.com/tink-kokoro-build-badges/tink-py-release-bdist-create-macos-external.svg\\n\\n**Test**              | **GCP Ubuntu**                                                | **GCP Ubuntu (aarch64)**                                               | **MacOS**                                      | **GCP Windows**\\n--------------------- | ------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------- | ---------------\\nBazel                 | [![Bazel_GcpUbuntu][bazel_badge_gcp_ubuntu]](#)               | N/A                                                                    | [![Bazel_MacOs][bazel_badge_macos]](#)         | [![Bazel_GcpWindows][bazel_badge_gcp_windows]](#)\\nBazel (with KMS)      | [![Bazel_Kms_GcpUbuntu][bazel_kms_badge_gcp_ubuntu]](#)       | N/A                                                                    | [![Bazel_Kms_MacOs][bazel_kms_badge_macos]](#) | N/A\\nPip                   | [![Pip_GcpUbuntu][pip_badge_gcp_ubuntu]](#)                   | N/A                                                                    | [![Pip_MacOs][pip_badge_macos]](#)             | [![Pip_GcpWindows][pip_badge_gcp_windows]](#)\\nPip (with KMS)        | [![Pip_Kms_GcpUbuntu][pip_kms_badge_gcp_ubuntu]](#)           | N/A                                                                    | [![Pip_Kms_MacOs][pip_kms_badge_macos]](#)     | N/A\\nBdist (Create + Test) | [![Bdist_GcpUbuntu][bdist_badge_gcp_ubuntu]](#)               | [![Bdist_GcpUbuntu_Aarch64][bdist_create_badge_gcp_ubuntu_aarch64]](#) | [![Bdist_MacOs][bdist_create_badge_macos]](#)  | [![Bdist_GcpWindows][bdist_badge_gcp_windows]](#)\\nSdist (Create)        | [![Sdist_Create_GcpUbuntu][sdist_create_badge_gcp_ubuntu]](#) | N/A                                                                    | N/A                                            | N/A\\nSdist (Test)          | [![Sdist_Test_GcpUbuntu][sdist_test_badge_gcp_ubuntu]](#)     | N/A                                                                    | N/A                                            | N/A\\n\\n\\nUsing crypto in your application [shouldn't have to][devs_are_users_too_slides]\\nfeel like juggling chainsaws in the dark. Tink is a crypto library written by a\\ngroup of cryptographers and security engineers at Google. It was born out of our\\nextensive experience working with Google's product teams,\\n[fixing weaknesses in implementations](https://github.com/google/wycheproof),\\nand providing simple APIs that can be used safely without needing a crypto\\nbackground.\\n\\nTink provides secure APIs that are easy to use correctly and hard(er) to misuse.\\nIt reduces common crypto pitfalls with user-centered design, careful\\nimplementation and code reviews, and extensive testing. At Google, Tink is one\\nof the standard crypto libraries, and has been deployed in hundreds of products\\nand systems.\\n\\nTo get a quick overview of Tink's design please take a look at\\n[Tink's goals](https://developers.google.com/tink/design/goals_of_tink).\\n\\nThe official documentation is available at https://developers.google.com/tink.\\n\\n[devs_are_users_too_slides]: https://www.usenix.org/sites/default/files/conference/protected-files/hotsec15_slides_green.pdf\\n\\n## Contact and mailing list\\n\\nIf you want to contribute, please read [CONTRIBUTING](docs/CONTRIBUTING.md) and\\nsend us pull requests. You can also report bugs or file feature requests.\\n\\nIf you'd like to talk to the developers or get notified about major product\\nupdates, you may want to subscribe to our\\n[mailing list](https://groups.google.com/forum/#!forum/tink-users).\\n\\n## Maintainers\\n\\nTink is maintained by (A-Z):\\n\\n-   Moreno Ambrosin\\n-   Taymon Beal\\n-   William Conner\\n-   Thomas Holenstein\\n-   Stefan K\\u00f6lbl\\n-   Charles Lee\\n-   Cindy Lin\\n-   Fernando Lobato Meeser\\n-   Ioana Nedelcu\\n-   Sophie Schmieg\\n-   Elizaveta Tretiakova\\n-   J\\u00fcrg Wullschleger\\n\\nAlumni:\\n\\n-   Haris Andrianakis\\n-   Daniel Bleichenbacher\\n-   Tanuj Dhir\\n-   Thai Duong\\n-   Atul Luykx\\n-   Rafael Misoczki\\n-   Quan Nguyen\\n-   Bartosz Przydatek\\n-   Enzo Puig\\n-   Laurent Simon\\n-   Veronika Sl\\u00edvov\\u00e1\\n-   Paula Vidas\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"A multi-language, cross-platform library that provides cryptographic APIs that are secure, easy to use correctly, and hard(er) to misuse.\", \"latest_version\": \"1.12.0\", \"weekly_downloads\": 787483, \"description_cleaned\": \"Tink Python\\nTest\\nGCP Ubuntu\\nGCP Ubuntu (aarch64)\\nMacOS\\nGCP Windows\\nBazel\\nN/A\\nBazel (with KMS)\\nN/A\\nN/A\\nPip\\nN/A\\nPip (with KMS)\\nN/A\\nN/A\\nBdist (Create + Test)\\nSdist (Create)\\nN/A\\nN/A\\nN/A\\nSdist (Test)\\nN/A\\nN/A\\nN/A\\nUsing crypto in your application\\nshouldn't have to\\nfeel like juggling chainsaws in the dark. Tink is a crypto library written by a\\ngroup of cryptographers and security engineers at Google. It was born out of our\\nextensive experience working with Google's product teams,\\nfixing weaknesses in implementations\\n,\\nand providing simple APIs that can be used safely without needing a crypto\\nbackground.\\nTink provides secure APIs that are easy to use correctly and hard(er) to misuse.\\nIt reduces common crypto pitfalls with user-centered design, careful\\nimplementation and code reviews, and extensive testing. At Google, Tink is one\\nof the standard crypto libraries, and has been deployed in hundreds of products\\nand systems.\\nTo get a quick overview of Tink's design please take a look at\\nTink's goals\\n.\\nThe official documentation is available at\\nhttps://developers.google.com/tink\\n.\\nContact and mailing list\\nIf you want to contribute, please read\\nCONTRIBUTING\\nand\\nsend us pull requests. You can also report bugs or file feature requests.\\nIf you'd like to talk to the developers or get notified about major product\\nupdates, you may want to subscribe to our\\nmailing list\\n.\\nMaintainers\\nTink is maintained by (A-Z):\\nMoreno Ambrosin\\nTaymon Beal\\nWilliam Conner\\nThomas Holenstein\\nStefan K\\u00f6lbl\\nCharles Lee\\nCindy Lin\\nFernando Lobato Meeser\\nIoana Nedelcu\\nSophie Schmieg\\nElizaveta Tretiakova\\nJ\\u00fcrg Wullschleger\\nAlumni:\\nHaris Andrianakis\\nDaniel Bleichenbacher\\nTanuj Dhir\\nThai Duong\\nAtul Luykx\\nRafael Misoczki\\nQuan Nguyen\\nBartosz Przydatek\\nEnzo Puig\\nLaurent Simon\\nVeronika Sl\\u00edvov\\u00e1\\nPaula Vidas\"}, {\"name\": \"eralchemy\", \"description\": \"[![license](https://img.shields.io/badge/License-Apache%202.0-yellow?logo=opensourceinitiative&logoColor=white)](LICENSE)\\n[![PyPI - Version](https://img.shields.io/pypi/v/eralchemy?logo=pypi&logoColor=white)](https://pypi.org/project/ERAlchemy/)\\n[![PyPI Downloads](https://img.shields.io/pypi/dm/eralchemy?logo=pypi&logoColor=white)](https://pypi.org/project/eralchemy/)\\n[![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/eralchemy/eralchemy/unit.yaml?logo=github&logoColor=white)](https://github.com/eralchemy/eralchemy/actions/workflows/unit.yaml)\\n[![Codecov](https://img.shields.io/codecov/c/github/eralchemy/eralchemy?logo=codecov&logoColor=white&token=gSfKRZVvAh)](https://app.codecov.io/gh/eralchemy/eralchemy/tree/main)\\n\\n# Entity relation diagrams generator\\n\\neralchemy generates Entity Relation (ER) diagram (like the one below) from databases or from SQLAlchemy models.\\n\\n## Example\\n\\n![Example for a graph](https://raw.githubusercontent.com/eralchemy/eralchemy/main/docs/_static/forum.svg \\\"Example for a simple Forum\\\")\\n\\n## Quick Start\\n\\n### Install\\n\\nTo install eralchemy, just do:\\n\\n    $ pip install eralchemy\\n\\n### Graph library flavors\\n\\nTo create Pictures and PDFs, eralchemy relies on either graphviz or pygraphviz.\\n\\nYou can use either\\n\\n    $ pip install eralchemy[graphviz]\\n\\nor\\n\\n    $ pip install eralchemy[pygraphviz]\\n\\nto retrieve the correct dependencies.\\nThe `graphviz` library is the default if both are installed.\\n\\n`eralchemy` requires [GraphViz](http://www.graphviz.org/download) to generate the graphs and Python. Both are available for Windows, Mac and Linux.\\n\\nFor Debian based systems, run:\\n\\n    $ apt install graphviz libgraphviz-dev\\n\\nbefore installing eralchemy.\\n\\n### Install using conda\\n\\nThere is also a packaged version in conda-forge, which directly installs the dependencies:\\n\\n    $ conda install -c conda-forge eralchemy\\n\\n### Usage from Command Line\\n\\n#### From a database\\n\\n    $ eralchemy -i sqlite:///relative/path/to/db.db -o erd_from_sqlite.pdf\\n\\nThe database is specified as a [SQLAlchemy](https://docs.sqlalchemy.org/en/20/core/engines.html#database-urls)\\ndatabase url.\\n\\n#### From a markdown file.\\n\\n    $ curl 'https://raw.githubusercontent.com/eralchemy/eralchemy/main/example/forum.er' > markdown_file.er\\n    $ eralchemy -i 'markdown_file.er' -o erd_from_markdown_file.pdf\\n\\n#### From a Postgresql DB to a markdown file excluding tables named `temp` and `audit`\\n\\n    $ eralchemy -i 'postgresql+psycopg2://username:password@hostname:5432/databasename' -o filtered.er --exclude-tables temp audit\\n\\n#### From a Postgresql DB to a markdown file excluding columns named `created_at` and `updated_at` from all tables\\n\\n    $ eralchemy -i 'postgresql+psycopg2://username:password@hostname:5432/databasename' -o filtered.er --exclude-columns created_at updated_at\\n\\n#### From a Postgresql DB to a markdown file for the schemas `schema1` and `schema2`\\n\\n    $ eralchemy -i 'postgresql+psycopg2://username:password@hostname:5432/databasename' -s \\\"schema1, schema2\\\"\\n\\n#### Specify Output Mode\\n\\n    $ eralchemy -i 'markdown_file.er' -o erd_from_markdown_file.md -m mermaid_er\\n\\n### Usage from Python\\n\\n```python\\nfrom eralchemy import render_er\\n## Draw from SQLAlchemy base\\nrender_er(Base, 'erd_from_sqlalchemy.png')\\n\\n## Draw from database\\nrender_er(\\\"sqlite:///relative/path/to/db.db\\\", 'erd_from_sqlite.png')\\n```\\n\\n#### Adjustments to the rendering config\\n\\nWhen rendering dot files, it can be needed to adjust how some parts are visualized.\\nThis can be used to get `crowfoot` relations, stars instead of underlines for primary keys or a top-bottom rendering instead the default left-right rendering.\\n\\nIt can be adjusted by manipulating the global `from eralchemy.cst import config` dictionary.\\n\\nSome helper functions exist like `dot_star_primary`, `dot_top_down`, `dot_digraph` and `dot_crowfoot`.\\nThe config can be reset using `reset_config`.\\n\\nThis can be used like\\n\\n```python\\nfrom eralchemy import render_er\\nfrom eralchemy.cst import dot_crowfoot, dot_digraph\\ndot_crowfoot()\\ndot_digraph()\\n\\nrender_er(Base, \\\"forum.svg\\\")\\n```\\n\\n## Architecture\\n\\n```mermaid\\n\\ngraph LR\\n    subgraph Inputs\\n        A[Markdown representation]\\n        )B[SQLAlchemy Schema]\\n        C[Existing database]\\n        D[Other ORM ?]\\n    end\\n\\n    E[Intermediary representation]\\n\\n    subgraph Outputs\\n        F[Markdown representation]\\n        G[Graphviz code]\\n        H[Drawing]\\n    end\\n\\n    A --> E\\n    B --> E\\n    C --> E\\n    D --> E\\n    E --> F\\n    E --> G\\n    E --> H\\n\\n```\\n\\nThanks to it's modular architecture, it can be connected to other ORMs/ODMs/OGMs/O\\\\*Ms.\\n\\n## Contribute\\n\\nEvery feedback is welcome on the [GitHub issues](https://github.com/eralchemy/eralchemy/issues).\\n\\n### Development\\n\\nInstall the development dependencies using\\n\\n    $ pip install -e .[ci,dev]\\n\\nMake sure to run the pre-commit to fix formatting\\n\\n    $ pre-commit run --all\\n\\nAll tested PR are welcome.\\n\\n## Running tests\\n\\nThis project uses the pytest test suite.\\nTo run the tests, use : `$ pytest` or `$ tox`.\\n\\nSome tests require having a local PostgreSQL database with a schema named test in a database\\nnamed test all owned by a user named eralchemy with a password of eralchemy.\\nIf docker compose is available, one can use `docker compose up -d` for this purpose.\\nYou can deselect the tests which require a PostgreSQL database using:\\n\\n    $ pytest -m \\\"not external_db\\\"\\n\\n## Publishing a release\\n\\n    $ rm -r dist && python -m build && python3 -m twine upload --repository pypi dist/*\\n\\n## Notes\\n\\nERAlchemy was inspired by [erd](https://github.com/BurntSushi/erd), though it is able to render the ER diagram directly\\nfrom the database and not just only from the `ER` markup language.\\n\\nReleased under an Apache License 2.0\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Simple entity relation (ER) diagrams generation\", \"latest_version\": \"1.6.0\", \"weekly_downloads\": 786180, \"description_cleaned\": \"Entity relation diagrams generator\\neralchemy generates Entity Relation (ER) diagram (like the one below) from databases or from SQLAlchemy models.\\nExample\\nQuick Start\\nInstall\\nTo install eralchemy, just do:\\n$ pip install eralchemy\\nGraph library flavors\\nTo create Pictures and PDFs, eralchemy relies on either graphviz or pygraphviz.\\nYou can use either\\n$ pip install eralchemy[graphviz]\\nor\\n$ pip install eralchemy[pygraphviz]\\nto retrieve the correct dependencies.\\nThe\\ngraphviz\\nlibrary is the default if both are installed.\\neralchemy\\nrequires\\nGraphViz\\nto generate the graphs and Python. Both are available for Windows, Mac and Linux.\\nFor Debian based systems, run:\\n$ apt install graphviz libgraphviz-dev\\nbefore installing eralchemy.\\nInstall using conda\\nThere is also a packaged version in conda-forge, which directly installs the dependencies:\\n$ conda install -c conda-forge eralchemy\\nUsage from Command Line\\nFrom a database\\n$ eralchemy -i sqlite:///relative/path/to/db.db -o erd_from_sqlite.pdf\\nThe database is specified as a\\nSQLAlchemy\\ndatabase url.\\nFrom a markdown file.\\n$ curl 'https://raw.githubusercontent.com/eralchemy/eralchemy/main/example/forum.er' > markdown_file.er\\n$ eralchemy -i 'markdown_file.er' -o erd_from_markdown_file.pdf\\nFrom a Postgresql DB to a markdown file excluding tables named\\ntemp\\nand\\naudit\\n$ eralchemy -i 'postgresql+psycopg2://username:password@hostname:5432/databasename' -o filtered.er --exclude-tables temp audit\\nFrom a Postgresql DB to a markdown file excluding columns named\\ncreated_at\\nand\\nupdated_at\\nfrom all tables\\n$ eralchemy -i 'postgresql+psycopg2://username:password@hostname:5432/databasename' -o filtered.er --exclude-columns created_at updated_at\\nFrom a Postgresql DB to a markdown file for the schemas\\nschema1\\nand\\nschema2\\n$ eralchemy -i 'postgresql+psycopg2://username:password@hostname:5432/databasename' -s \\\"schema1, schema2\\\"\\nSpecify Output Mode\\n$ eralchemy -i 'markdown_file.er' -o erd_from_markdown_file.md -m mermaid_er\\nUsage from Python\\nfrom\\neralchemy\\nimport\\nrender_er\\n## Draw from SQLAlchemy base\\nrender_er\\n(\\nBase\\n,\\n'erd_from_sqlalchemy.png'\\n)\\n## Draw from database\\nrender_er\\n(\\n\\\"sqlite:///relative/path/to/db.db\\\"\\n,\\n'erd_from_sqlite.png'\\n)\\nAdjustments to the rendering config\\nWhen rendering dot files, it can be needed to adjust how some parts are visualized.\\nThis can be used to get\\ncrowfoot\\nrelations, stars instead of underlines for primary keys or a top-bottom rendering instead the default left-right rendering.\\nIt can be adjusted by manipulating the global\\nfrom eralchemy.cst import config\\ndictionary.\\nSome helper functions exist like\\ndot_star_primary\\n,\\ndot_top_down\\n,\\ndot_digraph\\nand\\ndot_crowfoot\\n.\\nThe config can be reset using\\nreset_config\\n.\\nThis can be used like\\nfrom\\neralchemy\\nimport\\nrender_er\\nfrom\\neralchemy.cst\\nimport\\ndot_crowfoot\\n,\\ndot_digraph\\ndot_crowfoot\\n()\\ndot_digraph\\n()\\nrender_er\\n(\\nBase\\n,\\n\\\"forum.svg\\\"\\n)\\nArchitecture\\ngraph LR\\n    subgraph Inputs\\n        A[Markdown representation]\\n        )B[SQLAlchemy Schema]\\n        C[Existing database]\\n        D[Other ORM ?]\\n    end\\n\\n    E[Intermediary representation]\\n\\n    subgraph Outputs\\n        F[Markdown representation]\\n        G[Graphviz code]\\n        H[Drawing]\\n    end\\n\\n    A --> E\\n    B --> E\\n    C --> E\\n    D --> E\\n    E --> F\\n    E --> G\\n    E --> H\\nThanks to it's modular architecture, it can be connected to other ORMs/ODMs/OGMs/O*Ms.\\nContribute\\nEvery feedback is welcome on the\\nGitHub issues\\n.\\nDevelopment\\nInstall the development dependencies using\\n$ pip install -e .[ci,dev]\\nMake sure to run the pre-commit to fix formatting\\n$ pre-commit run --all\\nAll tested PR are welcome.\\nRunning tests\\nThis project uses the pytest test suite.\\nTo run the tests, use :\\n$ pytest\\nor\\n$ tox\\n.\\nSome tests require having a local PostgreSQL database with a schema named test in a database\\nnamed test all owned by a user named eralchemy with a password of eralchemy.\\nIf docker compose is available, one can use\\ndocker compose up -d\\nfor this purpose.\\nYou can deselect the tests which require a PostgreSQL database using:\\n$ pytest -m \\\"not external_db\\\"\\nPublishing a release\\n$ rm -r dist && python -m build && python3 -m twine upload --repository pypi dist/*\\nNotes\\nERAlchemy was inspired by\\nerd\\n, though it is able to render the ER diagram directly\\nfrom the database and not just only from the\\nER\\nmarkup language.\\nReleased under an Apache License 2.0\"}, {\"name\": \"azure-mgmt-dns\", \"description\": \"# Microsoft Azure SDK for Python\\n\\nThis is the Microsoft Azure DNS Management Client Library.\\nThis package has been tested with Python 3.9+.\\nFor a more complete view of Azure libraries, see the [azure sdk python release](https://aka.ms/azsdk/python/all).\\n\\n## _Disclaimer_\\n\\n_Azure SDK Python packages support for Python 2.7 has ended 01 January 2022. For more information and questions, please refer to https://github.com/Azure/azure-sdk-for-python/issues/20691_\\n\\n## Getting started\\n\\n### Prerequisites\\n\\n- Python 3.9+ is required to use this package.\\n- [Azure subscription](https://azure.microsoft.com/free/)\\n\\n### Install the package\\n\\n```bash\\npip install azure-mgmt-dns\\npip install azure-identity\\n```\\n\\n### Authentication\\n\\nBy default, [Azure Active Directory](https://aka.ms/awps/aad) token authentication depends on correct configuration of the following environment variables.\\n\\n- `AZURE_CLIENT_ID` for Azure client ID.\\n- `AZURE_TENANT_ID` for Azure tenant ID.\\n- `AZURE_CLIENT_SECRET` for Azure client secret.\\n\\nIn addition, Azure subscription ID can be configured via environment variable `AZURE_SUBSCRIPTION_ID`.\\n\\nWith above configuration, client can be authenticated by following code:\\n\\n```python\\nfrom azure.identity import DefaultAzureCredential\\nfrom azure.mgmt.dns import DnsManagementClient\\nimport os\\n\\nsub_id = os.getenv(\\\"AZURE_SUBSCRIPTION_ID\\\")\\nclient = DnsManagementClient(credential=DefaultAzureCredential(), subscription_id=sub_id)\\n```\\n\\n## Examples\\n\\nCode samples for this package can be found at:\\n- [Search DNS Management](https://docs.microsoft.com/samples/browse/?languages=python&term=Getting%20started%20-%20Managing&terms=Getting%20started%20-%20Managing) on docs.microsoft.com\\n- [Azure Python Mgmt SDK Samples Repo](https://aka.ms/azsdk/python/mgmt/samples)\\n\\n\\n## Troubleshooting\\n\\n## Next steps\\n\\n## Provide Feedback\\n\\nIf you encounter any bugs or have suggestions, please file an issue in the\\n[Issues](https://github.com/Azure/azure-sdk-for-python/issues)\\nsection of the project. \\n\\n\\n# Release History\\n\\n## 9.0.0 (2025-07-14)\\n\\n### Breaking Changes\\n\\n  - This package now only targets the latest Api-Version available on Azure and removes APIs of other Api-Version. After this change, the package can have much smaller size. If your application requires a specific and non-latest Api-Version, it's recommended to pin this package to the previous released version; If your application always only use latest Api-Version, please ignore this change.\\n  - Deleted or renamed client operation group `DnsManagementClient.dnssec_configs`\\n  - Model `RecordSet` deleted or renamed its instance variable `traffic_management_profile`\\n  - Model `RecordSet` deleted or renamed its instance variable `ds_records`\\n  - Model `RecordSet` deleted or renamed its instance variable `tlsa_records`\\n  - Model `RecordSet` deleted or renamed its instance variable `naptr_records`\\n  - Deleted or renamed enum value `RecordType.DS`\\n  - Deleted or renamed enum value `RecordType.NAPTR`\\n  - Deleted or renamed enum value `RecordType.TLSA`\\n  - Model `Zone` deleted or renamed its instance variable `system_data`\\n  - Model `Zone` deleted or renamed its instance variable `signing_keys`\\n  - Deleted or renamed model `CreatedByType`\\n  - Deleted or renamed model `DelegationSignerInfo`\\n  - Deleted or renamed model `Digest`\\n  - Deleted or renamed model `DnssecConfig`\\n  - Deleted or renamed model `DsRecord`\\n  - Deleted or renamed model `NaptrRecord`\\n  - Deleted or renamed model `SigningKey`\\n  - Deleted or renamed model `SystemData`\\n  - Deleted or renamed model `TlsaRecord`\\n  - Deleted or renamed operation group `DnssecConfigsOperations`\\n\\n## 8.2.0 (2024-10-22)\\n\\n### Features Added\\n\\n  - Model RecordSet has a new parameter traffic_management_profile\\n\\n## 8.1.0 (2023-06-14)\\n\\n### Features Added\\n\\n  - Added operation group DnssecConfigsOperations\\n  - Model RecordSet has a new parameter ds_records\\n  - Model RecordSet has a new parameter naptr_records\\n  - Model RecordSet has a new parameter tlsa_records\\n  - Model Zone has a new parameter signing_keys\\n  - Model Zone has a new parameter system_data\\n\\n## 8.1.0b1 (2023-02-10)\\n\\n### Other Changes\\n\\n  - Added generated samples in github repo\\n  - Drop support for python<3.7.0\\n\\n## 8.0.0 (2021-04-14)\\n\\n - GA release\\n\\n## 8.0.0b1 (2021-03-10)\\n\\nThis is beta preview version.\\nFor detailed changelog please refer to equivalent stable version 10.2.0 (https://pypi.org/project/azure-mgmt-network/10.2.0/)\\n\\nThis version uses a next-generation code generator that introduces important breaking changes, but also important new features (like unified authentication and async programming).\\n\\n**General breaking changes**\\n\\n- Credential system has been completly revamped:\\n\\n  - `azure.common.credentials` or `msrestazure.azure_active_directory` instances are no longer supported, use the `azure-identity` classes instead: https://pypi.org/project/azure-identity/\\n  - `credentials` parameter has been renamed `credential`\\n\\n- The `config` attribute no longer exists on a client, configuration should be passed as kwarg. Example: `MyClient(credential, subscription_id, enable_logging=True)`. For a complete set of\\n  supported options, see the [parameters accept in init documentation of azure-core](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/CLIENT_LIBRARY_DEVELOPER.md#available-policies)\\n- You can't import a `version` module anymore, use `__version__` instead\\n- Operations that used to return a `msrest.polling.LROPoller` now returns a `azure.core.polling.LROPoller` and are prefixed with `begin_`.\\n- Exceptions tree have been simplified and most exceptions are now `azure.core.exceptions.HttpResponseError` (`CloudError` has been removed).\\n- Most of the operation kwarg have changed. Some of the most noticeable:\\n\\n  - `raw` has been removed. Equivalent feature can be found using `cls`, a callback that will give access to internal HTTP response for advanced user\\n  - For a complete set of\\n  supported options, see the [parameters accept in Request documentation of azure-core](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/CLIENT_LIBRARY_DEVELOPER.md#available-policies)\\n\\n**General new features**\\n\\n- Type annotations support using `typing`. SDKs are mypy ready.\\n- This client has now stable and official support for async. Check the `aio` namespace of your package to find the async client.\\n- This client now support natively tracing library like OpenCensus or OpenTelemetry. See this [tracing quickstart](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/core/azure-core-tracing-opentelemetry) for an overview.\\n\\n\\n## 3.0.0 (2019-06-18)\\n\\n**General Breaking changes**\\n\\nThis version uses a next-generation code generator that *might*\\nintroduce breaking changes if you were importing from the v20xx_yy_zz\\nAPI folders. In summary, some modules were incorrectly\\nvisible/importable and have been renamed. This fixed several issues\\ncaused by usage of classes that were not supposed to be used in the\\nfirst place.\\n\\n  - DnsManagementClient cannot be imported from\\n    `azure.mgmt.dns.v20xx_yy_zz.dns_management_client` anymore\\n    (import from `azure.mgmt.dns.v20xx_yy_zz` works like before)\\n  - DnsManagementClientConfiguration import has been moved from\\n    `azure.mgmt.dns.v20xx_yy_zz.dns_management_client` to\\n    `azure.mgmt.dns.v20xx_yy_zz`\\n  - A model `MyClass` from a \\\"models\\\" sub-module cannot be imported\\n    anymore using `azure.mgmt.dns.v20xx_yy_zz.models.my_class`\\n    (import from `azure.mgmt.dns.v20xx_yy_zz.models` works like\\n    before)\\n  - An operation class `MyClassOperations` from an `operations`\\n    sub-module cannot be imported anymore using\\n    `azure.mgmt.dns.v20xx_yy_zz.operations.my_class_operations`\\n    (import from `azure.mgmt.dns.v20xx_yy_zz.operations` works like\\n    before)\\n\\nLast but not least, HTTP connection pooling is now enabled by default.\\nYou should always use a client as a context manager, or call close(), or\\nuse no more than one client per process.\\n\\n## 2.1.0 (2018-09-10)\\n\\n**Features**\\n\\n  - Model RecordSet has a new parameter target_resource\\n  - Added operation group DnsResourceReferenceOperations\\n\\n## 2.0.0 (2018-07-01)\\n\\n**Bugfixes**\\n\\n  - Fix ARM compliance (correct settings of location, tags, etc.)\\n\\n## 2.0.0rc2 (2018-07-05)\\n\\n**Features**\\n\\n  - Client class can be used as a context manager to keep the underlying\\n    HTTP session open for performance\\n\\n**General Breaking changes**\\n\\nThis version uses a next-generation code generator that *might*\\nintroduce breaking changes.\\n\\n  - Model signatures now use only keyword-argument syntax. All\\n    positional arguments must be re-written as keyword-arguments. To\\n    keep auto-completion in most cases, models are now generated for\\n    Python 2 and Python 3. Python 3 uses the \\\"*\\\" syntax for\\n    keyword-only arguments.\\n  - Enum types now use the \\\"str\\\" mixin (class AzureEnum(str, Enum)) to\\n    improve the behavior when unrecognized enum values are encountered.\\n    While this is not a breaking change, the distinctions are important,\\n    and are documented here:\\n    <https://docs.python.org/3/library/enum.html#others> At a glance:\\n      - \\\"is\\\" should not be used at all.\\n      - \\\"format\\\" will return the string value, where \\\"%s\\\" string\\n        formatting will return `NameOfEnum.stringvalue`. Format syntax\\n        should be prefered.\\n  - New Long Running Operation:\\n      - Return type changes from\\n        `msrestazure.azure_operation.AzureOperationPoller` to\\n        `msrest.polling.LROPoller`. External API is the same.\\n      - Return type is now **always** a `msrest.polling.LROPoller`,\\n        regardless of the optional parameters used.\\n      - The behavior has changed when using `raw=True`. Instead of\\n        returning the initial call result as `ClientRawResponse`,\\n        without polling, now this returns an LROPoller. After polling,\\n        the final resource will be returned as a `ClientRawResponse`.\\n      - New `polling` parameter. The default behavior is\\n        `Polling=True` which will poll using ARM algorithm. When\\n        `Polling=False`, the response of the initial call will be\\n        returned without polling.\\n      - `polling` parameter accepts instances of subclasses of\\n        `msrest.polling.PollingMethod`.\\n      - `add_done_callback` will no longer raise if called after\\n        polling is finished, but will instead execute the callback right\\n        away.\\n\\n**Bugfixes**\\n\\n  - Compatibility of the sdist with wheel 0.31.0\\n\\n## 2.0.0rc1 (2018-03-14)\\n\\n**Features**\\n\\n  - Add public/private zone\\n  - Add record_sets.list_all_by_dns_zone operation\\n  - Add zones.update operation\\n\\n**Breaking changes**\\n\\n  - 'zone_type' is now required when creating a zone ('Public' is\\n    equivalent as previous behavior)\\n\\nNew API version 2018-03-01-preview\\n\\n## 1.2.0 (2017-10-26)\\n\\n  - add record_type CAA\\n  - remove pointless return type of delete\\n\\nApi version moves from 2016-04-01 to 2017-09-01\\n\\n## 1.1.0 (2017-10-10)\\n\\n  - Add \\\"recordsetnamesuffix\\\" filter parameter to list operations\\n\\n## 1.0.1 (2017-04-20)\\n\\nThis wheel package is now built with the azure wheel extension\\n\\n## 1.0.0 (2016-12-12)\\n\\n  - Initial release\\n\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Microsoft Azure DNS Management Client Library for Python\", \"latest_version\": \"9.0.0\", \"weekly_downloads\": 784138, \"description_cleaned\": \"Microsoft Azure SDK for Python\\nThis is the Microsoft Azure DNS Management Client Library.\\nThis package has been tested with Python 3.9+.\\nFor a more complete view of Azure libraries, see the\\nazure sdk python release\\n.\\nDisclaimer\\nAzure SDK Python packages support for Python 2.7 has ended 01 January 2022. For more information and questions, please refer to\\nhttps://github.com/Azure/azure-sdk-for-python/issues/20691\\nGetting started\\nPrerequisites\\nPython 3.9+ is required to use this package.\\nAzure subscription\\nInstall the package\\npip\\ninstall\\nazure-mgmt-dns\\npip\\ninstall\\nazure-identity\\nAuthentication\\nBy default,\\nAzure Active Directory\\ntoken authentication depends on correct configuration of the following environment variables.\\nAZURE_CLIENT_ID\\nfor Azure client ID.\\nAZURE_TENANT_ID\\nfor Azure tenant ID.\\nAZURE_CLIENT_SECRET\\nfor Azure client secret.\\nIn addition, Azure subscription ID can be configured via environment variable\\nAZURE_SUBSCRIPTION_ID\\n.\\nWith above configuration, client can be authenticated by following code:\\nfrom\\nazure.identity\\nimport\\nDefaultAzureCredential\\nfrom\\nazure.mgmt.dns\\nimport\\nDnsManagementClient\\nimport\\nos\\nsub_id\\n=\\nos\\n.\\ngetenv\\n(\\n\\\"AZURE_SUBSCRIPTION_ID\\\"\\n)\\nclient\\n=\\nDnsManagementClient\\n(\\ncredential\\n=\\nDefaultAzureCredential\\n(),\\nsubscription_id\\n=\\nsub_id\\n)\\nExamples\\nCode samples for this package can be found at:\\nSearch DNS Management\\non docs.microsoft.com\\nAzure Python Mgmt SDK Samples Repo\\nTroubleshooting\\nNext steps\\nProvide Feedback\\nIf you encounter any bugs or have suggestions, please file an issue in the\\nIssues\\nsection of the project.\\nRelease History\\n9.0.0 (2025-07-14)\\nBreaking Changes\\nThis package now only targets the latest Api-Version available on Azure and removes APIs of other Api-Version. After this change, the package can have much smaller size. If your application requires a specific and non-latest Api-Version, it's recommended to pin this package to the previous released version; If your application always only use latest Api-Version, please ignore this change.\\nDeleted or renamed client operation group\\nDnsManagementClient.dnssec_configs\\nModel\\nRecordSet\\ndeleted or renamed its instance variable\\ntraffic_management_profile\\nModel\\nRecordSet\\ndeleted or renamed its instance variable\\nds_records\\nModel\\nRecordSet\\ndeleted or renamed its instance variable\\ntlsa_records\\nModel\\nRecordSet\\ndeleted or renamed its instance variable\\nnaptr_records\\nDeleted or renamed enum value\\nRecordType.DS\\nDeleted or renamed enum value\\nRecordType.NAPTR\\nDeleted or renamed enum value\\nRecordType.TLSA\\nModel\\nZone\\ndeleted or renamed its instance variable\\nsystem_data\\nModel\\nZone\\ndeleted or renamed its instance variable\\nsigning_keys\\nDeleted or renamed model\\nCreatedByType\\nDeleted or renamed model\\nDelegationSignerInfo\\nDeleted or renamed model\\nDigest\\nDeleted or renamed model\\nDnssecConfig\\nDeleted or renamed model\\nDsRecord\\nDeleted or renamed model\\nNaptrRecord\\nDeleted or renamed model\\nSigningKey\\nDeleted or renamed model\\nSystemData\\nDeleted or renamed model\\nTlsaRecord\\nDeleted or renamed operation group\\nDnssecConfigsOperations\\n8.2.0 (2024-10-22)\\nFeatures Added\\nModel RecordSet has a new parameter traffic_management_profile\\n8.1.0 (2023-06-14)\\nFeatures Added\\nAdded operation group DnssecConfigsOperations\\nModel RecordSet has a new parameter ds_records\\nModel RecordSet has a new parameter naptr_records\\nModel RecordSet has a new parameter tlsa_records\\nModel Zone has a new parameter signing_keys\\nModel Zone has a new parameter system_data\\n8.1.0b1 (2023-02-10)\\nOther Changes\\nAdded generated samples in github repo\\nDrop support for python<3.7.0\\n8.0.0 (2021-04-14)\\nGA release\\n8.0.0b1 (2021-03-10)\\nThis is beta preview version.\\nFor detailed changelog please refer to equivalent stable version 10.2.0 (\\nhttps://pypi.org/project/azure-mgmt-network/10.2.0/\\n)\\nThis version uses a next-generation code generator that introduces important breaking changes, but also important new features (like unified authentication and async programming).\\nGeneral breaking changes\\nCredential system has been completly revamped:\\nazure.common.credentials\\nor\\nmsrestazure.azure_active_directory\\ninstances are no longer supported, use the\\nazure-identity\\nclasses instead:\\nhttps://pypi.org/project/azure-identity/\\ncredentials\\nparameter has been renamed\\ncredential\\nThe\\nconfig\\nattribute no longer exists on a client, configuration should be passed as kwarg. Example:\\nMyClient(credential, subscription_id, enable_logging=True)\\n. For a complete set of\\nsupported options, see the\\nparameters accept in init documentation of azure-core\\nYou can't import a\\nversion\\nmodule anymore, use\\n__version__\\ninstead\\nOperations that used to return a\\nmsrest.polling.LROPoller\\nnow returns a\\nazure.core.polling.LROPoller\\nand are prefixed with\\nbegin_\\n.\\nExceptions tree have been simplified and most exceptions are now\\nazure.core.exceptions.HttpResponseError\\n(\\nCloudError\\nhas been removed).\\nMost of the operation kwarg have changed. Some of the most noticeable:\\nraw\\nhas been removed. Equivalent feature can be found using\\ncls\\n, a callback that will give access to internal HTTP response for advanced user\\nFor a complete set of\\nsupported options, see the\\nparameters accept in Request documentation of azure-core\\nGeneral new features\\nType annotations support using\\ntyping\\n. SDKs are mypy ready.\\nThis client has now stable and official support for async. Check the\\naio\\nnamespace of your package to find the async client.\\nThis client now support natively tracing library like OpenCensus or OpenTelemetry. See this\\ntracing quickstart\\nfor an overview.\\n3.0.0 (2019-06-18)\\nGeneral Breaking changes\\nThis version uses a next-generation code generator that\\nmight\\nintroduce breaking changes if you were importing from the v20xx_yy_zz\\nAPI folders. In summary, some modules were incorrectly\\nvisible/importable and have been renamed. This fixed several issues\\ncaused by usage of classes that were not supposed to be used in the\\nfirst place.\\nDnsManagementClient cannot be imported from\\nazure.mgmt.dns.v20xx_yy_zz.dns_management_client\\nanymore\\n(import from\\nazure.mgmt.dns.v20xx_yy_zz\\nworks like before)\\nDnsManagementClientConfiguration import has been moved from\\nazure.mgmt.dns.v20xx_yy_zz.dns_management_client\\nto\\nazure.mgmt.dns.v20xx_yy_zz\\nA model\\nMyClass\\nfrom a \\\"models\\\" sub-module cannot be imported\\nanymore using\\nazure.mgmt.dns.v20xx_yy_zz.models.my_class\\n(import from\\nazure.mgmt.dns.v20xx_yy_zz.models\\nworks like\\nbefore)\\nAn operation class\\nMyClassOperations\\nfrom an\\noperations\\nsub-module cannot be imported anymore using\\nazure.mgmt.dns.v20xx_yy_zz.operations.my_class_operations\\n(import from\\nazure.mgmt.dns.v20xx_yy_zz.operations\\nworks like\\nbefore)\\nLast but not least, HTTP connection pooling is now enabled by default.\\nYou should always use a client as a context manager, or call close(), or\\nuse no more than one client per process.\\n2.1.0 (2018-09-10)\\nFeatures\\nModel RecordSet has a new parameter target_resource\\nAdded operation group DnsResourceReferenceOperations\\n2.0.0 (2018-07-01)\\nBugfixes\\nFix ARM compliance (correct settings of location, tags, etc.)\\n2.0.0rc2 (2018-07-05)\\nFeatures\\nClient class can be used as a context manager to keep the underlying\\nHTTP session open for performance\\nGeneral Breaking changes\\nThis version uses a next-generation code generator that\\nmight\\nintroduce breaking changes.\\nModel signatures now use only keyword-argument syntax. All\\npositional arguments must be re-written as keyword-arguments. To\\nkeep auto-completion in most cases, models are now generated for\\nPython 2 and Python 3. Python 3 uses the \\\"*\\\" syntax for\\nkeyword-only arguments.\\nEnum types now use the \\\"str\\\" mixin (class AzureEnum(str, Enum)) to\\nimprove the behavior when unrecognized enum values are encountered.\\nWhile this is not a breaking change, the distinctions are important,\\nand are documented here:\\nhttps://docs.python.org/3/library/enum.html#others\\nAt a glance:\\n\\\"is\\\" should not be used at all.\\n\\\"format\\\" will return the string value, where \\\"%s\\\" string\\nformatting will return\\nNameOfEnum.stringvalue\\n. Format syntax\\nshould be prefered.\\nNew Long Running Operation:\\nReturn type changes from\\nmsrestazure.azure_operation.AzureOperationPoller\\nto\\nmsrest.polling.LROPoller\\n. External API is the same.\\nReturn type is now\\nalways\\na\\nmsrest.polling.LROPoller\\n,\\nregardless of the optional parameters used.\\nThe behavior has changed when using\\nraw=True\\n. Instead of\\nreturning the initial call result as\\nClientRawResponse\\n,\\nwithout polling, now this returns an LROPoller. After polling,\\nthe final resource will be returned as a\\nClientRawResponse\\n.\\nNew\\npolling\\nparameter. The default behavior is\\nPolling=True\\nwhich will poll using ARM algorithm. When\\nPolling=False\\n, the response of the initial call will be\\nreturned without polling.\\npolling\\nparameter accepts instances of subclasses of\\nmsrest.polling.PollingMethod\\n.\\nadd_done_callback\\nwill no longer raise if called after\\npolling is finished, but will instead execute the callback right\\naway.\\nBugfixes\\nCompatibility of the sdist with wheel 0.31.0\\n2.0.0rc1 (2018-03-14)\\nFeatures\\nAdd public/private zone\\nAdd record_sets.list_all_by_dns_zone operation\\nAdd zones.update operation\\nBreaking changes\\n'zone_type' is now required when creating a zone ('Public' is\\nequivalent as previous behavior)\\nNew API version 2018-03-01-preview\\n1.2.0 (2017-10-26)\\nadd record_type CAA\\nremove pointless return type of delete\\nApi version moves from 2016-04-01 to 2017-09-01\\n1.1.0 (2017-10-10)\\nAdd \\\"recordsetnamesuffix\\\" filter parameter to list operations\\n1.0.1 (2017-04-20)\\nThis wheel package is now built with the azure wheel extension\\n1.0.0 (2016-12-12)\\nInitial release\"}, {\"name\": \"boxsdk\", \"description\": \"<p align=\\\"center\\\">\\n  <img src=\\\"https://github.com/box/sdks/blob/master/images/box-dev-logo.png\\\" alt= \\u201cbox-dev-logo\\u201d width=\\\"30%\\\" height=\\\"50%\\\">\\n</p>\\n\\n# Box Python SDK v4\\n\\n[![image](http://opensource.box.com/badges/active.svg)](http://opensource.box.com/badges)\\n[![image](https://github.com/box/box-python-sdk/actions/workflows/build.yml/badge.svg)](https://github.com/box/box-python-sdk/actions)\\n[![image](https://img.shields.io/pypi/v/boxsdk.svg)](https://pypi.python.org/pypi/boxsdk)\\n[![image](https://img.shields.io/pypi/dm/boxsdk.svg)](https://pypi.python.org/pypi/boxsdk)\\n[![image](https://coveralls.io/repos/github/box/box-python-sdk/badge.svg?branch=main)](https://coveralls.io/github/box/box-python-sdk?branch=main)\\n\\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\\n\\n- [Introduction](#introduction)\\n- [Supported versions](#supported-versions)\\n  - [Version v4](#version-v4)\\n  - [Version v10](#version-v10)\\n  - [Deprecation of `boxsdk` package](#deprecation-of-boxsdk-package)\\n  - [Which Version Should I Use?](#which-version-should-i-use)\\n- [Installing](#installing)\\n- [Getting Started](#getting-started)\\n  - [With box_sdk_gen package (recommended)](#with-box_sdk_gen-package-recommended)\\n  - [With boxsdk package (deprecated)](#with-boxsdk-package-deprecated)\\n- [Authentication](#authentication)\\n- [Using both box_sdk_gen and boxsdk packages simultaneously](#using-both-box_sdk_gen-and-boxsdk-packages-simultaneously)\\n- [Documentation](#documentation)\\n- [Migration guides](#migration-guides)\\n- [Versioning](#versioning)\\n  - [Version schedule](#version-schedule)\\n- [Contributing](#contributing)\\n- [FIPS 140-2 Compliance](#fips-140-2-compliance)\\n- [Questions, Bugs, and Feature Requests?](#questions-bugs-and-feature-requests)\\n- [Copyright and License](#copyright-and-license)\\n\\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\\n\\n# Introduction\\n\\nWe are excited to introduce the v4 major release of the Box Python SDK,\\ndesigned to elevate the developer experience and streamline your integration with the Box Content Cloud.\\n\\nWith this SDK version, alongside the existing `boxsdk` package, we\\u2019re introducing a new `box_sdk_gen` package, which gives you access to:\\n\\n1. Full API Support: The new generation of Box SDKs empowers developers with complete coverage of the Box API ecosystem. You can now access all the latest features and functionalities offered by Box, allowing you to build even more sophisticated and feature-rich applications.\\n2. Rapid API Updates: Say goodbye to waiting for new Box APIs to be incorporated into the SDK. With our new auto-generation development approach, we can now add new Box APIs to the SDK at a much faster pace (in a matter of days). This means you can leverage the most up-to-date features in your applications without delay.\\n3. Embedded Documentation: We understand that easy access to information is crucial for developers. With our new approach, we have included comprehensive documentation for all objects and parameters directly in the source code of the SDK. This means you no longer need to look up this information on the developer portal, saving you time and streamlining your development process.\\n4. Enhanced Convenience Methods: Our commitment to enhancing your development experience continues with the introduction of convenience methods. These methods cover various aspects such as chunk uploads, classification, and much more.\\n5. Seamless Start: The new SDKs integrate essential functionalities like authentication, automatic retries with exponential backoff, exception handling, request cancellation, and type checking, enabling you to focus solely on your application's business logic.\\n\\nEmbrace the new generation of Box SDKs and unlock the full potential of the Box Content Cloud.\\n\\n# Supported versions\\n\\nTo enhance developer experience, we have introduced the new generated codebase through the `box_sdk_gen` package.\\nThe `box_sdk_gen` package is available in two major supported versions: v4 and v10.\\n\\n## Version v4\\n\\nIn v4 of the Box Python SDK, we are introducing a version that consolidates both the manually written package (`boxsdk`)\\nand the new generated package (`box_sdk_gen`). This allows developers to use both packages simultaneously within a single project.\\n\\nThe codebase for v4 of the Box Python SDK is currently available on the [combined-sdk](https://github.com/box/box-python-sdk/tree/combined-sdk) branch.\\nMigration guide which would help with migration from `boxsdk` to `box_sdk_gen` can be found [here](./migration-guides/from-boxsdk-to-box_sdk_gen.md).\\n\\nVersion v4 is intended for:\\n- Existing developers of the Box Python SDK v3 who want to access new API features while keeping their current codebase largely unchanged.\\n- Existing developers who are in the process of migrating to `box_sdk_gen`, but do not want to move all their code to the new package immediately.\\n\\n## Version v10\\n\\nStarting with v10, the SDK is built entirely on the generated `box_sdk_gen` package, which fully and exclusively replaces the old `boxsdk` package.\\nThe codebase for v10 of the Box Python SDK is currently available on the [sdk-gen](https://github.com/box/box-python-sdk/tree/sdk-gen) branch.\\n\\nVersion v10 is intended for:\\n- New users of the Box Python SDK.\\n- Developers already working with the generated Box Python SDK previously available under the [Box Python SDK Gen repository](https://github.com/box/box-python-sdk-gen).\\n\\n## Deprecation of `boxsdk` package\\n\\nThe `boxsdk` package will be marked as deprecated, will receive only bug fixes and security patches, and reach end of support in 2027.\\nAll new features and support for new Box APIs will be provided exclusively in the `box_sdk_gen` package.\\n\\n## Which Version Should I Use?\\n\\n| Scenario                                                                                                                     | Recommended Version                                                      | Example `pip install`       |\\n|------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------|-----------------------------|\\n| Creating a new application                                                                                                   | Use [v10](https://github.com/box/box-python-sdk/tree/sdk-gen)            | `pip install \\\"boxsdk>=10\\\"`  |\\n| App using [box-sdk-gen](https://pypi.org/project/box-sdk-gen/) artifact                                                      | Migrate to [v10](https://github.com/box/box-python-sdk/tree/sdk-gen)     | `pip install \\\"boxsdk>=10\\\"`  |\\n| App using both [box-sdk-gen](https://pypi.org/project/box-sdk-gen/) and [boxsdk](https://pypi.org/project/boxsdk/) artifacts | Upgrade to [v4](https://github.com/box/box-python-sdk/tree/combined-sdk) | `pip install \\\"boxsdk~=4.0\\\"` |\\n| App using v3 of [boxsdk](https://pypi.org/project/boxsdk/) artifact                                                          | Upgrade to [v4](https://github.com/box/box-python-sdk/tree/combined-sdk) | `pip install \\\"boxsdk~=4.0\\\"` |\\n\\nFor full guidance on SDK versioning, see the [Box SDK Versioning Guide](https://developer.box.com/guides/tooling/sdks/sdk-versioning/).\\n\\n# Installing\\n\\nTo install Box Python SDK v4 version that consolidates both the manual package (`boxsdk`)\\nand the new generated package (`box_sdk_gen`) run the command:\\n\\n``` console\\npip install boxsdk~=4.0\\n```\\n\\nTo install also extra dependencies required for JWT authentication, use command:\\n``` console\\npip install \\\"boxsdk[jwt]~=4.0\\\"\\n```\\n\\nSupported Python versions are Python 3.8 and above.\\n\\n# Getting Started\\n\\nTo get started with the SDK, get a Developer Token from the Configuration page of your app in the [Box Developer Console](https://app.box.com/developers/console).\\nDeveloper Tokens are short-lived and expire after 60 minutes, which is good for testing but not for production use.\\nTo learn about other authentication methods, see the [Authentication](#authentication) section below.\\n\\nThe examples below demonstrate how to authenticate with Developer Token and print names of all items inside a root folder.\\n\\n## With box_sdk_gen package (recommended)\\n\\nThe SDK provides an `BoxDeveloperTokenAuth` class, which allows you to authenticate using your Developer Token.\\nUse instance of `BoxDeveloperTokenAuth` to initialize BoxClient object. Using `BoxClient` object you can access managers,\\nwhich allow you to perform some operations on your Box account.\\n\\n``` python\\nfrom box_sdk_gen import BoxClient, BoxDeveloperTokenAuth\\n\\ndef main(token: str):\\n    auth: BoxDeveloperTokenAuth = BoxDeveloperTokenAuth(token=token)\\n    client: BoxClient = BoxClient(auth=auth)\\n    for item in client.folders.get_folder_items('0').entries:\\n        print(item.name)\\n\\nif __name__ == '__main__':\\n    main('INSERT YOUR DEVELOPER TOKEN HERE')\\n```\\n\\n## With boxsdk package (deprecated)\\n\\n``` python\\nfrom boxsdk import OAuth2, Client\\n\\ndef main():\\n  oauth = OAuth2(\\n      client_id='YOUR_CLIENT_ID',\\n      client_secret='YOUR_CLIENT_SECRET',\\n      access_token='YOUR_DEVELOPER_TOKEN'\\n  )\\n  client = Client(oauth)\\n  for item in client.folder(folder_id='0').get_items():\\n      print(item.name)\\n\\nif __name__ == '__main__':\\n    main()\\n```\\n\\n# Authentication\\n\\nBoth the `box_sdk_gen` and `boxsdk` packages support multiple authentication methods, including\\nDeveloper Token, OAuth 2.0, Client Credentials Grant, and JSON Web Token (JWT).\\n\\nYou can find detailed instructions and example code for each authentication method in the following documentation:\\n- [Authentication for the `box_sdk_gen` package](./docs/box_sdk_gen/authentication.md)\\n- [Authentication for the `boxsdk` package](./docs/boxsdk/usage/authentication.md)\\n\\n# Using both box_sdk_gen and boxsdk packages simultaneously\\n\\nWith v4 of the Box Python SDK, you can use both the `box_sdk_gen` and `boxsdk` packages in the same project.\\nThis allows you to gradually migrate your codebase to the new generated package while still using the manual package for existing functionality.\\n\\n``` python\\nfrom boxsdk import JWTAuth, Client\\nfrom box_sdk_gen import BoxJWTAuth, JWTConfig, BoxClient \\n\\ndef main():\\n  auth = JWTAuth.from_settings_file('/path/to/settings.json')\\n  legacy_client = Client(auth)\\n  \\n  jwt_config = JWTConfig.from_config_file(config_file_path='/path/to/settings.json')\\n  auth = BoxJWTAuth(config=jwt_config)\\n  new_client = BoxClient(auth=auth)\\n  \\n  folder = legacy_client.folder(folder_id='0').create_subfolder('My Subfolder')\\n  updated_folder = new_client.folders.update_folder(folder_id=folder.id, name='My Updated Subfolder')\\n  print(f'Created folder with ID {folder.id} has been updated to {updated_folder.name}')\\n\\nif __name__ == '__main__':\\n    main()\\n```\\n\\n# Documentation\\n\\nFull documentation of the available functionality, along with example code can be found:\\n- for the `box_sdk_gen` package, is available [here](./docs/boxsdk/usage).\\n- for the `boxsdk` package can be found [here](./docs/box_sdk_gen/README.md).\\n\\nYou can also see the [API Reference](https://developer.box.com/reference/) for additional information.\\n\\n# Migration guides\\n\\nMigration guides which help you to migrate to supported major SDK versions can be found [here](./migration-guides).\\n\\n# Versioning\\n\\nWe use a modified version of [Semantic Versioning](https://semver.org/) for all changes. See [version strategy](VERSIONS.md) for details which is effective from 30 July 2022.\\n\\nA current release is on the leading edge of our SDK development, and is intended for customers who are in active development and want the latest and greatest features.  \\nInstead of stating a release date for a new feature, we set a fixed minor or patch release cadence of maximum 2-3 months (while we may release more often).\\nAt the same time, there is no schedule for major or breaking release. Instead, we will communicate one quarter in advance the upcoming breaking change to allow customers to plan for the upgrade.\\n\\nWe always recommend that all users run the latest available minor release for whatever major version is in use.\\nWe highly recommend upgrading to the latest SDK major release at the earliest convenient time and before the EOL date.\\n\\n## Version schedule\\n\\n| Version | Supported Environments | State     | First Release | EOL/Terminated         |\\n|---------|------------------------|-----------|---------------|------------------------|\\n| 10      | Python 3.8+            | Supported | 17 Sep 2025   | TBD                    |\\n| 4       | Python 3.8+            | Supported | 23 Oct 2025   | 2027 or v5 is released |\\n| 3       | Python 3.6+            | EOL       | 17 Jan 2022   | 23 Oct 2025            |\\n| 2       |                        | EOL       | 01 Nov 2018   | 17 Jan 2022            |\\n| 1       |                        | EOL       | 10 Feb 2015   | 01 Nov 2018            |\\n\\n# Contributing\\n\\nSee [CONTRIBUTING.md](./CONTRIBUTING.md).\\n\\n# FIPS 140-2 Compliance\\n\\nThe Python SDK allows the use of FIPS 140-2 validated SSL libraries, such as OpenSSL 3.0.\\nHowever, some actions are required to enable this functionality.\\n\\nCurrently, the latest distributions of Python default to OpenSSL v1.1.1, which is not FIPS compliant.\\nTherefore, if you want to use OpenSSL 3.0 in your network communication,\\nyou need to ensure that Python uses a custom SSL library.\\nOne way to achieve this is by creating a custom Python distribution with the ssl module replaced.\\n\\nIf you are using JWT for authentication, it is also necessary to ensure that the cryptography library,\\nwhich is one of the extra dependencies for JWT, uses OpenSSL 3.0.\\nTo enable FIPS mode for the `cryptography` library, you need to install a FIPS-compliant version of OpenSSL\\nduring the installation process of cryptography using the `pip` command.\\n\\n# Questions, Bugs, and Feature Requests?\\n\\nNeed to contact us directly? [Browse the issues tickets](https://github.com/box/box-python-sdk/issues)! Or, if that\\ndoesn't work, [file a new one](https://github.com/box/box-python-sdk/issues/new) and we will get\\nback to you. If you have general questions about the Box API, you can post to the [Box Developer Forum](https://community.box.com/box-platform-5).\\n\\n# Copyright and License\\n\\nCopyright 2025 Box, Inc. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \\\"License\\\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Official Box Python SDK\", \"latest_version\": \"4.0.0\", \"weekly_downloads\": 783437, \"description_cleaned\": \"Box Python SDK v4\\nIntroduction\\nSupported versions\\nVersion v4\\nVersion v10\\nDeprecation of\\nboxsdk\\npackage\\nWhich Version Should I Use?\\nInstalling\\nGetting Started\\nWith box_sdk_gen package (recommended)\\nWith boxsdk package (deprecated)\\nAuthentication\\nUsing both box_sdk_gen and boxsdk packages simultaneously\\nDocumentation\\nMigration guides\\nVersioning\\nVersion schedule\\nContributing\\nFIPS 140-2 Compliance\\nQuestions, Bugs, and Feature Requests?\\nCopyright and License\\nIntroduction\\nWe are excited to introduce the v4 major release of the Box Python SDK,\\ndesigned to elevate the developer experience and streamline your integration with the Box Content Cloud.\\nWith this SDK version, alongside the existing\\nboxsdk\\npackage, we\\u2019re introducing a new\\nbox_sdk_gen\\npackage, which gives you access to:\\nFull API Support: The new generation of Box SDKs empowers developers with complete coverage of the Box API ecosystem. You can now access all the latest features and functionalities offered by Box, allowing you to build even more sophisticated and feature-rich applications.\\nRapid API Updates: Say goodbye to waiting for new Box APIs to be incorporated into the SDK. With our new auto-generation development approach, we can now add new Box APIs to the SDK at a much faster pace (in a matter of days). This means you can leverage the most up-to-date features in your applications without delay.\\nEmbedded Documentation: We understand that easy access to information is crucial for developers. With our new approach, we have included comprehensive documentation for all objects and parameters directly in the source code of the SDK. This means you no longer need to look up this information on the developer portal, saving you time and streamlining your development process.\\nEnhanced Convenience Methods: Our commitment to enhancing your development experience continues with the introduction of convenience methods. These methods cover various aspects such as chunk uploads, classification, and much more.\\nSeamless Start: The new SDKs integrate essential functionalities like authentication, automatic retries with exponential backoff, exception handling, request cancellation, and type checking, enabling you to focus solely on your application's business logic.\\nEmbrace the new generation of Box SDKs and unlock the full potential of the Box Content Cloud.\\nSupported versions\\nTo enhance developer experience, we have introduced the new generated codebase through the\\nbox_sdk_gen\\npackage.\\nThe\\nbox_sdk_gen\\npackage is available in two major supported versions: v4 and v10.\\nVersion v4\\nIn v4 of the Box Python SDK, we are introducing a version that consolidates both the manually written package (\\nboxsdk\\n)\\nand the new generated package (\\nbox_sdk_gen\\n). This allows developers to use both packages simultaneously within a single project.\\nThe codebase for v4 of the Box Python SDK is currently available on the\\ncombined-sdk\\nbranch.\\nMigration guide which would help with migration from\\nboxsdk\\nto\\nbox_sdk_gen\\ncan be found\\nhere\\n.\\nVersion v4 is intended for:\\nExisting developers of the Box Python SDK v3 who want to access new API features while keeping their current codebase largely unchanged.\\nExisting developers who are in the process of migrating to\\nbox_sdk_gen\\n, but do not want to move all their code to the new package immediately.\\nVersion v10\\nStarting with v10, the SDK is built entirely on the generated\\nbox_sdk_gen\\npackage, which fully and exclusively replaces the old\\nboxsdk\\npackage.\\nThe codebase for v10 of the Box Python SDK is currently available on the\\nsdk-gen\\nbranch.\\nVersion v10 is intended for:\\nNew users of the Box Python SDK.\\nDevelopers already working with the generated Box Python SDK previously available under the\\nBox Python SDK Gen repository\\n.\\nDeprecation of\\nboxsdk\\npackage\\nThe\\nboxsdk\\npackage will be marked as deprecated, will receive only bug fixes and security patches, and reach end of support in 2027.\\nAll new features and support for new Box APIs will be provided exclusively in the\\nbox_sdk_gen\\npackage.\\nWhich Version Should I Use?\\nScenario\\nRecommended Version\\nExample\\npip install\\nCreating a new application\\nUse\\nv10\\npip install \\\"boxsdk>=10\\\"\\nApp using\\nbox-sdk-gen\\nartifact\\nMigrate to\\nv10\\npip install \\\"boxsdk>=10\\\"\\nApp using both\\nbox-sdk-gen\\nand\\nboxsdk\\nartifacts\\nUpgrade to\\nv4\\npip install \\\"boxsdk~=4.0\\\"\\nApp using v3 of\\nboxsdk\\nartifact\\nUpgrade to\\nv4\\npip install \\\"boxsdk~=4.0\\\"\\nFor full guidance on SDK versioning, see the\\nBox SDK Versioning Guide\\n.\\nInstalling\\nTo install Box Python SDK v4 version that consolidates both the manual package (\\nboxsdk\\n)\\nand the new generated package (\\nbox_sdk_gen\\n) run the command:\\npip install boxsdk~=4.0\\nTo install also extra dependencies required for JWT authentication, use command:\\npip install \\\"boxsdk[jwt]~=4.0\\\"\\nSupported Python versions are Python 3.8 and above.\\nGetting Started\\nTo get started with the SDK, get a Developer Token from the Configuration page of your app in the\\nBox Developer Console\\n.\\nDeveloper Tokens are short-lived and expire after 60 minutes, which is good for testing but not for production use.\\nTo learn about other authentication methods, see the\\nAuthentication\\nsection below.\\nThe examples below demonstrate how to authenticate with Developer Token and print names of all items inside a root folder.\\nWith box_sdk_gen package (recommended)\\nThe SDK provides an\\nBoxDeveloperTokenAuth\\nclass, which allows you to authenticate using your Developer Token.\\nUse instance of\\nBoxDeveloperTokenAuth\\nto initialize BoxClient object. Using\\nBoxClient\\nobject you can access managers,\\nwhich allow you to perform some operations on your Box account.\\nfrom\\nbox_sdk_gen\\nimport\\nBoxClient\\n,\\nBoxDeveloperTokenAuth\\ndef\\nmain\\n(\\ntoken\\n:\\nstr\\n):\\nauth\\n:\\nBoxDeveloperTokenAuth\\n=\\nBoxDeveloperTokenAuth\\n(\\ntoken\\n=\\ntoken\\n)\\nclient\\n:\\nBoxClient\\n=\\nBoxClient\\n(\\nauth\\n=\\nauth\\n)\\nfor\\nitem\\nin\\nclient\\n.\\nfolders\\n.\\nget_folder_items\\n(\\n'0'\\n)\\n.\\nentries\\n:\\nprint\\n(\\nitem\\n.\\nname\\n)\\nif\\n__name__\\n==\\n'__main__'\\n:\\nmain\\n(\\n'INSERT YOUR DEVELOPER TOKEN HERE'\\n)\\nWith boxsdk package (deprecated)\\nfrom\\nboxsdk\\nimport\\nOAuth2\\n,\\nClient\\ndef\\nmain\\n():\\noauth\\n=\\nOAuth2\\n(\\nclient_id\\n=\\n'YOUR_CLIENT_ID'\\n,\\nclient_secret\\n=\\n'YOUR_CLIENT_SECRET'\\n,\\naccess_token\\n=\\n'YOUR_DEVELOPER_TOKEN'\\n)\\nclient\\n=\\nClient\\n(\\noauth\\n)\\nfor\\nitem\\nin\\nclient\\n.\\nfolder\\n(\\nfolder_id\\n=\\n'0'\\n)\\n.\\nget_items\\n():\\nprint\\n(\\nitem\\n.\\nname\\n)\\nif\\n__name__\\n==\\n'__main__'\\n:\\nmain\\n()\\nAuthentication\\nBoth the\\nbox_sdk_gen\\nand\\nboxsdk\\npackages support multiple authentication methods, including\\nDeveloper Token, OAuth 2.0, Client Credentials Grant, and JSON Web Token (JWT).\\nYou can find detailed instructions and example code for each authentication method in the following documentation:\\nAuthentication for the\\nbox_sdk_gen\\npackage\\nAuthentication for the\\nboxsdk\\npackage\\nUsing both box_sdk_gen and boxsdk packages simultaneously\\nWith v4 of the Box Python SDK, you can use both the\\nbox_sdk_gen\\nand\\nboxsdk\\npackages in the same project.\\nThis allows you to gradually migrate your codebase to the new generated package while still using the manual package for existing functionality.\\nfrom\\nboxsdk\\nimport\\nJWTAuth\\n,\\nClient\\nfrom\\nbox_sdk_gen\\nimport\\nBoxJWTAuth\\n,\\nJWTConfig\\n,\\nBoxClient\\ndef\\nmain\\n():\\nauth\\n=\\nJWTAuth\\n.\\nfrom_settings_file\\n(\\n'/path/to/settings.json'\\n)\\nlegacy_client\\n=\\nClient\\n(\\nauth\\n)\\njwt_config\\n=\\nJWTConfig\\n.\\nfrom_config_file\\n(\\nconfig_file_path\\n=\\n'/path/to/settings.json'\\n)\\nauth\\n=\\nBoxJWTAuth\\n(\\nconfig\\n=\\njwt_config\\n)\\nnew_client\\n=\\nBoxClient\\n(\\nauth\\n=\\nauth\\n)\\nfolder\\n=\\nlegacy_client\\n.\\nfolder\\n(\\nfolder_id\\n=\\n'0'\\n)\\n.\\ncreate_subfolder\\n(\\n'My Subfolder'\\n)\\nupdated_folder\\n=\\nnew_client\\n.\\nfolders\\n.\\nupdate_folder\\n(\\nfolder_id\\n=\\nfolder\\n.\\nid\\n,\\nname\\n=\\n'My Updated Subfolder'\\n)\\nprint\\n(\\nf\\n'Created folder with ID\\n{\\nfolder\\n.\\nid\\n}\\nhas been updated to\\n{\\nupdated_folder\\n.\\nname\\n}\\n'\\n)\\nif\\n__name__\\n==\\n'__main__'\\n:\\nmain\\n()\\nDocumentation\\nFull documentation of the available functionality, along with example code can be found:\\nfor the\\nbox_sdk_gen\\npackage, is available\\nhere\\n.\\nfor the\\nboxsdk\\npackage can be found\\nhere\\n.\\nYou can also see the\\nAPI Reference\\nfor additional information.\\nMigration guides\\nMigration guides which help you to migrate to supported major SDK versions can be found\\nhere\\n.\\nVersioning\\nWe use a modified version of\\nSemantic Versioning\\nfor all changes. See\\nversion strategy\\nfor details which is effective from 30 July 2022.\\nA current release is on the leading edge of our SDK development, and is intended for customers who are in active development and want the latest and greatest features.\\nInstead of stating a release date for a new feature, we set a fixed minor or patch release cadence of maximum 2-3 months (while we may release more often).\\nAt the same time, there is no schedule for major or breaking release. Instead, we will communicate one quarter in advance the upcoming breaking change to allow customers to plan for the upgrade.\\nWe always recommend that all users run the latest available minor release for whatever major version is in use.\\nWe highly recommend upgrading to the latest SDK major release at the earliest convenient time and before the EOL date.\\nVersion schedule\\nVersion\\nSupported Environments\\nState\\nFirst Release\\nEOL/Terminated\\n10\\nPython 3.8+\\nSupported\\n17 Sep 2025\\nTBD\\n4\\nPython 3.8+\\nSupported\\n23 Oct 2025\\n2027 or v5 is released\\n3\\nPython 3.6+\\nEOL\\n17 Jan 2022\\n23 Oct 2025\\n2\\nEOL\\n01 Nov 2018\\n17 Jan 2022\\n1\\nEOL\\n10 Feb 2015\\n01 Nov 2018\\nContributing\\nSee\\nCONTRIBUTING.md\\n.\\nFIPS 140-2 Compliance\\nThe Python SDK allows the use of FIPS 140-2 validated SSL libraries, such as OpenSSL 3.0.\\nHowever, some actions are required to enable this functionality.\\nCurrently, the latest distributions of Python default to OpenSSL v1.1.1, which is not FIPS compliant.\\nTherefore, if you want to use OpenSSL 3.0 in your network communication,\\nyou need to ensure that Python uses a custom SSL library.\\nOne way to achieve this is by creating a custom Python distribution with the ssl module replaced.\\nIf you are using JWT for authentication, it is also necessary to ensure that the cryptography library,\\nwhich is one of the extra dependencies for JWT, uses OpenSSL 3.0.\\nTo enable FIPS mode for the\\ncryptography\\nlibrary, you need to install a FIPS-compliant version of OpenSSL\\nduring the installation process of cryptography using the\\npip\\ncommand.\\nQuestions, Bugs, and Feature Requests?\\nNeed to contact us directly?\\nBrowse the issues tickets\\n! Or, if that\\ndoesn't work,\\nfile a new one\\nand we will get\\nback to you. If you have general questions about the Box API, you can post to the\\nBox Developer Forum\\n.\\nCopyright and License\\nCopyright 2025 Box, Inc. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \\\"License\\\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\"}, {\"name\": \"cog\", \"description\": \"# Cog: Containers for machine learning\\n\\nCog is an open-source tool that lets you package machine learning models in a standard, production-ready container.\\n\\nYou can deploy your packaged model to your own infrastructure, or to [Replicate](https://replicate.com/).\\n\\n## Highlights\\n\\n- \\ud83d\\udce6 **Docker containers without the pain.** Writing your own `Dockerfile` can be a bewildering process. With Cog, you define your environment with a [simple configuration file](#how-it-works) and it generates a Docker image with all the best practices: Nvidia base images, efficient caching of dependencies, installing specific Python versions, sensible environment variable defaults, and so on.\\n\\n- \\ud83e\\udd2c\\ufe0f **No more CUDA hell.** Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will set it all up correctly for you.\\n\\n- \\u2705 **Define the inputs and outputs for your model with standard Python.** Then, Cog generates an OpenAPI schema and validates the inputs and outputs with Pydantic.\\n\\n- \\ud83c\\udf81 **Automatic HTTP prediction server**: Your model's types are used to dynamically generate a RESTful HTTP API using [FastAPI](https://fastapi.tiangolo.com/).\\n\\n- \\ud83e\\udd5e **Automatic queue worker.** Long-running deep learning models or batch processing is best architected with a queue. Cog models do this out of the box. Redis is currently supported, with more in the pipeline.\\n\\n- \\u2601\\ufe0f **Cloud storage.** Files can be read and written directly to Amazon S3 and Google Cloud Storage. (Coming soon.)\\n\\n- \\ud83d\\ude80 **Ready for production.** Deploy your model anywhere that Docker images run. Your own infrastructure, or [Replicate](https://replicate.com).\\n\\n## How it works\\n\\nDefine the Docker environment your model runs in with `cog.yaml`:\\n\\n```yaml\\nbuild:\\n  gpu: true\\n  system_packages:\\n    - \\\"libgl1-mesa-glx\\\"\\n    - \\\"libglib2.0-0\\\"\\n  python_version: \\\"3.12\\\"\\n  python_packages:\\n    - \\\"torch==2.3\\\"\\npredict: \\\"predict.py:Predictor\\\"\\n```\\n\\nDefine how predictions are run on your model with `predict.py`:\\n\\n```python\\nfrom cog import BasePredictor, Input, Path\\nimport torch\\n\\nclass Predictor(BasePredictor):\\n    def setup(self):\\n        \\\"\\\"\\\"Load the model into memory to make running multiple predictions efficient\\\"\\\"\\\"\\n        self.model = torch.load(\\\"./weights.pth\\\")\\n\\n    # The arguments and types the model takes as input\\n    def predict(self,\\n          image: Path = Input(description=\\\"Grayscale input image\\\")\\n    ) -> Path:\\n        \\\"\\\"\\\"Run a single prediction on the model\\\"\\\"\\\"\\n        processed_image = preprocess(image)\\n        output = self.model(processed_image)\\n        return postprocess(output)\\n```\\n\\nIn the above we accept a path to the image as an input, and return a path to our transformed image after running it through our model.\\n\\nNow, you can run predictions on this model:\\n\\n```console\\n$ cog predict -i image=@input.jpg\\n--> Building Docker image...\\n--> Running Prediction...\\n--> Output written to output.jpg\\n```\\n\\nOr, build a Docker image for deployment:\\n\\n```console\\n$ cog build -t my-colorization-model\\n--> Building Docker image...\\n--> Built my-colorization-model:latest\\n\\n$ docker run -d -p 5000:5000 --gpus all my-colorization-model\\n\\n$ curl http://localhost:5000/predictions -X POST \\\\\\n    -H 'Content-Type: application/json' \\\\\\n    -d '{\\\"input\\\": {\\\"image\\\": \\\"https://.../input.jpg\\\"}}'\\n```\\n\\nOr, combine build and run via the `serve` command:\\n\\n```console\\n$ cog serve -p 8080\\n\\n$ curl http://localhost:8080/predictions -X POST \\\\\\n    -H 'Content-Type: application/json' \\\\\\n    -d '{\\\"input\\\": {\\\"image\\\": \\\"https://.../input.jpg\\\"}}'\\n```\\n\\n<!-- NOTE (bfirsh): Development environment instructions intentionally left out of readme for now, so as not to confuse the \\\"ship a model to production\\\" message.\\n\\nIn development, you can also run arbitrary commands inside the Docker environment:\\n\\n```console\\n$ cog run python train.py\\n...\\n```\\n\\nOr, [spin up a Jupyter notebook](docs/notebooks.md):\\n\\n```console\\n$ cog run -p 8888 jupyter notebook --allow-root --ip=0.0.0.0\\n```\\n-->\\n\\n## Why are we building this?\\n\\nIt's really hard for researchers to ship machine learning models to production.\\n\\nPart of the solution is Docker, but it is so complex to get it to work: Dockerfiles, pre-/post-processing, Flask servers, CUDA versions. More often than not the researcher has to sit down with an engineer to get the damn thing deployed.\\n\\n[Andreas](https://github.com/andreasjansson) and [Ben](https://github.com/bfirsh) created Cog. Andreas used to work at Spotify, where he built tools for building and deploying ML models with Docker. Ben worked at Docker, where he created [Docker Compose](https://github.com/docker/compose).\\n\\nWe realized that, in addition to Spotify, other companies were also using Docker to build and deploy machine learning models. [Uber](https://eng.uber.com/michelangelo-pyml/) and others have built similar systems. So, we're making an open source version so other people can do this too.\\n\\nHit us up if you're interested in using it or want to collaborate with us. [We're on Discord](https://discord.gg/replicate) or email us at [team@replicate.com](mailto:team@replicate.com).\\n\\n## Prerequisites\\n\\n- **macOS, Linux or Windows 11**. Cog works on macOS, Linux and Windows 11 with [WSL 2](docs/wsl2/wsl2.md)\\n- **Docker**. Cog uses Docker to create a container for your model. You'll need to [install Docker](https://docs.docker.com/get-docker/) before you can run Cog. If you install Docker Engine instead of Docker Desktop, you will need to [install Buildx](https://docs.docker.com/build/architecture/#buildx) as well.\\n\\n## Install\\n\\nIf you're using macOS, you can install Cog using Homebrew:\\n\\n```console\\nbrew install cog\\n```\\n\\nYou can also download and install the latest release using our \\n[install script](https://cog.run/install):\\n\\n```sh\\n# fish shell\\nsh (curl -fsSL https://cog.run/install.sh | psub)\\n\\n# bash, zsh, and other shells\\nsh <(curl -fsSL https://cog.run/install.sh)\\n\\n# download with wget and run in a separate command\\nwget -qO- https://cog.run/install.sh\\nsh ./install.sh\\n```\\n\\nYou can manually install the latest release of Cog directly from GitHub \\nby running the following commands in a terminal:\\n\\n```console\\nsudo curl -o /usr/local/bin/cog -L \\\"https://github.com/replicate/cog/releases/latest/download/cog_$(uname -s)_$(uname -m)\\\"\\nsudo chmod +x /usr/local/bin/cog\\n```\\n\\nAlternatively, you can build Cog from source and install it with these commands:\\n\\n```console\\nmake\\nsudo make install\\n```\\n\\nOr if you are on docker:\\n\\n```\\nRUN sh -c \\\"INSTALL_DIR=\\\\\\\"/usr/local/bin\\\\\\\" SUDO=\\\\\\\"\\\\\\\" $(curl -fsSL https://cog.run/install.sh)\\\"\\n```\\n\\n## Upgrade\\n\\nIf you're using macOS and you previously installed Cog with Homebrew, run the following:\\n\\n```console\\nbrew upgrade cog\\n```\\n\\nOtherwise, you can upgrade to the latest version by running the same commands you used to install it.\\n\\n## Next steps\\n\\n- [Get started with an example model](docs/getting-started.md)\\n- [Get started with your own model](docs/getting-started-own-model.md)\\n- [Using Cog with notebooks](docs/notebooks.md)\\n- [Using Cog with Windows 11](docs/wsl2/wsl2.md)\\n- [Take a look at some examples of using Cog](https://github.com/replicate/cog-examples)\\n- [Deploy models with Cog](docs/deploy.md)\\n- [`cog.yaml` reference](docs/yaml.md) to learn how to define your model's environment\\n- [Prediction interface reference](docs/python.md) to learn how the `Predictor` interface works\\n- [Training interface reference](docs/training.md) to learn how to add a fine-tuning API to your model\\n- [HTTP API reference](docs/http.md) to learn how to use the HTTP API that models serve\\n\\n## Need help?\\n\\n[Join us in #cog on Discord.](https://discord.gg/replicate)\\n\\n## Contributors \\u2728\\n\\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\\n\\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\\n<!-- prettier-ignore-start -->\\n<!-- markdownlint-disable -->\\n<table>\\n  <tbody>\\n    <tr>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://fir.sh/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/40906?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Ben Firshman\\\"/><br /><sub><b>Ben Firshman</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=bfirsh\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=bfirsh\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://replicate.ai/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/713993?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Andreas Jansson\\\"/><br /><sub><b>Andreas Jansson</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=andreasjansson\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=andreasjansson\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"#maintenance-andreasjansson\\\" title=\\\"Maintenance\\\">\\ud83d\\udea7</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://zeke.sikelianos.com/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/2289?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Zeke Sikelianos\\\"/><br /><sub><b>Zeke Sikelianos</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=zeke\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=zeke\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"#tool-zeke\\\" title=\\\"Tools\\\">\\ud83d\\udd27</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://rory.bio/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/9436784?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Rory Byrne\\\"/><br /><sub><b>Rory Byrne</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=synek\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=synek\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"https://github.com/replicate/cog/commits?author=synek\\\" title=\\\"Tests\\\">\\u26a0\\ufe0f</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/hangtwenty\\\"><img src=\\\"https://avatars.githubusercontent.com/u/2420688?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Michael Floering\\\"/><br /><sub><b>Michael Floering</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=hangtwenty\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=hangtwenty\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"#ideas-hangtwenty\\\" title=\\\"Ideas, Planning, & Feedback\\\">\\ud83e\\udd14</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://bencevans.io/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/638535?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Ben Evans\\\"/><br /><sub><b>Ben Evans</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=bencevans\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://shashank.pw/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/778870?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"shashank agarwal\\\"/><br /><sub><b>shashank agarwal</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=imshashank\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=imshashank\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n    </tr>\\n    <tr>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://victorxlr.me/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/22397950?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"VictorXLR\\\"/><br /><sub><b>VictorXLR</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=VictorXLR\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=VictorXLR\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"https://github.com/replicate/cog/commits?author=VictorXLR\\\" title=\\\"Tests\\\">\\u26a0\\ufe0f</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://annahung31.github.io/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/39179888?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"hung anna\\\"/><br /><sub><b>hung anna</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aannahung31\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://notes.variogr.am/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/76612?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Brian Whitman\\\"/><br /><sub><b>Brian Whitman</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Abwhitman\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/JimothyJohn\\\"><img src=\\\"https://avatars.githubusercontent.com/u/24216724?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"JimothyJohn\\\"/><br /><sub><b>JimothyJohn</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3AJimothyJohn\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/ericguizzo\\\"><img src=\\\"https://avatars.githubusercontent.com/u/26746670?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"ericguizzo\\\"/><br /><sub><b>ericguizzo</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aericguizzo\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://www.dominicbaggott.com\\\"><img src=\\\"https://avatars.githubusercontent.com/u/74812?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Dominic Baggott\\\"/><br /><sub><b>Dominic Baggott</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=evilstreak\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=evilstreak\\\" title=\\\"Tests\\\">\\u26a0\\ufe0f</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/dashstander\\\"><img src=\\\"https://avatars.githubusercontent.com/u/7449128?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Dashiell Stander\\\"/><br /><sub><b>Dashiell Stander</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Adashstander\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a> <a href=\\\"https://github.com/replicate/cog/commits?author=dashstander\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=dashstander\\\" title=\\\"Tests\\\">\\u26a0\\ufe0f</a></td>\\n    </tr>\\n    <tr>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/Hurricane-eye\\\"><img src=\\\"https://avatars.githubusercontent.com/u/31437546?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Shuwei Liang\\\"/><br /><sub><b>Shuwei Liang</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3AHurricane-eye\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a> <a href=\\\"#question-Hurricane-eye\\\" title=\\\"Answering Questions\\\">\\ud83d\\udcac</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/ericallam\\\"><img src=\\\"https://avatars.githubusercontent.com/u/534?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Eric Allam\\\"/><br /><sub><b>Eric Allam</b></sub></a><br /><a href=\\\"#ideas-ericallam\\\" title=\\\"Ideas, Planning, & Feedback\\\">\\ud83e\\udd14</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://perdomo.me\\\"><img src=\\\"https://avatars.githubusercontent.com/u/178474?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Iv\\u00e1n Perdomo\\\"/><br /><sub><b>Iv\\u00e1n Perdomo</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aiperdomo\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://charlesfrye.github.io\\\"><img src=\\\"https://avatars.githubusercontent.com/u/10442975?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Charles Frye\\\"/><br /><sub><b>Charles Frye</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=charlesfrye\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/phamquiluan\\\"><img src=\\\"https://avatars.githubusercontent.com/u/24642166?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Luan Pham\\\"/><br /><sub><b>Luan Pham</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aphamquiluan\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a> <a href=\\\"https://github.com/replicate/cog/commits?author=phamquiluan\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/TommyDew42\\\"><img src=\\\"https://avatars.githubusercontent.com/u/46992350?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"TommyDew\\\"/><br /><sub><b>TommyDew</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=TommyDew42\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://m4ke.org\\\"><img src=\\\"https://avatars.githubusercontent.com/u/27?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Jesse Andrews\\\"/><br /><sub><b>Jesse Andrews</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=anotherjesse\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=anotherjesse\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"https://github.com/replicate/cog/commits?author=anotherjesse\\\" title=\\\"Tests\\\">\\u26a0\\ufe0f</a></td>\\n    </tr>\\n    <tr>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://whiteink.com\\\"><img src=\\\"https://avatars.githubusercontent.com/u/3602?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Nick Stenning\\\"/><br /><sub><b>Nick Stenning</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=nickstenning\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=nickstenning\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"#design-nickstenning\\\" title=\\\"Design\\\">\\ud83c\\udfa8</a> <a href=\\\"#infra-nickstenning\\\" title=\\\"Infrastructure (Hosting, Build-Tools, etc)\\\">\\ud83d\\ude87</a> <a href=\\\"https://github.com/replicate/cog/commits?author=nickstenning\\\" title=\\\"Tests\\\">\\u26a0\\ufe0f</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://merrell.io/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/14996837?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Justin Merrell\\\"/><br /><sub><b>Justin Merrell</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=justinmerrell\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/ruriky\\\"><img src=\\\"https://avatars.githubusercontent.com/u/19946546?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Rurik Yl\\u00e4-Onnenvuori\\\"/><br /><sub><b>Rurik Yl\\u00e4-Onnenvuori</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aruriky\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://www.youka.club/\\\"><img src=\\\"https://avatars.githubusercontent.com/u/59315275?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Youka\\\"/><br /><sub><b>Youka</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Ayoukaclub\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/afiaka87\\\"><img src=\\\"https://avatars.githubusercontent.com/u/3994972?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Clay Mullis\\\"/><br /><sub><b>Clay Mullis</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=afiaka87\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/mattt\\\"><img src=\\\"https://avatars.githubusercontent.com/u/7659?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Mattt\\\"/><br /><sub><b>Mattt</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=mattt\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"https://github.com/replicate/cog/commits?author=mattt\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"#infra-mattt\\\" title=\\\"Infrastructure (Hosting, Build-Tools, etc)\\\">\\ud83d\\ude87</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/Juneezee\\\"><img src=\\\"https://avatars.githubusercontent.com/u/20135478?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Eng Zer Jun\\\"/><br /><sub><b>Eng Zer Jun</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=Juneezee\\\" title=\\\"Tests\\\">\\u26a0\\ufe0f</a></td>\\n    </tr>\\n    <tr>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/bbedward\\\"><img src=\\\"https://avatars.githubusercontent.com/u/550752?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"BB\\\"/><br /><sub><b>BB</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=bbedward\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/williamluer\\\"><img src=\\\"https://avatars.githubusercontent.com/u/85975676?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"williamluer\\\"/><br /><sub><b>williamluer</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=williamluer\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://sirupsen.com\\\"><img src=\\\"https://avatars.githubusercontent.com/u/97400?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Simon Eskildsen\\\"/><br /><sub><b>Simon Eskildsen</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=sirupsen\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://erbridge.co.uk\\\"><img src=\\\"https://avatars.githubusercontent.com/u/1027364?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"F\\\"/><br /><sub><b>F</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aerbridge\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a> <a href=\\\"https://github.com/replicate/cog/commits?author=erbridge\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/philandstuff\\\"><img src=\\\"https://avatars.githubusercontent.com/u/581269?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Philip Potter\\\"/><br /><sub><b>Philip Potter</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aphilandstuff\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a> <a href=\\\"https://github.com/replicate/cog/commits?author=philandstuff\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/joannejchen\\\"><img src=\\\"https://avatars.githubusercontent.com/u/33409024?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Joanne Chen\\\"/><br /><sub><b>Joanne Chen</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=joannejchen\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://technillogue.github.io\\\"><img src=\\\"https://avatars.githubusercontent.com/u/945691?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"technillogue\\\"/><br /><sub><b>technillogue</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=technillogue\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n    </tr>\\n    <tr>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://aroncarroll.com\\\"><img src=\\\"https://avatars.githubusercontent.com/u/47144?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Aron Carroll\\\"/><br /><sub><b>Aron Carroll</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=aron\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"https://github.com/replicate/cog/commits?author=aron\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a> <a href=\\\"#ideas-aron\\\" title=\\\"Ideas, Planning, & Feedback\\\">\\ud83e\\udd14</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/Theodotus1243\\\"><img src=\\\"https://avatars.githubusercontent.com/u/32220358?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Bohdan Mykhailenko\\\"/><br /><sub><b>Bohdan Mykhailenko</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=Theodotus1243\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"https://github.com/replicate/cog/issues?q=author%3ATheodotus1243\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/one1zero1one\\\"><img src=\\\"https://avatars.githubusercontent.com/u/724604?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Daniel Radu\\\"/><br /><sub><b>Daniel Radu</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=one1zero1one\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a> <a href=\\\"https://github.com/replicate/cog/issues?q=author%3Aone1zero1one\\\" title=\\\"Bug reports\\\">\\ud83d\\udc1b</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://github.com/Etelis\\\"><img src=\\\"https://avatars.githubusercontent.com/u/92247226?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Itay Etelis\\\"/><br /><sub><b>Itay Etelis</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=Etelis\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://www.wavefunction.dev\\\"><img src=\\\"https://avatars.githubusercontent.com/u/54407820?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Gennaro Schiano\\\"/><br /><sub><b>Gennaro Schiano</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=gschian0\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"http://andreknoerig.de\\\"><img src=\\\"https://avatars.githubusercontent.com/u/481350?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Andr\\u00e9 Kn\\u00f6rig\\\"/><br /><sub><b>Andr\\u00e9 Kn\\u00f6rig</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=aknoerig\\\" title=\\\"Documentation\\\">\\ud83d\\udcd6</a></td>\\n      <td align=\\\"center\\\" valign=\\\"top\\\" width=\\\"14.28%\\\"><a href=\\\"https://condense.live\\\"><img src=\\\"https://avatars.githubusercontent.com/u/24726?v=4?s=100\\\" width=\\\"100px;\\\" alt=\\\"Dan Fairs\\\"/><br /><sub><b>Dan Fairs</b></sub></a><br /><a href=\\\"https://github.com/replicate/cog/commits?author=danfairs\\\" title=\\\"Code\\\">\\ud83d\\udcbb</a></td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n<!-- markdownlint-restore -->\\n<!-- prettier-ignore-end -->\\n\\n<!-- ALL-CONTRIBUTORS-LIST:END -->\\n\\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Containers for machine learning\", \"latest_version\": \"0.16.8\", \"weekly_downloads\": 780172, \"description_cleaned\": \"Cog: Containers for machine learning\\nCog is an open-source tool that lets you package machine learning models in a standard, production-ready container.\\nYou can deploy your packaged model to your own infrastructure, or to\\nReplicate\\n.\\nHighlights\\n\\ud83d\\udce6\\nDocker containers without the pain.\\nWriting your own\\nDockerfile\\ncan be a bewildering process. With Cog, you define your environment with a\\nsimple configuration file\\nand it generates a Docker image with all the best practices: Nvidia base images, efficient caching of dependencies, installing specific Python versions, sensible environment variable defaults, and so on.\\n\\ud83e\\udd2c\\ufe0f\\nNo more CUDA hell.\\nCog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will set it all up correctly for you.\\n\\u2705\\nDefine the inputs and outputs for your model with standard Python.\\nThen, Cog generates an OpenAPI schema and validates the inputs and outputs with Pydantic.\\n\\ud83c\\udf81\\nAutomatic HTTP prediction server\\n: Your model's types are used to dynamically generate a RESTful HTTP API using\\nFastAPI\\n.\\n\\ud83e\\udd5e\\nAutomatic queue worker.\\nLong-running deep learning models or batch processing is best architected with a queue. Cog models do this out of the box. Redis is currently supported, with more in the pipeline.\\n\\u2601\\ufe0f\\nCloud storage.\\nFiles can be read and written directly to Amazon S3 and Google Cloud Storage. (Coming soon.)\\n\\ud83d\\ude80\\nReady for production.\\nDeploy your model anywhere that Docker images run. Your own infrastructure, or\\nReplicate\\n.\\nHow it works\\nDefine the Docker environment your model runs in with\\ncog.yaml\\n:\\nbuild\\n:\\ngpu\\n:\\ntrue\\nsystem_packages\\n:\\n-\\n\\\"libgl1-mesa-glx\\\"\\n-\\n\\\"libglib2.0-0\\\"\\npython_version\\n:\\n\\\"3.12\\\"\\npython_packages\\n:\\n-\\n\\\"torch==2.3\\\"\\npredict\\n:\\n\\\"predict.py:Predictor\\\"\\nDefine how predictions are run on your model with\\npredict.py\\n:\\nfrom\\ncog\\nimport\\nBasePredictor\\n,\\nInput\\n,\\nPath\\nimport\\ntorch\\nclass\\nPredictor\\n(\\nBasePredictor\\n):\\ndef\\nsetup\\n(\\nself\\n):\\n\\\"\\\"\\\"Load the model into memory to make running multiple predictions efficient\\\"\\\"\\\"\\nself\\n.\\nmodel\\n=\\ntorch\\n.\\nload\\n(\\n\\\"./weights.pth\\\"\\n)\\n# The arguments and types the model takes as input\\ndef\\npredict\\n(\\nself\\n,\\nimage\\n:\\nPath\\n=\\nInput\\n(\\ndescription\\n=\\n\\\"Grayscale input image\\\"\\n)\\n)\\n->\\nPath\\n:\\n\\\"\\\"\\\"Run a single prediction on the model\\\"\\\"\\\"\\nprocessed_image\\n=\\npreprocess\\n(\\nimage\\n)\\noutput\\n=\\nself\\n.\\nmodel\\n(\\nprocessed_image\\n)\\nreturn\\npostprocess\\n(\\noutput\\n)\\nIn the above we accept a path to the image as an input, and return a path to our transformed image after running it through our model.\\nNow, you can run predictions on this model:\\n$\\ncog\\npredict\\n-i\\nimage\\n=\\n@input.jpg\\n--> Building Docker image...\\n--> Running Prediction...\\n--> Output written to output.jpg\\nOr, build a Docker image for deployment:\\n$\\ncog\\nbuild\\n-t\\nmy-colorization-model\\n--> Building Docker image...\\n--> Built my-colorization-model:latest\\n$\\ndocker\\nrun\\n-d\\n-p\\n5000\\n:5000\\n--gpus\\nall\\nmy-colorization-model\\n$\\ncurl\\nhttp://localhost:5000/predictions\\n-X\\nPOST\\n\\\\\\n-H\\n'Content-Type: application/json'\\n\\\\\\n-d\\n'{\\\"input\\\": {\\\"image\\\": \\\"https://.../input.jpg\\\"}}'\\nOr, combine build and run via the\\nserve\\ncommand:\\n$\\ncog\\nserve\\n-p\\n8080\\n$\\ncurl\\nhttp://localhost:8080/predictions\\n-X\\nPOST\\n\\\\\\n-H\\n'Content-Type: application/json'\\n\\\\\\n-d\\n'{\\\"input\\\": {\\\"image\\\": \\\"https://.../input.jpg\\\"}}'\\nWhy are we building this?\\nIt's really hard for researchers to ship machine learning models to production.\\nPart of the solution is Docker, but it is so complex to get it to work: Dockerfiles, pre-/post-processing, Flask servers, CUDA versions. More often than not the researcher has to sit down with an engineer to get the damn thing deployed.\\nAndreas\\nand\\nBen\\ncreated Cog. Andreas used to work at Spotify, where he built tools for building and deploying ML models with Docker. Ben worked at Docker, where he created\\nDocker Compose\\n.\\nWe realized that, in addition to Spotify, other companies were also using Docker to build and deploy machine learning models.\\nUber\\nand others have built similar systems. So, we're making an open source version so other people can do this too.\\nHit us up if you're interested in using it or want to collaborate with us.\\nWe're on Discord\\nor email us at\\nteam@replicate.com\\n.\\nPrerequisites\\nmacOS, Linux or Windows 11\\n. Cog works on macOS, Linux and Windows 11 with\\nWSL 2\\nDocker\\n. Cog uses Docker to create a container for your model. You'll need to\\ninstall Docker\\nbefore you can run Cog. If you install Docker Engine instead of Docker Desktop, you will need to\\ninstall Buildx\\nas well.\\nInstall\\nIf you're using macOS, you can install Cog using Homebrew:\\nbrew install cog\\nYou can also download and install the latest release using our\\ninstall script\\n:\\n# fish shell\\nsh\\n(\\ncurl\\n-fsSL\\nhttps://cog.run/install.sh\\n|\\npsub\\n)\\n# bash, zsh, and other shells\\nsh\\n<\\n(\\ncurl\\n-fsSL\\nhttps://cog.run/install.sh\\n)\\n# download with wget and run in a separate command\\nwget\\n-qO-\\nhttps://cog.run/install.sh\\nsh\\n./install.sh\\nYou can manually install the latest release of Cog directly from GitHub\\nby running the following commands in a terminal:\\nsudo curl -o /usr/local/bin/cog -L \\\"https://github.com/replicate/cog/releases/latest/download/cog_$(uname -s)_$(uname -m)\\\"\\nsudo chmod +x /usr/local/bin/cog\\nAlternatively, you can build Cog from source and install it with these commands:\\nmake\\nsudo make install\\nOr if you are on docker:\\nRUN sh -c \\\"INSTALL_DIR=\\\\\\\"/usr/local/bin\\\\\\\" SUDO=\\\\\\\"\\\\\\\" $(curl -fsSL https://cog.run/install.sh)\\\"\\nUpgrade\\nIf you're using macOS and you previously installed Cog with Homebrew, run the following:\\nbrew upgrade cog\\nOtherwise, you can upgrade to the latest version by running the same commands you used to install it.\\nNext steps\\nGet started with an example model\\nGet started with your own model\\nUsing Cog with notebooks\\nUsing Cog with Windows 11\\nTake a look at some examples of using Cog\\nDeploy models with Cog\\ncog.yaml\\nreference\\nto learn how to define your model's environment\\nPrediction interface reference\\nto learn how the\\nPredictor\\ninterface works\\nTraining interface reference\\nto learn how to add a fine-tuning API to your model\\nHTTP API reference\\nto learn how to use the HTTP API that models serve\\nNeed help?\\nJoin us in #cog on Discord.\\nContributors \\u2728\\nThanks goes to these wonderful people (\\nemoji key\\n):\\nBen Firshman\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\nAndreas Jansson\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\ud83d\\udea7\\nZeke Sikelianos\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\ud83d\\udd27\\nRory Byrne\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\u26a0\\ufe0f\\nMichael Floering\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\ud83e\\udd14\\nBen Evans\\n\\ud83d\\udcd6\\nshashank agarwal\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\nVictorXLR\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\u26a0\\ufe0f\\nhung anna\\n\\ud83d\\udc1b\\nBrian Whitman\\n\\ud83d\\udc1b\\nJimothyJohn\\n\\ud83d\\udc1b\\nericguizzo\\n\\ud83d\\udc1b\\nDominic Baggott\\n\\ud83d\\udcbb\\n\\u26a0\\ufe0f\\nDashiell Stander\\n\\ud83d\\udc1b\\n\\ud83d\\udcbb\\n\\u26a0\\ufe0f\\nShuwei Liang\\n\\ud83d\\udc1b\\n\\ud83d\\udcac\\nEric Allam\\n\\ud83e\\udd14\\nIv\\u00e1n Perdomo\\n\\ud83d\\udc1b\\nCharles Frye\\n\\ud83d\\udcd6\\nLuan Pham\\n\\ud83d\\udc1b\\n\\ud83d\\udcd6\\nTommyDew\\n\\ud83d\\udcbb\\nJesse Andrews\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\u26a0\\ufe0f\\nNick Stenning\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\ud83c\\udfa8\\n\\ud83d\\ude87\\n\\u26a0\\ufe0f\\nJustin Merrell\\n\\ud83d\\udcd6\\nRurik Yl\\u00e4-Onnenvuori\\n\\ud83d\\udc1b\\nYouka\\n\\ud83d\\udc1b\\nClay Mullis\\n\\ud83d\\udcd6\\nMattt\\n\\ud83d\\udcbb\\n\\ud83d\\udcd6\\n\\ud83d\\ude87\\nEng Zer Jun\\n\\u26a0\\ufe0f\\nBB\\n\\ud83d\\udcbb\\nwilliamluer\\n\\ud83d\\udcd6\\nSimon Eskildsen\\n\\ud83d\\udcbb\\nF\\n\\ud83d\\udc1b\\n\\ud83d\\udcbb\\nPhilip Potter\\n\\ud83d\\udc1b\\n\\ud83d\\udcbb\\nJoanne Chen\\n\\ud83d\\udcd6\\ntechnillogue\\n\\ud83d\\udcbb\\nAron Carroll\\n\\ud83d\\udcd6\\n\\ud83d\\udcbb\\n\\ud83e\\udd14\\nBohdan Mykhailenko\\n\\ud83d\\udcd6\\n\\ud83d\\udc1b\\nDaniel Radu\\n\\ud83d\\udcd6\\n\\ud83d\\udc1b\\nItay Etelis\\n\\ud83d\\udcbb\\nGennaro Schiano\\n\\ud83d\\udcd6\\nAndr\\u00e9 Kn\\u00f6rig\\n\\ud83d\\udcd6\\nDan Fairs\\n\\ud83d\\udcbb\\nThis project follows the\\nall-contributors\\nspecification. Contributions of any kind welcome!\"}, {\"name\": \"unearth\", \"description\": \"# unearth\\n\\n<!--index start-->\\n\\n[![Tests](https://github.com/frostming/unearth/workflows/Tests/badge.svg)](https://github.com/frostming/unearth/actions?query=workflow%3Aci)\\n[![pypi version](https://img.shields.io/pypi/v/unearth.svg)](https://pypi.org/project/unearth/)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\\n[![pdm-managed](https://img.shields.io/endpoint?url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Fpdm-project%2F.github%2Fbadge.json)](https://pdm-project.org)\\n\\nA utility to fetch and download python packages\\n\\n## Why this project?\\n\\nThis project exists as the last piece to complete the puzzle of a package manager. The other pieces are:\\n\\n- [resolvelib](https://pypi.org/project/resolvelib/) - Resolves concrete dependencies from a set of (abstract) requirements.\\n- [unearth](https://pypi.org/project/unearth/) _(This project)_ - Finds and downloads the best match(es) for a given requirement.\\n- [build](https://pypi.org/project/build/) - Builds wheels from the source code.\\n- [installer](https://pypi.org/project/installer/) - Installs packages from wheels.\\n\\nThey provide all the low-level functionalities that are needed to resolve and install packages.\\n\\n## Why not pip?\\n\\nThe core functionality is basically extracted from pip. However, pip is not designed to be used as a library and hence the API is not very stable.\\nUnearth serves as a stable replacement for pip's `PackageFinder` API. It will follow the conventions of [Semantic Versioning](https://semver.org/) so that downstream projects can use it to develop their own package finding and downloading.\\n\\n## Requirements\\n\\nunearth requires Python >=3.8\\n\\n## Installation\\n\\n```bash\\n$ python -m pip install --upgrade unearth\\n```\\n\\n## Quickstart\\n\\nGet the best matching candidate for a requirement:\\n\\n```python\\n>>> from unearth import PackageFinder\\n>>> finder = PackageFinder(index_urls=[\\\"https://pypi.org/simple/\\\"])\\n>>> result = finder.find_best_match(\\\"flask>=2\\\")\\n>>> result.best\\nPackage(name='flask', version='2.1.2')\\n```\\n\\nUsing the CLI:\\n\\n```bash\\n$ unearth \\\"flask>=2\\\"\\n{\\n  \\\"name\\\": \\\"flask\\\",\\n  \\\"version\\\": \\\"3.0.0\\\",\\n  \\\"link\\\": {\\n    \\\"url\\\": \\\"https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl\\\",\\n    \\\"comes_from\\\": \\\"https://pypi.org/simple/flask/\\\",\\n    \\\"yank_reason\\\": null,\\n    \\\"requires_python\\\": \\\">=3.8\\\",\\n    \\\"metadata\\\": \\\"https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl.metadata\\\"\\n  }\\n}\\n```\\n\\n<!--index end-->\\n\\n## Documentation\\n\\n[Read the docs](https://unearth.readthedocs.io/en/latest/)\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"A utility to fetch and download python packages\", \"latest_version\": \"0.18.1\", \"weekly_downloads\": 779961, \"description_cleaned\": \"unearth\\nA utility to fetch and download python packages\\nWhy this project?\\nThis project exists as the last piece to complete the puzzle of a package manager. The other pieces are:\\nresolvelib\\n- Resolves concrete dependencies from a set of (abstract) requirements.\\nunearth\\n(This project)\\n- Finds and downloads the best match(es) for a given requirement.\\nbuild\\n- Builds wheels from the source code.\\ninstaller\\n- Installs packages from wheels.\\nThey provide all the low-level functionalities that are needed to resolve and install packages.\\nWhy not pip?\\nThe core functionality is basically extracted from pip. However, pip is not designed to be used as a library and hence the API is not very stable.\\nUnearth serves as a stable replacement for pip's\\nPackageFinder\\nAPI. It will follow the conventions of\\nSemantic Versioning\\nso that downstream projects can use it to develop their own package finding and downloading.\\nRequirements\\nunearth requires Python >=3.8\\nInstallation\\n$\\npython\\n-m\\npip\\ninstall\\n--upgrade\\nunearth\\nQuickstart\\nGet the best matching candidate for a requirement:\\n>>>\\nfrom\\nunearth\\nimport\\nPackageFinder\\n>>>\\nfinder\\n=\\nPackageFinder\\n(\\nindex_urls\\n=\\n[\\n\\\"https://pypi.org/simple/\\\"\\n])\\n>>>\\nresult\\n=\\nfinder\\n.\\nfind_best_match\\n(\\n\\\"flask>=2\\\"\\n)\\n>>>\\nresult\\n.\\nbest\\nPackage\\n(\\nname\\n=\\n'flask'\\n,\\nversion\\n=\\n'2.1.2'\\n)\\nUsing the CLI:\\n$\\nunearth\\n\\\"flask>=2\\\"\\n{\\n\\\"name\\\"\\n:\\n\\\"flask\\\"\\n,\\n\\\"version\\\"\\n:\\n\\\"3.0.0\\\"\\n,\\n\\\"link\\\"\\n:\\n{\\n\\\"url\\\"\\n:\\n\\\"https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl\\\"\\n,\\n\\\"comes_from\\\"\\n:\\n\\\"https://pypi.org/simple/flask/\\\"\\n,\\n\\\"yank_reason\\\"\\n:\\nnull,\\n\\\"requires_python\\\"\\n:\\n\\\">=3.8\\\"\\n,\\n\\\"metadata\\\"\\n:\\n\\\"https://files.pythonhosted.org/packages/36/42/015c23096649b908c809c69388a805a571a3bea44362fe87e33fc3afa01f/flask-3.0.0-py3-none-any.whl.metadata\\\"\\n}\\n}\\nDocumentation\\nRead the docs\"}, {\"name\": \"eth-typing\", \"description\": \"# eth-typing\\n\\n[![Join the conversation on Discord](https://img.shields.io/discord/809793915578089484?color=blue&label=chat&logo=discord&logoColor=white)](https://discord.gg/GHryRvPB84)\\n[![Build Status](https://circleci.com/gh/ethereum/eth-typing.svg?style=shield)](https://circleci.com/gh/ethereum/eth-typing)\\n[![PyPI version](https://badge.fury.io/py/eth-typing.svg)](https://badge.fury.io/py/eth-typing)\\n[![Python versions](https://img.shields.io/pypi/pyversions/eth-typing.svg)](https://pypi.python.org/pypi/eth-typing)\\n[![Docs build](https://readthedocs.org/projects/eth-typing/badge/?version=latest)](https://eth-typing.readthedocs.io/en/latest/?badge=latest)\\n\\nCommon type annotations for ethereum python packages.\\n\\nRead the [documentation](https://eth-typing.readthedocs.io/).\\n\\nView the [change log](https://eth-typing.readthedocs.io/en/latest/release_notes.html).\\n\\n## Installation\\n\\n```sh\\npython -m pip install eth-typing\\n```\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"eth-typing: Common type annotations for ethereum python packages\", \"latest_version\": \"5.2.1\", \"weekly_downloads\": 779882, \"description_cleaned\": \"eth-typing\\nCommon type annotations for ethereum python packages.\\nRead the\\ndocumentation\\n.\\nView the\\nchange log\\n.\\nInstallation\\npython\\n-m\\npip\\ninstall\\neth-typing\"}, {\"name\": \"opentelemetry-instrumentation-boto3sqs\", \"description\": \"OpenTelemetry Boto3 SQS Instrumentation\\n=======================================\\n\\n|pypi|\\n\\n.. |pypi| image:: https://badge.fury.io/py/opentelemetry-instrumentation-boto3sqs.svg\\n   :target: https://pypi.org/project/opentelemetry-instrumentation-boto3sqs/\\n\\nThis library allows tracing requests made by the Boto3 library to the SQS service.\\n\\nInstallation\\n------------\\n\\n::\\n\\n    pip install opentelemetry-instrumentation-boto3sqs\\n\\n\\nReferences\\n----------\\n\\n* `OpenTelemetry boto3sqs/ Tracing <https://opentelemetry-python-contrib.readthedocs.io/en/latest/instrumentation/boto3sqs/boto3sqs.html>`_\\n* `OpenTelemetry Project <https://opentelemetry.io/>`_\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"Boto3 SQS service tracing for OpenTelemetry\", \"latest_version\": \"0.59b0\", \"weekly_downloads\": 779386, \"description_cleaned\": \"This library allows tracing requests made by the Boto3 library to the SQS service.\\nInstallation\\npip install opentelemetry-instrumentation-boto3sqs\\nReferences\\nOpenTelemetry boto3sqs/ Tracing\\nOpenTelemetry Project\"}, {\"name\": \"Flask-RESTful\", \"description\": \"\", \"description_content_type\": null, \"summary\": \"Simple framework for creating REST APIs\", \"latest_version\": \"0.3.10\", \"weekly_downloads\": 778574, \"description_cleaned\": \"\"}, {\"name\": \"pykwalify\", \"description\": \"# pyKwalify\\n\\nYAML/JSON validation library\\n\\nThis framework is a port with a lot of added functionality of the Java version of the framework kwalify that can be found at http://www.kuwata-lab.com/kwalify/\\n\\nThe original source code can be found at http://sourceforge.net/projects/kwalify/files/kwalify-java/0.5.1/\\n\\nThe source code of the latest release that has been used can be found at https://github.com/sunaku/kwalify. Please note that source code is not the original authors code but a fork/upload of the last release available in Ruby.\\n\\nThe schema this library is based on and extended from: http://www.kuwata-lab.com/kwalify/ruby/users-guide.01.html#schema\\n\\n\\n# Usage\\n\\nCreate a data file. `Json` and `Yaml` formats are both supported.\\n\\n```yaml\\n- foo\\n- bar\\n```\\n\\nCreate a schema file with validation rules.\\n\\n```yaml\\ntype: seq\\nsequence:\\n  - type: str\\n```\\n\\nRun validation from cli.\\n\\n```bash\\npykwalify -d data.yaml -s schema.yaml\\n```\\n\\n\\n## Examples\\n\\nThe documentation describes in detail how each keyword and type works and what is possible in each case.\\n\\nBut there is a lot of real world examples that can be found in the test data/files. It shows a lot of examples of how all keywords and types work in practice and in combination with each other.\\n\\nThe files can be found here and show both schema/data combinations that will work and that will fail.\\n\\n - `tests/files/success/`\\n - `tests/files/fail/`\\n - `tests/files/partial_schemas/`\\n\\n\\n# YAML parser\\n\\n`ruamel.yaml` is the default YAMl parser installed with pykwalify.\\n\\nRuamel.yaml is more supported in the yaml 1.2 spec and is more actively developed.\\n\\nDepending on how both libraries are developed, this can change in the future in any major update.\\n\\n\\n\\n## UTF-8 and data encoding\\n\\nIf you have problems with unicode values not working properly when running pykwalify on Python 3.6x then try to add this environment variable to your execution:\\n\\n```\\nPYTHONIOENCODING=UTF-8 pykwalify ...\\n```\\n\\nand it might help to force UTF-8 encoding on all string objects. If this does not work please open up an issue with your schema and data that can be used to track down the problem in the source code.\\n\\n\\n# Project details\\n\\n|   |   |\\n|---|---|\\n| python support         | 3.6, 3.7, 3.8, 3.9 |\\n| Source                 | https://github.com/Grokzen/pykwalify |\\n| Docs (Latest release)  | http://pykwalify.readthedocs.io/en/master/ |\\n| Docs (Unstable branch) | http://pykwalify.readthedocs.io/en/unstable/ |\\n| Gitter (Free Chat)     | [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Grokzen/pykwalify?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) |\\n| Changelog              | https://github.com/Grokzen/pykwalify/blob/unstable/docs/release-notes.rst |\\n| Upgrade instructions   | https://github.com/Grokzen/pykwalify/blob/unstable/docs/upgrade-instructions.rst |\\n| Issues                 | https://github.com/Grokzen/pykwalify/issues |\\n| Travis (master)        | [![Build Status](https://travis-ci.org/Grokzen/pykwalify.svg?branch=master)](https://travis-ci.org/Grokzen/pykwalify) https://travis-ci.org/Grokzen/pykwalify |\\n| Travis (unstable)      | [![Build Status](https://travis-ci.org/Grokzen/pykwalify.svg?branch=unstable)](https://travis-ci.org/Grokzen/pykwalify) https://travis-ci.org/Grokzen/pykwalify |\\n| Test coverage          | [![Coverage Status](https://coveralls.io/repos/Grokzen/pykwalify/badge.png?branch=master)](https://coveralls.io/r/Grokzen/pykwalify) https://coveralls.io/github/Grokzen/pykwalify |\\n| pypi                   | https://pypi.python.org/pypi/pykwalify/ |\\n| Open Hub               | https://www.openhub.net/p/pykwalify |\\n| License                | `MIT` https://github.com/Grokzen/pykwalify/blob/unstable/docs/license.rst |\\n| Copyright              | `Copyright (c) 2013-2017 Johan Andersson` |\\n| git repo               | `git clone git@github.com:Grokzen/pykwalify.git` |\\n| install stable         | `pip install pykwalify` |\\n| install dev            | `$ git clone git@github.com:Grokzen/pykwalify.git pykwalify`<br>`$ cd ./pykwalify`<br>`$ virtualenv .venv`<br>`$ source .venv/bin/activate`<br>`$ pip install -r dev-requirements.txt`<br>`$ pip install -e .` |\\n| required dependencies  | `docopt >= 0.6.2`<br> `python-dateutil >= 2.4.2` |\\n| supported yml parsers  | `ruamel.yaml >= 0.11.0` |\", \"description_content_type\": \"text/markdown\", \"summary\": \"Python lib/cli for JSON/YAML schema validation\", \"latest_version\": \"1.8.0\", \"weekly_downloads\": 777945, \"description_cleaned\": \"pyKwalify\\nYAML/JSON validation library\\nThis framework is a port with a lot of added functionality of the Java version of the framework kwalify that can be found at\\nhttp://www.kuwata-lab.com/kwalify/\\nThe original source code can be found at\\nhttp://sourceforge.net/projects/kwalify/files/kwalify-java/0.5.1/\\nThe source code of the latest release that has been used can be found at\\nhttps://github.com/sunaku/kwalify\\n. Please note that source code is not the original authors code but a fork/upload of the last release available in Ruby.\\nThe schema this library is based on and extended from:\\nhttp://www.kuwata-lab.com/kwalify/ruby/users-guide.01.html#schema\\nUsage\\nCreate a data file.\\nJson\\nand\\nYaml\\nformats are both supported.\\n-\\nfoo\\n-\\nbar\\nCreate a schema file with validation rules.\\ntype\\n:\\nseq\\nsequence\\n:\\n-\\ntype\\n:\\nstr\\nRun validation from cli.\\npykwalify\\n-d\\ndata.yaml\\n-s\\nschema.yaml\\nExamples\\nThe documentation describes in detail how each keyword and type works and what is possible in each case.\\nBut there is a lot of real world examples that can be found in the test data/files. It shows a lot of examples of how all keywords and types work in practice and in combination with each other.\\nThe files can be found here and show both schema/data combinations that will work and that will fail.\\ntests/files/success/\\ntests/files/fail/\\ntests/files/partial_schemas/\\nYAML parser\\nruamel.yaml\\nis the default YAMl parser installed with pykwalify.\\nRuamel.yaml is more supported in the yaml 1.2 spec and is more actively developed.\\nDepending on how both libraries are developed, this can change in the future in any major update.\\nUTF-8 and data encoding\\nIf you have problems with unicode values not working properly when running pykwalify on Python 3.6x then try to add this environment variable to your execution:\\nPYTHONIOENCODING=UTF-8 pykwalify ...\\nand it might help to force UTF-8 encoding on all string objects. If this does not work please open up an issue with your schema and data that can be used to track down the problem in the source code.\\nProject details\\npython support\\n3.6, 3.7, 3.8, 3.9\\nSource\\nhttps://github.com/Grokzen/pykwalify\\nDocs (Latest release)\\nhttp://pykwalify.readthedocs.io/en/master/\\nDocs (Unstable branch)\\nhttp://pykwalify.readthedocs.io/en/unstable/\\nGitter (Free Chat)\\nChangelog\\nhttps://github.com/Grokzen/pykwalify/blob/unstable/docs/release-notes.rst\\nUpgrade instructions\\nhttps://github.com/Grokzen/pykwalify/blob/unstable/docs/upgrade-instructions.rst\\nIssues\\nhttps://github.com/Grokzen/pykwalify/issues\\nTravis (master)\\nhttps://travis-ci.org/Grokzen/pykwalify\\nTravis (unstable)\\nhttps://travis-ci.org/Grokzen/pykwalify\\nTest coverage\\nhttps://coveralls.io/github/Grokzen/pykwalify\\npypi\\nhttps://pypi.python.org/pypi/pykwalify/\\nOpen Hub\\nhttps://www.openhub.net/p/pykwalify\\nLicense\\nMIT\\nhttps://github.com/Grokzen/pykwalify/blob/unstable/docs/license.rst\\nCopyright\\nCopyright (c) 2013-2017 Johan Andersson\\ngit repo\\ngit clone git@github.com:Grokzen/pykwalify.git\\ninstall stable\\npip install pykwalify\\ninstall dev\\n$ git clone git@github.com:Grokzen/pykwalify.git pykwalify\\n$ cd ./pykwalify\\n$ virtualenv .venv\\n$ source .venv/bin/activate\\n$ pip install -r dev-requirements.txt\\n$ pip install -e .\\nrequired dependencies\\ndocopt >= 0.6.2\\npython-dateutil >= 2.4.2\\nsupported yml parsers\\nruamel.yaml >= 0.11.0\"}, {\"name\": \"dbt-bigquery\", \"description\": \"<p align=\\\"center\\\">\\n    <img\\n        src=\\\"https://raw.githubusercontent.com/dbt-labs/dbt/ec7dee39f793aa4f7dd3dae37282cc87664813e4/etc/dbt-logo-full.svg\\\"\\n        alt=\\\"dbt logo\\\"\\n        width=\\\"500\\\"\\n    />\\n</p>\\n\\n<p align=\\\"center\\\">\\n    <a href=\\\"https://pypi.org/project/dbt-bigquery/\\\">\\n        <img src=\\\"https://badge.fury.io/py/dbt-bigquery.svg\\\" />\\n    </a>\\n    <a target=\\\"_blank\\\" href=\\\"https://pypi.org/project/dbt-bigquery/\\\" style=\\\"background:none\\\">\\n        <img src=\\\"https://img.shields.io/pypi/pyversions/dbt-bigquery\\\">\\n    </a>\\n    <a href=\\\"https://github.com/psf/black\\\">\\n        <img src=\\\"https://img.shields.io/badge/code%20style-black-000000.svg\\\" />\\n    </a>\\n    <a href=\\\"https://github.com/python/mypy\\\">\\n        <img src=\\\"https://www.mypy-lang.org/static/mypy_badge.svg\\\" />\\n    </a>\\n    <a href=\\\"https://pepy.tech/project/dbt-bigquery\\\">\\n        <img src=\\\"https://static.pepy.tech/badge/dbt-bigquery/month\\\" />\\n    </a>\\n</p>\\n\\n# dbt\\n\\n**[dbt](https://www.getdbt.com/)** enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.\\n\\ndbt is the T in ELT. Organize, cleanse, denormalize, filter, rename, and pre-aggregate the raw data in your warehouse so that it's ready for analysis.\\n\\n## dbt-bigquery\\n\\n`dbt-bigquery` enables dbt to work with Google BigQuery.\\nFor more information on using dbt with BigQuery, consult [the docs](https://docs.getdbt.com/docs/profile-bigquery).\\n\\n# Getting started\\n\\nReview the repository [README.md](/README.md) as most of that information pertains to `dbt-bigquery`.\\n\\n## Contribute\\n\\n- Want to help us build `dbt-bigquery`? Check out the [Contributing Guide](CONTRIBUTING.md).\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"The BigQuery adapter plugin for dbt\", \"latest_version\": \"1.10.3\", \"weekly_downloads\": 777336, \"description_cleaned\": \"dbt\\ndbt\\nenables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.\\ndbt is the T in ELT. Organize, cleanse, denormalize, filter, rename, and pre-aggregate the raw data in your warehouse so that it's ready for analysis.\\ndbt-bigquery\\ndbt-bigquery\\nenables dbt to work with Google BigQuery.\\nFor more information on using dbt with BigQuery, consult\\nthe docs\\n.\\nGetting started\\nReview the repository\\nREADME.md\\nas most of that information pertains to\\ndbt-bigquery\\n.\\nContribute\\nWant to help us build\\ndbt-bigquery\\n? Check out the\\nContributing Guide\\n.\"}, {\"name\": \"polib\", \"description\": \"\\n=====\\npolib\\n=====\\n\\n|pypi-version| |pypi-stats| |build-status-image| |codecov-image| |documentation-status-image| |py-versions|\\n\\nOverview\\n--------\\n\\npolib is a library to manipulate, create, modify gettext files (pot, po and mo\\nfiles). You can load existing files, iterate through it's entries, add, modify\\nentries, comments or metadata, etc... or create new po files from scratch.\\n\\npolib supports out of the box any version of python ranging from 2.7 to latest\\n3.X version.\\n\\npolib is pretty stable now and is used by many \\n`opensource projects <http://polib.readthedocs.org/en/latest/projects.html>`_.\\n\\nThe project code and bugtracker is hosted on \\n`Github <https://github.com/izimobil/polib/>`_.\\n\\npolib is generously documented, you can `browse the documentation online \\n<http://polib.readthedocs.org/>`_, a good start is to read \\n`the quickstart guide  <http://polib.readthedocs.org/en/latest/quickstart.html>`_.\\n\\n\\nInstallation\\n~~~~~~~~~~~~\\n\\nJust use ``pip``:\\n\\n.. code:: bash\\n\\n    $ pip install polib\\n\\n\\nBasic example\\n~~~~~~~~~~~~~\\n\\n.. code:: python\\n\\n    import polib\\n\\n    pofile = polib.pofile('/path/to/pofile.po')\\n\\n    for entry in pofile:\\n        print(entry.msgid, entry.msgstr)\\n\\n\\n.. |build-status-image| image:: https://api.travis-ci.com/izimobil/polib.svg?branch=master\\n   :target: https://app.travis-ci.com/github/izimobil/polib\\n   :alt: Travis build\\n\\n.. |codecov-image| image:: https://codecov.io/gh/izimobil/polib/branch/master/graph/badge.svg\\n  :target: https://codecov.io/gh/izimobil/polib\\n\\n.. |pypi-version| image:: https://img.shields.io/pypi/v/polib.svg\\n   :target: https://pypi.python.org/pypi/polib\\n   :alt: Pypi version\\n\\n.. |pypi-stats| image:: https://img.shields.io/pypi/dm/polib.svg\\n   :target: https://pypistats.org/packages/polib\\n   :alt: Pypi downloads\\n\\n.. |documentation-status-image| image:: https://readthedocs.org/projects/polib/badge/?version=latest\\n   :target: http://polib.readthedocs.io/en/latest/?badge=latest\\n   :alt: Documentation Status\\n\\n.. |py-versions| image:: https://img.shields.io/pypi/pyversions/polib.svg\\n   :target: https://img.shields.io/pypi/pyversions/polib.svg\\n   :alt: Python versions\\n\\n\\n=========\\nChangelog\\n=========\\n\\nVersion 1.2.0 (2023/02/23)\\n--------------------------\\n - Added setter for the fuzzy property\\n - Escape/unescape \\\\v, \\\\b and \\\\f\\n - Added path to error message processing parser symbols\\n - Ensure empty previous values for msgid and msgstr are properly saved\\n - Fixed relative ordering of comments and translator comments\\n - Do not count obsolete entries in fuzzy() method\\n - Close files properly\\n - Fixed support for loading mo files from bytes object\\n - Fixed msgstr_plural comparison\\n - Simplified python version check\\n - Updated docs\\n\\nVersion 1.1.1 (2021/03/26)\\n--------------------------\\n - Strip UTF-16 BOM (U+FEFF) from start of file\\n - Message context (msgctxt) support for MO files\\n - Dropped Python < 2.7 support\\n - Updated docs and README after switch from Bitbucket to Github\\n\\nVersion 1.1.0 (2017/11/27)\\n--------------------------\\n - Fixed entries sorting when generating mo file (Fixes #78)\\n - Fixed find method (Fixes #84)\\n - Refactored POEntry.__cmp__ method (this should fix issues #60, #65 and #79)\\n - Fixed duplicated entries when merging po file with pot file (fixes #68)\\n - Fixed \\\"None\\\" string being outputted when polib deals with syntax error in string (Fixes issue #66)\\n - Added a fuzzy property to entries (Fixes #76)\\n - Take into account the message context when comparing entries\\n - Leave occurence untouched if line number is not a number (fixes #80 and #90)\\n - Fixed test for mo files that was failing because since gettext version 0.19.8.1, msgfmt skips the \\\"POT-Creation-Date\\\" metadata entry (Fixes #86)\\n - Fixed major revision number comparison\\n - Dropped python 2.4 support\\n\\nVersion 1.0.8 (2016/11/21)\\n--------------------------\\n - Fixed issue #70 (occurrences parsing for windows pathes)\\n - Fixed issue #71 (tcomment and flags not saved for obsolete entries)\\n - Fixed issue #72 (wrong metadata ordering)\\n - Fixed issue #73 (can't always unpickle POFile)\\n - Use natural sorting for additional headers (metadata)\\n - Fixed typos in various documents\\n\\nVersion 1.0.7 (2015/07/08)\\n--------------------------\\n - Fixed bad parsing of indented msgstr_plural\\n - Fixed ordering of \\\"Language\\\" metadata entry\\n - Removed space after \\\"#\\\" in header if comment line is empty (like gettext tools)\\n - Fixed typos / grammar errors (thanks Jakub Wilk)\\n - Take into account msgid_plural if needed when comparing entries (thanks Leonardo Constantino Oliveira)\\n - Fixed issue #63 (str() on a bytes instance when using python3) (thanks Jakub Wilk)\\n\\nVersion 1.0.6 (2015/01/04)\\n--------------------------\\n - Wheel support\\n - Add missing 'Language' and 'Plural-Forms' to metadata ordering\\n - More accurate float operation for POFile.percent_translated()\\n\\nVersion 1.0.5 (2014/08/22)\\n--------------------------\\n - Fixed issue #59: tokens variable referenced before assignment\\n - Implemented feature request #56: line number information in PO entries\\n - Fixed issue #61: polib does not handle previous msgid on multilines properly\\n\\nVersion 1.0.4 (2014/02/19)\\n--------------------------\\n - Fixed issue #43: improved check that determine if polib is dealing with a filepath or unicode content\\n - Fixed issue #44: polib now checks MO files revision number and throws an error if the number is unexpected\\n - Fixed issue #45: parse properly mo files with no header entry\\n - Fixed issue #47: added flags attribute for MOEntry to be consistent with POEntry\\n - Fixed issue #49: use integers rather than strings for msgstr_plural keys\\n - Fixed issue #51: if a PO file ends with a comment, polib adds a spurious empty entry at the end\\n - Fixed issue #52: bad magic number written on big endian platforms\\n - Fixed issue #53: added a __hash__() method to POEntry and MOEntry classes\\n - Fixed issue #54: use lowercase for state identifiers. This fixes issues with certain locales and string.lower()\\n - Fixed issue #58: use io.open() instead of codecs.open() because the latter doesn't handle very well universal line endings\\n - Make sure the mo file is closed at garbage collection, this prevents warnings on unclosed file when running tests with python >= 3.2\\n - Better way to test endianness\\n - polib download URL is now on Pypi\\n\\nVersion 1.0.3 (2013/02/09)\\n--------------------------\\n - Fixed issue #38: POFile.append() raised a duplicate exception when you tried to add a new entry with the same msgid and a different msgctxt (only when check_for_duplicates option is set to True)\\n - Fixed issue #39: Added __init__.py file for convenience\\n - Fixed issue #41: UnicodeDecodeError when running setup.py build on python3 with C locale\\n - polib is now fully PEP8 compliant\\n - Small improvements: remove unused \\\"typ\\\" var (thanks Rodrigo Silva), improved Makefile, Make sure _BaseFile.__contains__ returns a boolean value\\n\\nVersion 1.0.2 (2012/10/23)\\n--------------------------\\n - allow empty comments, flags or occurrences lines\\n\\nVersion 1.0.1 (2012/09/11)\\n--------------------------\\n - speed up POFile.merge method (thanks @encukou)\\n - allow comments starting with two '#' characters (thanks @goibhniu)\\n\\nVersion 1.0.0 (2012/06/08)\\n--------------------------\\nYeah... after nearly 6 years, polib reaches the stable state :)\\nChanges and fixes in this release :\\n\\n - polib.pofile and polib.mofile functions can now return a custom class (thanks Craig Blaszczyk)\\n - polib now can find the metadata entry no matter where it is located (thanks Fran\\u00e7ois Poirotte)\\n - fixed issue #28 (IOError on reading obsolete \\\"previous msgid\\\" entries) (thanks James Ni)\\n\\nVersion 0.7.0 (2011/07/14)\\n--------------------------\\nThis version adds support for python 3 (thanks to Vinay Sajip).\\npolib now supports out-of-the-box any version of python ranging from 2.4 to latest 3.X version.\\npolib is now 5 years old ;) so the 0.7.X branch will be the last before the 1.X stable branch.\\n\\nVersion 0.6.4 (2011/07/13)\\n--------------------------\\n - Better api, autodetected_encoding is no longer required to explicitly set the encoding (fixes issue #23),\\n - Fixed issue #24 Support indented PO files (thanks to Fran\\u00e7ois Poirotte).\\n\\nVersion 0.6.3 (2011/02/19)\\n--------------------------\\n - Fixed issue #19 (Disappearing newline characters due to textwrap module),\\n - ensure wrapping works as expected.\\n\\nVersion 0.6.2 (2011/02/09)\\n--------------------------\\n - Backported textwrap.TextWrapper._wrap_chunks that has support for the drop_whitespace parameter added in Python 2.6 (Fixes #18: broken compatibility with python 2.5, thanks @jezdez).\\n\\nVersion 0.6.1 (2011/02/09)\\n--------------------------\\n - fixed regression that prevented POFile initialization from data to work (issue #17).\\n\\nVersion 0.6.0 (2011/02/07)\\n--------------------------\\n - polib is now `fully documented <http://polib.readthedocs.org>`_,\\n - switched from doctests to unit tests to keep the polib.py file clean,\\n - fixed issue #7 (wrapping issues, thanks @jezdez),\\n - added a __eq__ method to _BaseFile (thanks @kost BebiX),\\n - handle msgctxt correctly when compiling mo files,\\n - compiled mo files are now exactly the same as those compiled by msgfmt without using hash tables.\\n\\nVersion 0.5.5 (2010/10/30)\\n--------------------------\\n - Removed multiline handling code, it was a mess and was the source of potential bugs like issue #11,\\n - Fixed typo in README and CHANGELOG, fixes issue #13.\\n\\nVersion 0.5.4 (2010/10/02)\\n--------------------------\\n - fixed an issue with detect_encoding(), in some cases it could return an invalid charset.\\n\\nVersion 0.5.3 (2010/08/29)\\n--------------------------\\n - correctly unescape lines containing both \\\\\\\\\\\\\\\\n and \\\\\\\\n (thanks to Martin Geisler),\\n - fixed issue #6: __str__() methods are returning unicode instead of str,\\n - fixed issue #8: POFile.merge error when an entry is obsolete in a .po, that this entry reappears in the .pot and that we merge the two,\\n - added support to instantiate POFile objects using data instead of file path (thanks to Diego B\\u00farigo Zacar\\u00e3o),\\n - fixed issue #9: POFile.merge drop fuzzy attributes from translations (thanks to Tim Gerundt),\\n - fixed issue #10: Finding entries with the same msgid and different context (msgctxt).\\n\\nVersion 0.5.2 (2010/06/09)\\n--------------------------\\n - fixed issue #1: untranslated_entries() also show fuzzy message,\\n - write back the fuzzy header if present in the pofile,\\n - added support for previous msgctxt, previous msgid and previous msgid_plural comments (fixes issue #5),\\n - better handling of lines wrapping.\\n\\nVersion 0.5.1 (2009/12/14)\\n--------------------------\\n - fixed issue #0025: setup.py requires CHANGELOG but it's not present in polib-0.5.0-tar.gz\\n\\nVersion 0.5.0 (2009/12/13)\\n--------------------------\\n - fixed issue #0017: UnicodeDecodeError while writing a mo-file,\\n - fixed issue #0018: implemented support for msgctxt,\\n - fixed bug when compiling plural msgids/strs,\\n - API docs are no longer included, hopefully next release will ship with sphinx documentation,\\n - parse msg plural entries correctly when reading mo files,\\n - fixed issue #0020 and #0021: added ability to check for duplicate when adding entries to po/mo files, this is optional and not enabled by default because it slows down considerably the library,\\n - fixed issue #0022: unescaping code is insufficient,\\n - fixed issue #0023: encoding error when saving mo file as po file (thanks to sebastien.sable for the patch !).\\n\\nVersion 0.4.2 (2009/06/05)\\n--------------------------\\n - fixed issue #0007: use the codecs module to open files,\\n - fixed issue #0014: plural forms are not saved correctly in the mo file (thanks lorenzo.gil.sanchez for the patch),\\n - fixed issue #0015: no LICENSE file included in tarball,  \\n - removed Version/Date from README,\\n - added test pot files to MANIFEST.in,\\n - performance improvement in find() method (thanks Thomas !).\\n\\nVersion 0.4.1 (2009/03/04)\\n--------------------------\\n - fixed issue #0006: plural msgstrs were saved unsorted,\\n - fixed issue #0008: long comment lines broke 'save()' method,\\n - removed performance shortcuts: they were in fact inefficient, I was mislead by the python profile module, kudos to Thomas for making me realise that,\\n - fixed issue #0010: wrong polib version number,\\n - fixed issue #0011: occurrences parsing is now more robust and can handle weird references formats (like in eToys OLPC po files),\\n - fixed issue #0012: improved merge() method.\\n\\nVersion 0.4.0 (2008/11/26)\\n--------------------------\\n - fixed bug #0005: percent_translated divide by 0 on empty po files,\\n - fixed bug #0004: occurrences that have hyphens are wrapped when they should not,\\n - changes in how encoding is handled,\\n - remove deprecation warnings for typo on \\\"occurrences\\\",\\n - added POEntry.__cmp__() method to sort entries like gettext does,\\n - fixed POEntry.transalated(),\\n - added a merge() method to POFile class, that behaves like the gettext msgmerge utility,\\n - obsolete entries are now written at the end of the file and with only msgid/msgstr like gettext does,\\n - fixed some bugs in mo files parsing,\\n - renamed quote/unquote functions to escape/unescape,\\n - various cosmetic changes.\\n\\nVersion 0.3.1 (2007/12/13)\\n--------------------------\\n - fixed bug #0002: typo on \\\"occurrences\\\",\\n - fixed bug #0003: mismatch in exception instance names,\\n - removed deprecation warnings,\\n - removed unused charset() method in POFile/MOFile objects,\\n - fixed bug in multibytes string length (added regression tests),\\n - fixed a bug in detect_encoding(),\\n - added a find() method to _BaseFile class,\\n - proper handling of quoting and unquoting,\\n - proper handling of multiline strings in metadata \\n\\nVersion 0.3.0 (2007/10/17)\\n--------------------------\\n - speed improvements,\\n - polib can now compile mo files,\\n - unicode support,\\n - fixed bug #0001: global name 'sorted' is not defined\\\" on python 2.3.\\n\\nVersion 0.1.0 (2006-08-08)\\n--------------------------\\nInitial release\\n\\n\", \"description_content_type\": null, \"summary\": \"A library to manipulate gettext files (po and mo files).\", \"latest_version\": \"1.2.0\", \"weekly_downloads\": 776470, \"description_cleaned\": \"polib\\nOverview\\npolib is a library to manipulate, create, modify gettext files (pot, po and mo\\nfiles). You can load existing files, iterate through it\\u2019s entries, add, modify\\nentries, comments or metadata, etc\\u2026 or create new po files from scratch.\\npolib supports out of the box any version of python ranging from 2.7 to latest\\n3.X version.\\npolib is pretty stable now and is used by many\\nopensource projects\\n.\\nThe project code and bugtracker is hosted on\\nGithub\\n.\\npolib is generously documented, you can\\nbrowse the documentation online\\n, a good start is to read\\nthe quickstart guide\\n.\\nInstallation\\nJust use\\npip\\n:\\n$\\npip\\ninstall\\npolib\\nBasic example\\nimport\\npolib\\npofile\\n=\\npolib\\n.\\npofile\\n(\\n'/path/to/pofile.po'\\n)\\nfor\\nentry\\nin\\npofile\\n:\\nprint\\n(\\nentry\\n.\\nmsgid\\n,\\nentry\\n.\\nmsgstr\\n)\\nChangelog\\nVersion 1.2.0 (2023/02/23)\\nAdded setter for the fuzzy property\\nEscape/unescape v, b and f\\nAdded path to error message processing parser symbols\\nEnsure empty previous values for msgid and msgstr are properly saved\\nFixed relative ordering of comments and translator comments\\nDo not count obsolete entries in fuzzy() method\\nClose files properly\\nFixed support for loading mo files from bytes object\\nFixed msgstr_plural comparison\\nSimplified python version check\\nUpdated docs\\nVersion 1.1.1 (2021/03/26)\\nStrip UTF-16 BOM (U+FEFF) from start of file\\nMessage context (msgctxt) support for MO files\\nDropped Python < 2.7 support\\nUpdated docs and README after switch from Bitbucket to Github\\nVersion 1.1.0 (2017/11/27)\\nFixed entries sorting when generating mo file (Fixes #78)\\nFixed find method (Fixes #84)\\nRefactored POEntry.__cmp__ method (this should fix issues #60, #65 and #79)\\nFixed duplicated entries when merging po file with pot file (fixes #68)\\nFixed \\u201cNone\\u201d string being outputted when polib deals with syntax error in string (Fixes issue #66)\\nAdded a fuzzy property to entries (Fixes #76)\\nTake into account the message context when comparing entries\\nLeave occurence untouched if line number is not a number (fixes #80 and #90)\\nFixed test for mo files that was failing because since gettext version 0.19.8.1, msgfmt skips the \\u201cPOT-Creation-Date\\u201d metadata entry (Fixes #86)\\nFixed major revision number comparison\\nDropped python 2.4 support\\nVersion 1.0.8 (2016/11/21)\\nFixed issue #70 (occurrences parsing for windows pathes)\\nFixed issue #71 (tcomment and flags not saved for obsolete entries)\\nFixed issue #72 (wrong metadata ordering)\\nFixed issue #73 (can\\u2019t always unpickle POFile)\\nUse natural sorting for additional headers (metadata)\\nFixed typos in various documents\\nVersion 1.0.7 (2015/07/08)\\nFixed bad parsing of indented msgstr_plural\\nFixed ordering of \\u201cLanguage\\u201d metadata entry\\nRemoved space after \\u201c#\\u201d in header if comment line is empty (like gettext tools)\\nFixed typos / grammar errors (thanks Jakub Wilk)\\nTake into account msgid_plural if needed when comparing entries (thanks Leonardo Constantino Oliveira)\\nFixed issue #63 (str() on a bytes instance when using python3) (thanks Jakub Wilk)\\nVersion 1.0.6 (2015/01/04)\\nWheel support\\nAdd missing \\u2018Language\\u2019 and \\u2018Plural-Forms\\u2019 to metadata ordering\\nMore accurate float operation for POFile.percent_translated()\\nVersion 1.0.5 (2014/08/22)\\nFixed issue #59: tokens variable referenced before assignment\\nImplemented feature request #56: line number information in PO entries\\nFixed issue #61: polib does not handle previous msgid on multilines properly\\nVersion 1.0.4 (2014/02/19)\\nFixed issue #43: improved check that determine if polib is dealing with a filepath or unicode content\\nFixed issue #44: polib now checks MO files revision number and throws an error if the number is unexpected\\nFixed issue #45: parse properly mo files with no header entry\\nFixed issue #47: added flags attribute for MOEntry to be consistent with POEntry\\nFixed issue #49: use integers rather than strings for msgstr_plural keys\\nFixed issue #51: if a PO file ends with a comment, polib adds a spurious empty entry at the end\\nFixed issue #52: bad magic number written on big endian platforms\\nFixed issue #53: added a __hash__() method to POEntry and MOEntry classes\\nFixed issue #54: use lowercase for state identifiers. This fixes issues with certain locales and string.lower()\\nFixed issue #58: use io.open() instead of codecs.open() because the latter doesn\\u2019t handle very well universal line endings\\nMake sure the mo file is closed at garbage collection, this prevents warnings on unclosed file when running tests with python >= 3.2\\nBetter way to test endianness\\npolib download URL is now on Pypi\\nVersion 1.0.3 (2013/02/09)\\nFixed issue #38: POFile.append() raised a duplicate exception when you tried to add a new entry with the same msgid and a different msgctxt (only when check_for_duplicates option is set to True)\\nFixed issue #39: Added __init__.py file for convenience\\nFixed issue #41: UnicodeDecodeError when running setup.py build on python3 with C locale\\npolib is now fully PEP8 compliant\\nSmall improvements: remove unused \\u201ctyp\\u201d var (thanks Rodrigo Silva), improved Makefile, Make sure _BaseFile.__contains__ returns a boolean value\\nVersion 1.0.2 (2012/10/23)\\nallow empty comments, flags or occurrences lines\\nVersion 1.0.1 (2012/09/11)\\nspeed up POFile.merge method (thanks @encukou)\\nallow comments starting with two \\u2018#\\u2019 characters (thanks @goibhniu)\\nVersion 1.0.0 (2012/06/08)\\nYeah\\u2026 after nearly 6 years, polib reaches the stable state :)\\nChanges and fixes in this release :\\npolib.pofile and polib.mofile functions can now return a custom class (thanks Craig Blaszczyk)\\npolib now can find the metadata entry no matter where it is located (thanks Fran\\u00e7ois Poirotte)\\nfixed issue #28 (IOError on reading obsolete \\u201cprevious msgid\\u201d entries) (thanks James Ni)\\nVersion 0.7.0 (2011/07/14)\\nThis version adds support for python 3 (thanks to Vinay Sajip).\\npolib now supports out-of-the-box any version of python ranging from 2.4 to latest 3.X version.\\npolib is now 5 years old ;) so the 0.7.X branch will be the last before the 1.X stable branch.\\nVersion 0.6.4 (2011/07/13)\\nBetter api, autodetected_encoding is no longer required to explicitly set the encoding (fixes issue #23),\\nFixed issue #24 Support indented PO files (thanks to Fran\\u00e7ois Poirotte).\\nVersion 0.6.3 (2011/02/19)\\nFixed issue #19 (Disappearing newline characters due to textwrap module),\\nensure wrapping works as expected.\\nVersion 0.6.2 (2011/02/09)\\nBackported textwrap.TextWrapper._wrap_chunks that has support for the drop_whitespace parameter added in Python 2.6 (Fixes #18: broken compatibility with python 2.5, thanks @jezdez).\\nVersion 0.6.1 (2011/02/09)\\nfixed regression that prevented POFile initialization from data to work (issue #17).\\nVersion 0.6.0 (2011/02/07)\\npolib is now\\nfully documented\\n,\\nswitched from doctests to unit tests to keep the polib.py file clean,\\nfixed issue #7 (wrapping issues, thanks @jezdez),\\nadded a __eq__ method to _BaseFile (thanks @kost BebiX),\\nhandle msgctxt correctly when compiling mo files,\\ncompiled mo files are now exactly the same as those compiled by msgfmt without using hash tables.\\nVersion 0.5.5 (2010/10/30)\\nRemoved multiline handling code, it was a mess and was the source of potential bugs like issue #11,\\nFixed typo in README and CHANGELOG, fixes issue #13.\\nVersion 0.5.4 (2010/10/02)\\nfixed an issue with detect_encoding(), in some cases it could return an invalid charset.\\nVersion 0.5.3 (2010/08/29)\\ncorrectly unescape lines containing both \\\\\\\\n and \\\\n (thanks to Martin Geisler),\\nfixed issue #6: __str__() methods are returning unicode instead of str,\\nfixed issue #8: POFile.merge error when an entry is obsolete in a .po, that this entry reappears in the .pot and that we merge the two,\\nadded support to instantiate POFile objects using data instead of file path (thanks to Diego B\\u00farigo Zacar\\u00e3o),\\nfixed issue #9: POFile.merge drop fuzzy attributes from translations (thanks to Tim Gerundt),\\nfixed issue #10: Finding entries with the same msgid and different context (msgctxt).\\nVersion 0.5.2 (2010/06/09)\\nfixed issue #1: untranslated_entries() also show fuzzy message,\\nwrite back the fuzzy header if present in the pofile,\\nadded support for previous msgctxt, previous msgid and previous msgid_plural comments (fixes issue #5),\\nbetter handling of lines wrapping.\\nVersion 0.5.1 (2009/12/14)\\nfixed issue #0025: setup.py requires CHANGELOG but it\\u2019s not present in polib-0.5.0-tar.gz\\nVersion 0.5.0 (2009/12/13)\\nfixed issue #0017: UnicodeDecodeError while writing a mo-file,\\nfixed issue #0018: implemented support for msgctxt,\\nfixed bug when compiling plural msgids/strs,\\nAPI docs are no longer included, hopefully next release will ship with sphinx documentation,\\nparse msg plural entries correctly when reading mo files,\\nfixed issue #0020 and #0021: added ability to check for duplicate when adding entries to po/mo files, this is optional and not enabled by default because it slows down considerably the library,\\nfixed issue #0022: unescaping code is insufficient,\\nfixed issue #0023: encoding error when saving mo file as po file (thanks to sebastien.sable for the patch !).\\nVersion 0.4.2 (2009/06/05)\\nfixed issue #0007: use the codecs module to open files,\\nfixed issue #0014: plural forms are not saved correctly in the mo file (thanks lorenzo.gil.sanchez for the patch),\\nfixed issue #0015: no LICENSE file included in tarball,\\nremoved Version/Date from README,\\nadded test pot files to MANIFEST.in,\\nperformance improvement in find() method (thanks Thomas !).\\nVersion 0.4.1 (2009/03/04)\\nfixed issue #0006: plural msgstrs were saved unsorted,\\nfixed issue #0008: long comment lines broke \\u2018save()\\u2019 method,\\nremoved performance shortcuts: they were in fact inefficient, I was mislead by the python profile module, kudos to Thomas for making me realise that,\\nfixed issue #0010: wrong polib version number,\\nfixed issue #0011: occurrences parsing is now more robust and can handle weird references formats (like in eToys OLPC po files),\\nfixed issue #0012: improved merge() method.\\nVersion 0.4.0 (2008/11/26)\\nfixed bug #0005: percent_translated divide by 0 on empty po files,\\nfixed bug #0004: occurrences that have hyphens are wrapped when they should not,\\nchanges in how encoding is handled,\\nremove deprecation warnings for typo on \\u201coccurrences\\u201d,\\nadded POEntry.__cmp__() method to sort entries like gettext does,\\nfixed POEntry.transalated(),\\nadded a merge() method to POFile class, that behaves like the gettext msgmerge utility,\\nobsolete entries are now written at the end of the file and with only msgid/msgstr like gettext does,\\nfixed some bugs in mo files parsing,\\nrenamed quote/unquote functions to escape/unescape,\\nvarious cosmetic changes.\\nVersion 0.3.1 (2007/12/13)\\nfixed bug #0002: typo on \\u201coccurrences\\u201d,\\nfixed bug #0003: mismatch in exception instance names,\\nremoved deprecation warnings,\\nremoved unused charset() method in POFile/MOFile objects,\\nfixed bug in multibytes string length (added regression tests),\\nfixed a bug in detect_encoding(),\\nadded a find() method to _BaseFile class,\\nproper handling of quoting and unquoting,\\nproper handling of multiline strings in metadata\\nVersion 0.3.0 (2007/10/17)\\nspeed improvements,\\npolib can now compile mo files,\\nunicode support,\\nfixed bug #0001: global name \\u2018sorted\\u2019 is not defined\\u201d on python 2.3.\\nVersion 0.1.0 (2006-08-08)\\nInitial release\"}, {\"name\": \"zarr\", \"description\": \"<div align=\\\"center\\\">\\n  <img src=\\\"https://raw.githubusercontent.com/zarr-developers/community/main/logos/logo2.png\\\"><br>\\n</div>\\n\\n# Zarr\\n\\n<table>\\n<tr>\\n  <td>Latest Release</td>\\n  <td>\\n    <a href=\\\"https://pypi.org/project/zarr/\\\">\\n    <img src=\\\"https://badge.fury.io/py/zarr.svg\\\" alt=\\\"latest release\\\" />\\n    </a>\\n  </td>\\n</tr>\\n  <td></td>\\n  <td>\\n    <a href=\\\"https://anaconda.org/anaconda/zarr/\\\">\\n    <img src=\\\"https://anaconda.org/conda-forge/zarr/badges/version.svg\\\" alt=\\\"latest release\\\" />\\n    </a>\\n</td>\\n</tr>\\n<tr>\\n  <td>Package Status</td>\\n  <td>\\n\\t\\t<a href=\\\"https://pypi.org/project/zarr/\\\">\\n\\t\\t<img src=\\\"https://img.shields.io/pypi/status/zarr.svg\\\" alt=\\\"status\\\" />\\n\\t\\t</a>\\n  </td>\\n</tr>\\n<tr>\\n  <td>License</td>\\n  <td>\\n    <a href=\\\"https://github.com/zarr-developers/zarr-python/blob/main/LICENSE.txt\\\">\\n    <img src=\\\"https://img.shields.io/pypi/l/zarr.svg\\\" alt=\\\"license\\\" />\\n    </a>\\n</td>\\n</tr>\\n<tr>\\n  <td>Build Status</td>\\n  <td>\\n    <a href=\\\"https://github.com/zarr-developers/zarr-python/blob/main/.github/workflows/python-package.yml\\\">\\n    <img src=\\\"https://github.com/zarr-developers/zarr-python/actions/workflows/python-package.yml/badge.svg\\\" alt=\\\"build status\\\" />\\n    </a>\\n  </td>\\n</tr>\\n<tr>\\n  <td>Pre-commit Status</td>\\n  <td>\\n    <a href=\\\"\\\"https://github.com/zarr-developers/zarr-python/blob/main/.pre-commit-config.yaml\\\">\\n    <img src=\\\"https://results.pre-commit.ci/badge/github/zarr-developers/zarr-python/main.svg\\\" alt=\\\"pre-commit status\\\" />\\n    </a>\\n  </td>\\n</tr>\\n\\n<tr>\\n  <td>Coverage</td>\\n  <td>\\n    <a href=\\\"https://codecov.io/gh/zarr-developers/zarr-python\\\">\\n    <img src=\\\"https://codecov.io/gh/zarr-developers/zarr-python/branch/main/graph/badge.svg\\\"/ alt=\\\"coverage\\\">\\n    </a>\\n  </td>\\n</tr>\\n<tr>\\n  <td>Downloads</td>\\n  <td>\\n    <a href=\\\"https://zarr.readthedocs.io\\\">\\n    <img src=\\\"https://pepy.tech/badge/zarr\\\" alt=\\\"pypi downloads\\\" />\\n    </a>\\n  </td>\\n</tr>\\n<tr>\\n\\t<td>Developer Chat</td>\\n\\t<td>\\n\\t\\t<a href=\\\"https://ossci.zulipchat.com/\\\">\\n\\t\\t<img src=\\\"https://img.shields.io/badge/zulip-join_chat-brightgreen.svg\\\" />\\n\\t\\t</a>\\n\\t</td>\\n</tr>\\n<tr>\\n\\t<td>Funding</td>\\n\\t<td>\\n\\t\\t<a href=\\\"https://chanzuckerberg.com/eoss/\\\">\\n\\t\\t\\t<img src=\\\"https://img.shields.io/badge/funded%20by-EOSS-FF414B.svg?logo=data:image/svg+xml;base64,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\\\" alt=\\\"CZI's Essential Open Source Software for Science\\\">\\n\\t\\t</a>\\n\\t</td>\\n</tr>\\n\\t<td>Citation</td>\\n\\t<td>\\n\\t\\t<a href=\\\"https://doi.org/10.5281/zenodo.3773450\\\">\\n\\t\\t\\t<img src=\\\"https://zenodo.org/badge/DOI/10.5281/zenodo.3773450.svg\\\" alt=\\\"DOI\\\">\\n\\t\\t</a>\\n\\t</td>\\n</tr>\\n\\n</table>\\n\\n## What is it?\\n\\nZarr is a Python package providing an implementation of compressed, chunked, N-dimensional arrays, designed for use in parallel computing. See the [documentation](https://zarr.readthedocs.io) for more information.\\n\\n## Main Features\\n\\n- [**Create**](https://zarr.readthedocs.io/en/stable/user-guide/arrays.html#creating-an-array) N-dimensional arrays with any NumPy `dtype`.\\n- [**Chunk arrays**](https://zarr.readthedocs.io/en/stable/user-guide/performance.html#chunk-optimizations) along any dimension.\\n- [**Compress**](https://zarr.readthedocs.io/en/stable/user-guide/arrays.html#compressors) and/or filter chunks using any NumCodecs codec.\\n- [**Store arrays**](https://zarr.readthedocs.io/en/stable/user-guide/storage.html) in memory, on disk, inside a zip file, on S3, etc...\\n- [**Read**](https://zarr.readthedocs.io/en/stable/user-guide/arrays.html#reading-and-writing-data) an array [**concurrently**](https://zarr.readthedocs.io/en/stable/user-guide/performance.html#parallel-computing-and-synchronization) from multiple threads or processes.\\n- [**Write**](https://zarr.readthedocs.io/en/stable/user-guide/arrays.html#reading-and-writing-data) to an array concurrently from multiple threads or processes.\\n- Organize arrays into hierarchies via [**groups**](https://zarr.readthedocs.io/en/stable/quickstart.html#hierarchical-groups).\\n\\n## Where to get it\\n\\nZarr can be installed from PyPI using `pip`:\\n\\n```bash\\npip install zarr\\n```\\n\\nor via `conda`:\\n\\n```bash\\nconda install -c conda-forge zarr\\n```\\n\\nFor more details, including how to install from source, see the [installation documentation](https://zarr.readthedocs.io/en/stable/index.html#installation).\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"An implementation of chunked, compressed, N-dimensional arrays for Python\", \"latest_version\": \"3.1.3\", \"weekly_downloads\": 775770, \"description_cleaned\": \"Zarr\\nLatest Release\\nPackage Status\\nLicense\\nBuild Status\\nPre-commit Status\\nCoverage\\nDownloads\\nDeveloper Chat\\nFunding\\nCitation\\nWhat is it?\\nZarr is a Python package providing an implementation of compressed, chunked, N-dimensional arrays, designed for use in parallel computing. See the\\ndocumentation\\nfor more information.\\nMain Features\\nCreate\\nN-dimensional arrays with any NumPy\\ndtype\\n.\\nChunk arrays\\nalong any dimension.\\nCompress\\nand/or filter chunks using any NumCodecs codec.\\nStore arrays\\nin memory, on disk, inside a zip file, on S3, etc...\\nRead\\nan array\\nconcurrently\\nfrom multiple threads or processes.\\nWrite\\nto an array concurrently from multiple threads or processes.\\nOrganize arrays into hierarchies via\\ngroups\\n.\\nWhere to get it\\nZarr can be installed from PyPI using\\npip\\n:\\npip\\ninstall\\nzarr\\nor via\\nconda\\n:\\nconda\\ninstall\\n-c\\nconda-forge\\nzarr\\nFor more details, including how to install from source, see the\\ninstallation documentation\\n.\"}, {\"name\": \"biotite\", \"description\": \".. image:: https://img.shields.io/pypi/v/biotite.svg\\n   :target: https://pypi.python.org/pypi/biotite\\n   :alt: Biotite at PyPI\\n.. image:: https://img.shields.io/pypi/pyversions/biotite.svg\\n   :alt: Python version\\n.. image:: https://github.com/biotite-dev/biotite/actions/workflows/test_and_deploy.yml/badge.svg\\n   :target: https://github.com/biotite-dev/biotite/actions/workflows/test_and_deploy.yml\\n   :alt: Test status\\n\\n.. image:: https://www.biotite-python.org/_static/assets/general/biotite_logo_m.png\\n   :alt: The Biotite Project\\n\\nBiotite project\\n===============\\n\\n*Biotite* is your Swiss army knife for bioinformatics.\\nWhether you want to identify homologous sequence regions in a protein family\\nor you would like to find disulfide bonds in a protein structure: *Biotite*\\nhas the right tool for you.\\nThis package bundles popular tasks in computational molecular biology\\ninto a uniform *Python* library.\\nIt can handle a major part of the typical workflow\\nfor sequence and biomolecular structure data:\\n\\n   - Searching and fetching data from biological databases\\n   - Reading and writing popular sequence/structure file formats\\n   - Analyzing and editing sequence/structure data\\n   - Visualizing sequence/structure data\\n   - Interfacing external applications for further analysis\\n\\n*Biotite* internally stores most of the data as *NumPy* `ndarray` objects,\\nenabling\\n\\n   - fast C-accelerated analysis,\\n   - intuitive usability through *NumPy*-like indexing syntax,\\n   - extensibility through direct access of the internal *NumPy* arrays.\\n\\nAs a result the user can skip writing code for basic functionality (like\\nfile parsers) and can focus on what their code makes unique - from\\nsmall analysis scripts to entire bioinformatics software packages.\\n\\nIf you use *Biotite* in a scientific publication, please cite:\\n\\n| Kunzmann, P. & Hamacher, K. BMC Bioinformatics (2018) 19:346.\\n| `<https://doi.org/10.1186/s12859-018-2367-z>`_\\n\\n\\nInstallation\\n------------\\n\\n*Biotite* requires the following packages:\\n\\n   - **numpy**\\n   - **requests**\\n   - **msgpack**\\n   - **networkx**\\n\\nSome functions require some extra packages:\\n\\n   - **matplotlib** - Required for plotting purposes.\\n\\n*Biotite* can be installed via *Conda*...\\n\\n.. code-block:: console\\n\\n   $ conda install -c conda-forge biotite\\n\\n... or *pip*\\n\\n.. code-block:: console\\n\\n   $ pip install biotite\\n\\n\\nUsage\\n-----\\n\\nHere is a small example that downloads two protein sequences from the\\n*NCBI Entrez* database and aligns them:\\n\\n.. code-block:: python\\n\\n   import biotite.sequence.align as align\\n   import biotite.sequence.io.fasta as fasta\\n   import biotite.database.entrez as entrez\\n\\n   # Download FASTA file for the sequences of avidin and streptavidin\\n   file_name = entrez.fetch_single_file(\\n       uids=[\\\"CAC34569\\\", \\\"ACL82594\\\"], file_name=\\\"sequences.fasta\\\",\\n       db_name=\\\"protein\\\", ret_type=\\\"fasta\\\"\\n   )\\n\\n   # Parse the downloaded FASTA file\\n   # and create 'ProteinSequence' objects from it\\n   fasta_file = fasta.FastaFile.read(file_name)\\n   avidin_seq, streptavidin_seq = fasta.get_sequences(fasta_file).values()\\n\\n   # Align sequences using the BLOSUM62 matrix with affine gap penalty\\n   matrix = align.SubstitutionMatrix.std_protein_matrix()\\n   alignments = align.align_optimal(\\n       avidin_seq, streptavidin_seq, matrix,\\n       gap_penalty=(-10, -1), terminal_penalty=False\\n   )\\n   print(alignments[0])\\n\\n.. code-block::\\n\\n   MVHATSPLLLLLLLSLALVAPGLSAR------KCSLTGKWDNDLGSNMTIGAVNSKGEFTGTYTTAV-TA\\n   -------------------DPSKESKAQAAVAEAGITGTWYNQLGSTFIVTA-NPDGSLTGTYESAVGNA\\n\\n   TSNEIKESPLHGTQNTINKRTQPTFGFTVNWKFS----ESTTVFTGQCFIDRNGKEV-LKTMWLLRSSVN\\n   ESRYVLTGRYDSTPATDGSGT--ALGWTVAWKNNYRNAHSATTWSGQYV---GGAEARINTQWLLTSGTT\\n\\n   DIGDDWKATRVGINIFTRLRTQKE---------------------\\n   -AANAWKSTLVGHDTFTKVKPSAASIDAAKKAGVNNGNPLDAVQQ\\n\\nMore documentation, including a tutorial, an example gallery and the API\\nreference is available at `<https://www.biotite-python.org/>`_.\\n\\n\\nContribution\\n------------\\n\\nInterested in improving *Biotite*?\\nHave a look at the\\n`contribution guidelines <https://www.biotite-python.org/latest/contribution/index.html>`_.\\nFeel free to join our community chat on `Discord <https://discord.gg/cUjDguF>`_.\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"A comprehensive library for computational molecular biology\", \"latest_version\": \"1.5.0\", \"weekly_downloads\": 775459, \"description_cleaned\": \"Biotite project\\nBiotite\\nis your Swiss army knife for bioinformatics.\\nWhether you want to identify homologous sequence regions in a protein family\\nor you would like to find disulfide bonds in a protein structure:\\nBiotite\\nhas the right tool for you.\\nThis package bundles popular tasks in computational molecular biology\\ninto a uniform\\nPython\\nlibrary.\\nIt can handle a major part of the typical workflow\\nfor sequence and biomolecular structure data:\\nSearching and fetching data from biological databases\\nReading and writing popular sequence/structure file formats\\nAnalyzing and editing sequence/structure data\\nVisualizing sequence/structure data\\nInterfacing external applications for further analysis\\nBiotite\\ninternally stores most of the data as\\nNumPy\\nndarray\\nobjects,\\nenabling\\nfast C-accelerated analysis,\\nintuitive usability through\\nNumPy\\n-like indexing syntax,\\nextensibility through direct access of the internal\\nNumPy\\narrays.\\nAs a result the user can skip writing code for basic functionality (like\\nfile parsers) and can focus on what their code makes unique - from\\nsmall analysis scripts to entire bioinformatics software packages.\\nIf you use\\nBiotite\\nin a scientific publication, please cite:\\nKunzmann, P. & Hamacher, K. BMC Bioinformatics (2018) 19:346.\\nhttps://doi.org/10.1186/s12859-018-2367-z\\nInstallation\\nBiotite\\nrequires the following packages:\\nnumpy\\nrequests\\nmsgpack\\nnetworkx\\nSome functions require some extra packages:\\nmatplotlib\\n- Required for plotting purposes.\\nBiotite\\ncan be installed via\\nConda\\n\\u2026\\n$\\nconda\\ninstall\\n-c\\nconda-forge\\nbiotite\\n\\u2026 or\\npip\\n$\\npip\\ninstall\\nbiotite\\nUsage\\nHere is a small example that downloads two protein sequences from the\\nNCBI Entrez\\ndatabase and aligns them:\\nimport\\nbiotite.sequence.align\\nas\\nalign\\nimport\\nbiotite.sequence.io.fasta\\nas\\nfasta\\nimport\\nbiotite.database.entrez\\nas\\nentrez\\n# Download FASTA file for the sequences of avidin and streptavidin\\nfile_name\\n=\\nentrez\\n.\\nfetch_single_file\\n(\\nuids\\n=\\n[\\n\\\"CAC34569\\\"\\n,\\n\\\"ACL82594\\\"\\n],\\nfile_name\\n=\\n\\\"sequences.fasta\\\"\\n,\\ndb_name\\n=\\n\\\"protein\\\"\\n,\\nret_type\\n=\\n\\\"fasta\\\"\\n)\\n# Parse the downloaded FASTA file\\n# and create 'ProteinSequence' objects from it\\nfasta_file\\n=\\nfasta\\n.\\nFastaFile\\n.\\nread\\n(\\nfile_name\\n)\\navidin_seq\\n,\\nstreptavidin_seq\\n=\\nfasta\\n.\\nget_sequences\\n(\\nfasta_file\\n)\\n.\\nvalues\\n()\\n# Align sequences using the BLOSUM62 matrix with affine gap penalty\\nmatrix\\n=\\nalign\\n.\\nSubstitutionMatrix\\n.\\nstd_protein_matrix\\n()\\nalignments\\n=\\nalign\\n.\\nalign_optimal\\n(\\navidin_seq\\n,\\nstreptavidin_seq\\n,\\nmatrix\\n,\\ngap_penalty\\n=\\n(\\n-\\n10\\n,\\n-\\n1\\n),\\nterminal_penalty\\n=\\nFalse\\n)\\nprint\\n(\\nalignments\\n[\\n0\\n])\\nMVHATSPLLLLLLLSLALVAPGLSAR------KCSLTGKWDNDLGSNMTIGAVNSKGEFTGTYTTAV-TA\\n-------------------DPSKESKAQAAVAEAGITGTWYNQLGSTFIVTA-NPDGSLTGTYESAVGNA\\n\\nTSNEIKESPLHGTQNTINKRTQPTFGFTVNWKFS----ESTTVFTGQCFIDRNGKEV-LKTMWLLRSSVN\\nESRYVLTGRYDSTPATDGSGT--ALGWTVAWKNNYRNAHSATTWSGQYV---GGAEARINTQWLLTSGTT\\n\\nDIGDDWKATRVGINIFTRLRTQKE---------------------\\n-AANAWKSTLVGHDTFTKVKPSAASIDAAKKAGVNNGNPLDAVQQ\\nMore documentation, including a tutorial, an example gallery and the API\\nreference is available at\\nhttps://www.biotite-python.org/\\n.\\nContribution\\nInterested in improving\\nBiotite\\n?\\nHave a look at the\\ncontribution guidelines\\n.\\nFeel free to join our community chat on\\nDiscord\\n.\"}, {\"name\": \"regress\", \"description\": \"===========\\n``regress``\\n===========\\n\\n|PyPI| |Pythons| |CI|\\n\\n.. |PyPI| image:: https://img.shields.io/pypi/v/regress.svg\\n  :alt: PyPI version\\n  :target: https://pypi.org/project/regress/\\n\\n.. |Pythons| image:: https://img.shields.io/pypi/pyversions/regress.svg\\n  :alt: Supported Python versions\\n  :target: https://pypi.org/project/regress/\\n\\n.. |CI| image:: https://github.com/Julian/regress/workflows/CI/badge.svg\\n  :alt: Build status\\n  :target: https://github.com/Julian/regress/actions?query=workflow%3ACI\\n\\n\\nPython bindings to the Rust `regress <https://docs.rs/regress/latest/regress/>`_ crate, exposing ECMA regular expressions.\\n\\n\\n.. code:: python\\n\\n    >>> from regress import Regex\\n    >>> regex = Regex(r\\\"\\\\d{4}\\\")\\n    >>> regex.find(\\\"2020-20-05\\\") is not None\\n    True\\n\\n\", \"description_content_type\": \"text/x-rst; charset=UTF-8\", \"summary\": \"Python bindings to Rust's regress ECMA regular expressions library\", \"latest_version\": \"2025.10.1\", \"weekly_downloads\": 775257, \"description_cleaned\": \"Python bindings to the Rust\\nregress\\ncrate, exposing ECMA regular expressions.\\n>>>\\nfrom\\nregress\\nimport\\nRegex\\n>>>\\nregex\\n=\\nRegex\\n(\\nr\\n\\\"\\\\d\\n{4}\\n\\\"\\n)\\n>>>\\nregex\\n.\\nfind\\n(\\n\\\"2020-20-05\\\"\\n)\\nis\\nnot\\nNone\\nTrue\"}, {\"name\": \"types-Pygments\", \"description\": \"## Typing stubs for Pygments\\n\\nThis is a [PEP 561](https://peps.python.org/pep-0561/) type stub package for\\nthe [`Pygments`](https://github.com/pygments/pygments) package. It can be used by type checkers\\nto check code that uses `Pygments`. This version of\\n`types-Pygments` aims to provide accurate annotations for\\n`Pygments==2.19.*`.\\n\\nThis stub package is marked as [partial](https://peps.python.org/pep-0561/#partial-stub-packages).\\nIf you find that annotations are missing, feel free to contribute and help complete them.\\n\\n\\nThis package is part of the [typeshed project](https://github.com/python/typeshed).\\nAll fixes for types and metadata should be contributed there.\\nSee [the README](https://github.com/python/typeshed/blob/main/README.md)\\nfor more details. The source for this package can be found in the\\n[`stubs/Pygments`](https://github.com/python/typeshed/tree/main/stubs/Pygments)\\ndirectory.\\n\\nThis package was tested with the following type checkers:\\n* [mypy](https://github.com/python/mypy/) 1.16.1\\n* [pyright](https://github.com/microsoft/pyright) 1.1.403\\n\\nIt was generated from typeshed commit\\n[`91ba0da4aad754c912a82b9e052cb4f8191ce520`](https://github.com/python/typeshed/commit/91ba0da4aad754c912a82b9e052cb4f8191ce520).\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"Typing stubs for Pygments\", \"latest_version\": \"2.19.0.20250809\", \"weekly_downloads\": 775017, \"description_cleaned\": \"Typing stubs for Pygments\\nThis is a\\nPEP 561\\ntype stub package for\\nthe\\nPygments\\npackage. It can be used by type checkers\\nto check code that uses\\nPygments\\n. This version of\\ntypes-Pygments\\naims to provide accurate annotations for\\nPygments==2.19.*\\n.\\nThis stub package is marked as\\npartial\\n.\\nIf you find that annotations are missing, feel free to contribute and help complete them.\\nThis package is part of the\\ntypeshed project\\n.\\nAll fixes for types and metadata should be contributed there.\\nSee\\nthe README\\nfor more details. The source for this package can be found in the\\nstubs/Pygments\\ndirectory.\\nThis package was tested with the following type checkers:\\nmypy\\n1.16.1\\npyright\\n1.1.403\\nIt was generated from typeshed commit\\n91ba0da4aad754c912a82b9e052cb4f8191ce520\\n.\"}, {\"name\": \"munch\", \"description\": \"[![Latest Version](https://img.shields.io/pypi/v/munch.svg)](https://pypi.python.org/pypi/munch/)\\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/munch.svg)](https://pypi.python.org/pypi/munch/)\\n[![Downloads](https://img.shields.io/pypi/dm/munch.svg)](https://pypi.python.org/pypi/munch/)\\n\\nmunch\\n==========\\n\\nInstallation\\n-------------\\n\\n```\\npip install munch\\n```\\n\\nUsage\\n-----\\n\\nmunch is a fork of David Schoonover's **Bunch** package, providing similar functionality. 99% of the work was done by him, and the fork was made mainly for lack of responsiveness for fixes and maintenance on the original code.\\n\\nMunch is a dictionary that supports attribute-style access, a la JavaScript:\\n\\n```python\\n\\n>>> from munch import Munch\\n>>> b = Munch()\\n>>> b.hello = 'world'\\n>>> b.hello\\n'world'\\n>>> b['hello'] += \\\"!\\\"\\n>>> b.hello\\n'world!'\\n>>> b.foo = Munch(lol=True)\\n>>> b.foo.lol\\nTrue\\n>>> b.foo is b['foo']\\nTrue\\n\\n```\\n\\n\\nDictionary Methods\\n------------------\\n\\nA Munch is a subclass of ``dict``; it supports all the methods a ``dict`` does:\\n\\n```python\\n\\n>>> list(b.keys())\\n['hello', 'foo']\\n\\n```\\n\\nIncluding ``update()``:\\n\\n```python\\n\\n>>> b.update({ 'ponies': 'are pretty!' }, hello=42)\\n>>> print(repr(b))\\nMunch({'hello': 42, 'foo': Munch({'lol': True}), 'ponies': 'are pretty!'})\\n\\n```\\n\\nAs well as iteration:\\n\\n```python\\n\\n>>> [ (k,b[k]) for k in b ]\\n[('hello', 42), ('foo', Munch({'lol': True})), ('ponies', 'are pretty!')]\\n\\n```\\n\\nAnd \\\"splats\\\":\\n\\n```python\\n\\n>>> \\\"The {knights} who say {ni}!\\\".format(**Munch(knights='lolcats', ni='can haz'))\\n'The lolcats who say can haz!'\\n\\n```\\n\\n\\nSerialization\\n-------------\\n\\nMunches happily and transparently serialize to JSON and YAML.\\n\\n```python\\n\\n>>> b = Munch(foo=Munch(lol=True), hello=42, ponies='are pretty!')\\n>>> import json\\n>>> json.dumps(b)\\n'{\\\"foo\\\": {\\\"lol\\\": true}, \\\"hello\\\": 42, \\\"ponies\\\": \\\"are pretty!\\\"}'\\n\\n```\\n\\nIf JSON support is present (``json`` or ``simplejson``), ``Munch`` will have a ``toJSON()`` method which returns the object as a JSON string.\\n\\nIf you have [PyYAML](http://pyyaml.org/wiki/PyYAML) installed, Munch attempts to register itself with the various YAML Representers so that Munches can be transparently dumped and loaded.\\n\\n```python\\n\\n>>> b = Munch(foo=Munch(lol=True), hello=42, ponies='are pretty!')\\n>>> import yaml\\n>>> yaml.dump(b)\\n'!munch.Munch\\\\nfoo: !munch.Munch\\\\n  lol: true\\\\nhello: 42\\\\nponies: are pretty!\\\\n'\\n>>> yaml.safe_dump(b)\\n'foo:\\\\n  lol: true\\\\nhello: 42\\\\nponies: are pretty!\\\\n'\\n\\n```\\n\\nIn addition, Munch instances will have a ``toYAML()`` method that returns the YAML string using ``yaml.safe_dump()``. This method also replaces ``__str__`` if present, as I find it far more readable. You can revert back to Python's default use of ``__repr__`` with a simple assignment: ``Munch.__str__ = Munch.__repr__``. The Munch class will also have a static method ``Munch.fromYAML()``, which loads a Munch out of a YAML string.\\n\\nFinally, Munch converts easily and recursively to (``unmunchify()``, ``Munch.toDict()``) and from (``munchify()``, ``Munch.fromDict()``) a normal ``dict``, making it easy to cleanly serialize them in other formats.\\n\\n\\nDefault Values\\n--------------\\n\\n``DefaultMunch`` instances return a specific default value when an attribute is missing from the collection. Like ``collections.defaultdict``, the first argument is the value to use for missing keys:\\n\\n```python\\n\\n>>> from munch import DefaultMunch\\n>>> undefined = object()\\n>>> b = DefaultMunch(undefined, {'hello': 'world!'})\\n>>> b.hello\\n'world!'\\n>>> b.foo is undefined\\nTrue\\n\\n```\\n\\n``DefaultMunch.fromDict()`` also takes the ``default`` argument:\\n\\n```python\\n\\n>>> undefined = object()\\n>>> b = DefaultMunch.fromDict({'recursively': {'nested': 'value'}}, undefined)\\n>>> b.recursively.nested == 'value'\\nTrue\\n>>> b.recursively.foo is undefined\\nTrue\\n\\n```\\n\\nOr you can use ``DefaultFactoryMunch`` to specify a factory for generating missing attributes. The first argument is the factory:\\n\\n```python\\n\\n>>> from munch import DefaultFactoryMunch\\n>>> b = DefaultFactoryMunch(list, {'hello': 'world!'})\\n>>> b.hello\\n'world!'\\n>>> b.foo\\n[]\\n>>> b.bar.append('hello')\\n>>> b.bar\\n['hello']\\n\\n```\\n\\n\\nMiscellaneous\\n-------------\\n\\n* It is safe to ``import *`` from this module. You'll get: ``Munch``, ``DefaultMunch``, ``DefaultFactoryMunch``, ``munchify`` and ``unmunchify``.\\n* Ample Tests. Just run ``pip install tox && tox`` from the project root.\\n\\nFeedback\\n--------\\n\\nOpen a ticket / fork the project on [GitHub](http://github.com/Infinidat/munch).\\n\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"A dot-accessible dictionary (a la JavaScript objects)\", \"latest_version\": \"4.0.0\", \"weekly_downloads\": 774497, \"description_cleaned\": \"munch\\nInstallation\\npip install munch\\nUsage\\nmunch is a fork of David Schoonover's\\nBunch\\npackage, providing similar functionality. 99% of the work was done by him, and the fork was made mainly for lack of responsiveness for fixes and maintenance on the original code.\\nMunch is a dictionary that supports attribute-style access, a la JavaScript:\\n>>>\\nfrom\\nmunch\\nimport\\nMunch\\n>>>\\nb\\n=\\nMunch\\n()\\n>>>\\nb\\n.\\nhello\\n=\\n'world'\\n>>>\\nb\\n.\\nhello\\n'world'\\n>>>\\nb\\n[\\n'hello'\\n]\\n+=\\n\\\"!\\\"\\n>>>\\nb\\n.\\nhello\\n'world!'\\n>>>\\nb\\n.\\nfoo\\n=\\nMunch\\n(\\nlol\\n=\\nTrue\\n)\\n>>>\\nb\\n.\\nfoo\\n.\\nlol\\nTrue\\n>>>\\nb\\n.\\nfoo\\nis\\nb\\n[\\n'foo'\\n]\\nTrue\\nDictionary Methods\\nA Munch is a subclass of\\ndict\\n; it supports all the methods a\\ndict\\ndoes:\\n>>>\\nlist\\n(\\nb\\n.\\nkeys\\n())\\n[\\n'hello'\\n,\\n'foo'\\n]\\nIncluding\\nupdate()\\n:\\n>>>\\nb\\n.\\nupdate\\n({\\n'ponies'\\n:\\n'are pretty!'\\n},\\nhello\\n=\\n42\\n)\\n>>>\\nprint\\n(\\nrepr\\n(\\nb\\n))\\nMunch\\n({\\n'hello'\\n:\\n42\\n,\\n'foo'\\n:\\nMunch\\n({\\n'lol'\\n:\\nTrue\\n}),\\n'ponies'\\n:\\n'are pretty!'\\n})\\nAs well as iteration:\\n>>>\\n[\\n(\\nk\\n,\\nb\\n[\\nk\\n])\\nfor\\nk\\nin\\nb\\n]\\n[(\\n'hello'\\n,\\n42\\n),\\n(\\n'foo'\\n,\\nMunch\\n({\\n'lol'\\n:\\nTrue\\n})),\\n(\\n'ponies'\\n,\\n'are pretty!'\\n)]\\nAnd \\\"splats\\\":\\n>>>\\n\\\"The\\n{knights}\\nwho say\\n{ni}\\n!\\\"\\n.\\nformat\\n(\\n**\\nMunch\\n(\\nknights\\n=\\n'lolcats'\\n,\\nni\\n=\\n'can haz'\\n))\\n'The lolcats who say can haz!'\\nSerialization\\nMunches happily and transparently serialize to JSON and YAML.\\n>>>\\nb\\n=\\nMunch\\n(\\nfoo\\n=\\nMunch\\n(\\nlol\\n=\\nTrue\\n),\\nhello\\n=\\n42\\n,\\nponies\\n=\\n'are pretty!'\\n)\\n>>>\\nimport\\njson\\n>>>\\njson\\n.\\ndumps\\n(\\nb\\n)\\n'{\\\"foo\\\": {\\\"lol\\\": true}, \\\"hello\\\": 42, \\\"ponies\\\": \\\"are pretty!\\\"}'\\nIf JSON support is present (\\njson\\nor\\nsimplejson\\n),\\nMunch\\nwill have a\\ntoJSON()\\nmethod which returns the object as a JSON string.\\nIf you have\\nPyYAML\\ninstalled, Munch attempts to register itself with the various YAML Representers so that Munches can be transparently dumped and loaded.\\n>>>\\nb\\n=\\nMunch\\n(\\nfoo\\n=\\nMunch\\n(\\nlol\\n=\\nTrue\\n),\\nhello\\n=\\n42\\n,\\nponies\\n=\\n'are pretty!'\\n)\\n>>>\\nimport\\nyaml\\n>>>\\nyaml\\n.\\ndump\\n(\\nb\\n)\\n'!munch.Munch\\n\\\\n\\nfoo: !munch.Munch\\n\\\\n\\nlol: true\\n\\\\n\\nhello: 42\\n\\\\n\\nponies: are pretty!\\n\\\\n\\n'\\n>>>\\nyaml\\n.\\nsafe_dump\\n(\\nb\\n)\\n'foo:\\n\\\\n\\nlol: true\\n\\\\n\\nhello: 42\\n\\\\n\\nponies: are pretty!\\n\\\\n\\n'\\nIn addition, Munch instances will have a\\ntoYAML()\\nmethod that returns the YAML string using\\nyaml.safe_dump()\\n. This method also replaces\\n__str__\\nif present, as I find it far more readable. You can revert back to Python's default use of\\n__repr__\\nwith a simple assignment:\\nMunch.__str__ = Munch.__repr__\\n. The Munch class will also have a static method\\nMunch.fromYAML()\\n, which loads a Munch out of a YAML string.\\nFinally, Munch converts easily and recursively to (\\nunmunchify()\\n,\\nMunch.toDict()\\n) and from (\\nmunchify()\\n,\\nMunch.fromDict()\\n) a normal\\ndict\\n, making it easy to cleanly serialize them in other formats.\\nDefault Values\\nDefaultMunch\\ninstances return a specific default value when an attribute is missing from the collection. Like\\ncollections.defaultdict\\n, the first argument is the value to use for missing keys:\\n>>>\\nfrom\\nmunch\\nimport\\nDefaultMunch\\n>>>\\nundefined\\n=\\nobject\\n()\\n>>>\\nb\\n=\\nDefaultMunch\\n(\\nundefined\\n,\\n{\\n'hello'\\n:\\n'world!'\\n})\\n>>>\\nb\\n.\\nhello\\n'world!'\\n>>>\\nb\\n.\\nfoo\\nis\\nundefined\\nTrue\\nDefaultMunch.fromDict()\\nalso takes the\\ndefault\\nargument:\\n>>>\\nundefined\\n=\\nobject\\n()\\n>>>\\nb\\n=\\nDefaultMunch\\n.\\nfromDict\\n({\\n'recursively'\\n:\\n{\\n'nested'\\n:\\n'value'\\n}},\\nundefined\\n)\\n>>>\\nb\\n.\\nrecursively\\n.\\nnested\\n==\\n'value'\\nTrue\\n>>>\\nb\\n.\\nrecursively\\n.\\nfoo\\nis\\nundefined\\nTrue\\nOr you can use\\nDefaultFactoryMunch\\nto specify a factory for generating missing attributes. The first argument is the factory:\\n>>>\\nfrom\\nmunch\\nimport\\nDefaultFactoryMunch\\n>>>\\nb\\n=\\nDefaultFactoryMunch\\n(\\nlist\\n,\\n{\\n'hello'\\n:\\n'world!'\\n})\\n>>>\\nb\\n.\\nhello\\n'world!'\\n>>>\\nb\\n.\\nfoo\\n[]\\n>>>\\nb\\n.\\nbar\\n.\\nappend\\n(\\n'hello'\\n)\\n>>>\\nb\\n.\\nbar\\n[\\n'hello'\\n]\\nMiscellaneous\\nIt is safe to\\nimport *\\nfrom this module. You'll get:\\nMunch\\n,\\nDefaultMunch\\n,\\nDefaultFactoryMunch\\n,\\nmunchify\\nand\\nunmunchify\\n.\\nAmple Tests. Just run\\npip install tox && tox\\nfrom the project root.\\nFeedback\\nOpen a ticket / fork the project on\\nGitHub\\n.\"}, {\"name\": \"tyro\", \"description\": \"<br />\\n<p align=\\\"center\\\">\\n    <!--\\n    This README will be used for both GitHub and PyPI. We therefore:\\n    - Keep all image URLs absolute.\\n    - In the GitHub action we use for publishing, strip some HTML tags that aren't supported by PyPI.\\n    -->\\n        <img alt=\\\"tyro logo\\\" src=\\\"https://brentyi.github.io/tyro/_static/logo-light.svg\\\" width=\\\"200px\\\" />\\n\\n</p>\\n\\n<p align=\\\"center\\\">\\n    <em><a href=\\\"https://brentyi.github.io/tyro\\\">Documentation</a></em>\\n    &nbsp;&nbsp;&bull;&nbsp;&nbsp;\\n    <em><code>pip install tyro</code></em>\\n</p>\\n\\n<p align=\\\"center\\\">\\n    <!-- <img alt=\\\"build\\\" src=\\\"https://github.com/brentyi/tyro/actions/workflows/build.yml/badge.svg\\\" /> -->\\n    <img alt=\\\"mypy\\\" src=\\\"https://github.com/brentyi/tyro/actions/workflows/mypy.yml/badge.svg\\\" />\\n    <img alt=\\\"pyright\\\" src=\\\"https://github.com/brentyi/tyro/actions/workflows/pyright.yml/badge.svg\\\" />\\n    <!-- <img alt=\\\"ruff\\\" src=\\\"https://github.com/brentyi/tyro/actions/workflows/ruff.yml/badge.svg\\\" /> -->\\n    <a href=\\\"https://codecov.io/gh/brentyi/tyro\\\">\\n        <img alt=\\\"codecov\\\" src=\\\"https://codecov.io/gh/brentyi/tyro/branch/main/graph/badge.svg\\\" />\\n    </a>\\n    <a href=\\\"https://pypi.org/project/tyro/\\\">\\n        <img alt=\\\"codecov\\\" src=\\\"https://img.shields.io/pypi/pyversions/tyro\\\" />\\n    </a>\\n</p>\\n\\n<br />\\n\\n<strong><code>tyro.cli()</code></strong> is a tool for generating CLI\\ninterfaces from type-annotated Python.\\n\\nWe can define configurable scripts using functions:\\n\\n```python\\n\\\"\\\"\\\"A command-line interface defined using a function signature.\\n\\nUsage: python script_name.py --foo INT [--bar STR]\\n\\\"\\\"\\\"\\n\\nimport tyro\\n\\ndef main(foo: int, bar: str = \\\"default\\\") -> None:\\n    ...  # Main body of a script.\\n\\nif __name__ == \\\"__main__\\\":\\n    # Generate a CLI and call `main` with its two arguments: `foo` and `bar`.\\n    tyro.cli(main)\\n```\\n\\nOr instantiate config objects defined using tools like `dataclasses`, `pydantic`, and `attrs`:\\n\\n```python\\n\\\"\\\"\\\"A command-line interface defined using a class signature.\\n\\nUsage: python script_name.py --foo INT [--bar STR]\\n\\\"\\\"\\\"\\n\\nfrom dataclasses import dataclass\\nimport tyro\\n\\n@dataclass\\nclass Config:\\n    foo: int\\n    bar: str = \\\"default\\\"\\n\\nif __name__ == \\\"__main__\\\":\\n    # Generate a CLI and instantiate `Config` with its two arguments: `foo` and `bar`.\\n    config = tyro.cli(Config)\\n\\n    # Rest of script.\\n    assert isinstance(config, Config)  # Should pass.\\n```\\n\\nOther features include helptext generation, nested structures, subcommands, and\\nshell completion. For examples and the API reference, see our\\n[documentation](https://brentyi.github.io/tyro).\\n\\n### Why `tyro`?\\n\\n1. **Define things once.** Standard Python type annotations, docstrings, and default values are parsed to automatically generate command-line interfaces with informative helptext.\\n\\n2. **Static types.** Unlike tools dependent on dictionaries, YAML, or dynamic\\n   namespaces, arguments populated by `tyro` benefit from IDE and language\\n   server-supported operations \\u2014 tab completion, rename, jump-to-def,\\n   docstrings on hover \\u2014 as well as static checking tools like `pyright` and\\n   `mypy`.\\n\\n3. **Modularity.** `tyro` supports hierarchical configuration structures, which\\n   make it easy to decentralize definitions, defaults, and documentation.\\n\\n### In the wild\\n\\n`tyro` is designed to be lightweight enough for throwaway scripts, while\\nfacilitating type safety and modularity for larger projects. Examples:\\n\\n<table>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/nerfstudio-project/nerfstudio/\\\">\\n        nerfstudio-project/nerfstudio\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/nerfstudio-project/nerfstudio?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>\\n      Open-source tools for neural radiance fields.\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/Sea-Snell/JAXSeq/\\\">\\n        Sea-Snell/JAXSeq\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/Sea-Snell/JAXSeq?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Train very large language models in Jax.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/kevinzakka/obj2mjcf\\\">\\n        kevinzakka/obj2mjcf\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/kevinzakka/obj2mjcf?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Interface for processing OBJ files for Mujoco.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/blurgyy/jaxngp\\\">\\n        blurgyy/jaxngp\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/blurgyy/jaxngp?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>\\n      CUDA-accelerated implementation of\\n      <a href=\\\"https://nvlabs.github.io/instant-ngp/\\\">instant-ngp</a>, in JAX.\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/NVIDIAGameWorks/kaolin-wisp\\\">\\n        NVIDIAGameWorks/kaolin-wisp\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/NVIDIAGameWorks/kaolin-wisp?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>PyTorch library for neural fields.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/autonomousvision/sdfstudio\\\">\\n        autonomousvision/sdfstudio\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/autonomousvision/sdfstudio?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Unified framework for surface reconstruction.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/openrlbenchmark/openrlbenchmark\\\">\\n        openrlbenchmark/openrlbenchmark\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/openrlbenchmark/openrlbenchmark?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Collection of tracked experiments for reinforcement learning.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/vwxyzjn/cleanrl\\\">\\n        vwxyzjn/cleanrl\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/vwxyzjn/cleanrl?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Single-file implementation of deep RL algorithms.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/pytorch-labs/LeanRL/\\\">\\n        pytorch-labs/LeanRL\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/pytorch-labs/LeanRL?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Fork of CleanRL, optimized using PyTorch 2 features.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/pytorch/torchtitan\\\">\\n        pytorch/torchtitan\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/pytorch/torchtitan?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>PyTorch-native platform for training generative AI models.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/KwaiVGI/LivePortrait\\\">\\n        KwaiVGI/LivePortrait\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/KwaiVGI/LivePortrait?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Stitching and retargeting for portraits.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/Physical-Intelligence/openpi/\\\">\\n        Physical-Intelligence/openpi\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/Physical-Intelligence/openpi?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Open-source models for robotics.</td>\\n  </tr>\\n  <tr>\\n    <td>\\n      <a href=\\\"https://github.com/MalcolmMielle/bark_monitor\\\">\\n        MalcolmMielle/bark_monitor\\n        <br /><img\\n          alt=\\\"GitHub stars\\\"\\n          src=\\\"https://img.shields.io/github/stars/MalcolmMielle/bark_monitor?style=social\\\"\\n        />\\n      </a>\\n    </td>\\n    <td>Show your neighbor that your dog doesn't bark!</td>\\n  </tr>\\n</table>\\n\\n### Alternatives\\n\\n`tyro` is an opinionated library. If any design decisions don't make sense,\\nfeel free to file an issue!\\n\\nYou might also consider one of many alternative libraries. Some that we\\nparticularly like:\\n- [cappa](https://github.com/dancardin/cappa), which offers a similar core feature\\n  set but with very different ergonomics. It looks polished and well-maintained!\\n- [cyclopts](https://github.com/BrianPugh/cyclopts) and\\n  [defopt](https://defopt.readthedocs.io/), which have very comprehensive type\\n  annotation support and a heavier emphasis on subcommand generation.\\n- [simple-parsing](https://github.com/lebrice/SimpleParsing) and\\n  [jsonargparse](https://github.com/omni-us/jsonargparse), which provide deeper\\n  integration with configuration file formats like YAML and JSON.\\n- [clipstick](https://github.com/sander76/clipstick), which focuses on\\n  simplicity + generating CLIs from Pydantic models.\\n- [datargs](https://github.com/roee30/datargs), which provides a minimal API for\\n  dataclasses.\\n- [fire](https://github.com/google/python-fire) and\\n  [clize](https://github.com/epsy/clize), which support arguments without type\\n  annotations.\\n\\nThere are also some options that directly build on and extend `tyro`:\\n- [mininterface](https://github.com/CZ-NIC/mininterface) simultaneously generates\\n  GUI, TUI, web, CLI, and file-based program configuration.\\n- [manuscript](https://github.com/stllfe/manuscript) generates CLI interfaces from\\n  a simple block of configuration variables.\\n\\n\\nWe also have some notes on `tyro`'s design goals and other alternatives in the\\ndocs [here](https://brentyi.github.io/tyro/goals_and_alternatives/).\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"CLI interfaces & config objects, from types\", \"latest_version\": \"0.10.0a5\", \"weekly_downloads\": 774091, \"description_cleaned\": \"Documentation\\n\\u2022\\npip install tyro\\ntyro.cli()\\nis a tool for generating CLI\\ninterfaces from type-annotated Python.\\nWe can define configurable scripts using functions:\\n\\\"\\\"\\\"A command-line interface defined using a function signature.\\nUsage: python script_name.py --foo INT [--bar STR]\\n\\\"\\\"\\\"\\nimport\\ntyro\\ndef\\nmain\\n(\\nfoo\\n:\\nint\\n,\\nbar\\n:\\nstr\\n=\\n\\\"default\\\"\\n)\\n->\\nNone\\n:\\n...\\n# Main body of a script.\\nif\\n__name__\\n==\\n\\\"__main__\\\"\\n:\\n# Generate a CLI and call `main` with its two arguments: `foo` and `bar`.\\ntyro\\n.\\ncli\\n(\\nmain\\n)\\nOr instantiate config objects defined using tools like\\ndataclasses\\n,\\npydantic\\n, and\\nattrs\\n:\\n\\\"\\\"\\\"A command-line interface defined using a class signature.\\nUsage: python script_name.py --foo INT [--bar STR]\\n\\\"\\\"\\\"\\nfrom\\ndataclasses\\nimport\\ndataclass\\nimport\\ntyro\\n@dataclass\\nclass\\nConfig\\n:\\nfoo\\n:\\nint\\nbar\\n:\\nstr\\n=\\n\\\"default\\\"\\nif\\n__name__\\n==\\n\\\"__main__\\\"\\n:\\n# Generate a CLI and instantiate `Config` with its two arguments: `foo` and `bar`.\\nconfig\\n=\\ntyro\\n.\\ncli\\n(\\nConfig\\n)\\n# Rest of script.\\nassert\\nisinstance\\n(\\nconfig\\n,\\nConfig\\n)\\n# Should pass.\\nOther features include helptext generation, nested structures, subcommands, and\\nshell completion. For examples and the API reference, see our\\ndocumentation\\n.\\nWhy\\ntyro\\n?\\nDefine things once.\\nStandard Python type annotations, docstrings, and default values are parsed to automatically generate command-line interfaces with informative helptext.\\nStatic types.\\nUnlike tools dependent on dictionaries, YAML, or dynamic\\nnamespaces, arguments populated by\\ntyro\\nbenefit from IDE and language\\nserver-supported operations \\u2014 tab completion, rename, jump-to-def,\\ndocstrings on hover \\u2014 as well as static checking tools like\\npyright\\nand\\nmypy\\n.\\nModularity.\\ntyro\\nsupports hierarchical configuration structures, which\\nmake it easy to decentralize definitions, defaults, and documentation.\\nIn the wild\\ntyro\\nis designed to be lightweight enough for throwaway scripts, while\\nfacilitating type safety and modularity for larger projects. Examples:\\nnerfstudio-project/nerfstudio\\nOpen-source tools for neural radiance fields.\\nSea-Snell/JAXSeq\\nTrain very large language models in Jax.\\nkevinzakka/obj2mjcf\\nInterface for processing OBJ files for Mujoco.\\nblurgyy/jaxngp\\nCUDA-accelerated implementation of\\ninstant-ngp\\n, in JAX.\\nNVIDIAGameWorks/kaolin-wisp\\nPyTorch library for neural fields.\\nautonomousvision/sdfstudio\\nUnified framework for surface reconstruction.\\nopenrlbenchmark/openrlbenchmark\\nCollection of tracked experiments for reinforcement learning.\\nvwxyzjn/cleanrl\\nSingle-file implementation of deep RL algorithms.\\npytorch-labs/LeanRL\\nFork of CleanRL, optimized using PyTorch 2 features.\\npytorch/torchtitan\\nPyTorch-native platform for training generative AI models.\\nKwaiVGI/LivePortrait\\nStitching and retargeting for portraits.\\nPhysical-Intelligence/openpi\\nOpen-source models for robotics.\\nMalcolmMielle/bark_monitor\\nShow your neighbor that your dog doesn't bark!\\nAlternatives\\ntyro\\nis an opinionated library. If any design decisions don't make sense,\\nfeel free to file an issue!\\nYou might also consider one of many alternative libraries. Some that we\\nparticularly like:\\ncappa\\n, which offers a similar core feature\\nset but with very different ergonomics. It looks polished and well-maintained!\\ncyclopts\\nand\\ndefopt\\n, which have very comprehensive type\\nannotation support and a heavier emphasis on subcommand generation.\\nsimple-parsing\\nand\\njsonargparse\\n, which provide deeper\\nintegration with configuration file formats like YAML and JSON.\\nclipstick\\n, which focuses on\\nsimplicity + generating CLIs from Pydantic models.\\ndatargs\\n, which provides a minimal API for\\ndataclasses.\\nfire\\nand\\nclize\\n, which support arguments without type\\nannotations.\\nThere are also some options that directly build on and extend\\ntyro\\n:\\nmininterface\\nsimultaneously generates\\nGUI, TUI, web, CLI, and file-based program configuration.\\nmanuscript\\ngenerates CLI interfaces from\\na simple block of configuration variables.\\nWe also have some notes on\\ntyro\\n's design goals and other alternatives in the\\ndocs\\nhere\\n.\"}, {\"name\": \"svglib\", \"description\": \".. -*- mode: rst -*-\\n\\n======\\nSvglib\\n======\\n\\n---------------------------------------------------------------------------\\nA pure-Python library for reading and converting SVG\\n---------------------------------------------------------------------------\\n\\n.. image:: https://github.com/deeplook/svglib/actions/workflows/ci-ubuntu.yml/badge.svg\\n   :target: https://github.com/deeplook/svglib/actions/workflows/ci-ubuntu.yml\\n\\n.. image:: https://results.pre-commit.ci/badge/github/deeplook/svglib/master.svg\\n  :target: https://results.pre-commit.ci/latest/github/deeplook/svglib/master\\n  :alt: pre-commit.ci status\\n\\n.. image:: https://pyup.io/repos/github/deeplook/svglib/shield.svg\\n  :target: https://pyup.io/repos/github/deeplook/svglib/\\n\\n.. image:: https://img.shields.io/pypi/implementation/svglib.svg\\n  :target: https://pypi.org/project/svglib\\n\\n.. image:: https://img.shields.io/pypi/pyversions/svglib.svg\\n  :target: https://pypi.org/project/svglib\\n\\n.. image:: https://img.shields.io/pypi/dm/svglib.svg\\n  :target: https://pepy.tech/project/svglib\\n\\n.. image:: https://img.shields.io/pypi/v/svglib.svg\\n  :target: https://pypi.org/project/svglib\\n\\n.. image:: https://img.shields.io/conda/vn/conda-forge/svglib.svg\\n  :target: https://github.com/conda-forge/svglib-feedstock\\n\\n.. image:: https://img.shields.io/conda/dn/conda-forge/svglib.svg\\n  :target: https://github.com/conda-forge/svglib-feedstock\\n\\n.. image:: https://img.shields.io/conda/pn/conda-forge/svglib.svg\\n  :target: https://pypi.org/project/svglib\\n\\n.. image:: https://img.shields.io/pypi/l/svglib.svg\\n  :target: https://pypi.org/project/svglib\\n\\n.. image:: https://static.streamlit.io/badges/streamlit_badge_black_white.svg\\n  :target: https://share.streamlit.io/deeplook/streamlit-svglib/master/streamlit_app.py\\n\\n\\nAbout\\n-----\\n\\n``Svglib`` is a Python library for reading SVG_ files and converting\\nthem (to a reasonable degree) to other formats using the ReportLab_ Open\\nSource toolkit.\\n\\nUsed as a package you can read existing SVG files and convert them into\\nReportLab ``Drawing`` objects that can be used in a variety of contexts,\\ne.g. as ReportLab Platypus ``Flowable`` objects or in RML_.\\nAs a command-line tool it converts SVG files into PDF ones (but adding\\nother output formats like bitmap or EPS is really easy and will be better\\nsupported, soon).\\n\\nTests include a huge `W3C SVG test suite`_ plus ca. 200 `flags from\\nWikipedia`_ and some selected `symbols from Wikipedia`_ (with increasingly\\nless pointing to missing features).\\n\\n\\nFeatures\\n--------\\n\\n- convert SVG_ files into ReportLab_ Graphics ``Drawing`` objects\\n- handle plain or compressed SVG files (.svg and .svgz)\\n- allow patterns for output files on command-line\\n- install a Python package named ``svglib``\\n- install a Python command-line script named ``svg2pdf``\\n- provide a PyTest_ test suite with over 90% code coverage\\n- test entire `W3C SVG test suite`_ after pulling from the internet\\n- test all SVG `flags from Wikipedia`_ after pulling from the internet\\n- test selected SVG `symbols from Wikipedia`_ after pulling from the net\\n- support Python 3.9+ and PyPy3\\n\\n\\nKnown limitations\\n-----------------\\n\\n- @import rules in stylesheets are ignored. CSS is supported, but the range\\n  of supported attributes is still limited\\n- clipping is limited to single paths, no mask support\\n- color gradients are not supported (limitation of reportlab)\\n- SVG ``ForeignObject`` elements are not supported.\\n\\n\\nExamples\\n--------\\n\\nYou can use ``svglib`` as a Python package e.g. like in the following\\ninteractive Python session:\\n\\n.. code:: python\\n\\n    >>> from svglib.svglib import svg2rlg\\n    >>> from reportlab.graphics import renderPDF, renderPM\\n    >>>\\n    >>> drawing = svg2rlg(\\\"file.svg\\\")\\n    >>> renderPDF.drawToFile(drawing, \\\"file.pdf\\\")\\n    >>> renderPM.drawToFile(drawing, \\\"file.png\\\", fmt=\\\"PNG\\\")\\n\\nNote that the second parameter of ``drawToFile`` can be any\\n`Python file object`_, like a ``BytesIO`` buffer if you don't want the result\\nto be written on disk for example.\\n\\nIn addition a script named ``svg2pdf`` can be used more easily from\\nthe system command-line. Here is the output from ``svg2pdf -h``::\\n\\n    usage: svg2pdf [-h] [-v] [-o PATH_PAT] [PATH [PATH ...]]\\n\\n    svg2pdf v. x.x.x\\n    A converter from SVG to PDF (via ReportLab Graphics)\\n\\n    positional arguments:\\n      PATH                  Input SVG file path with extension .svg or .svgz.\\n\\n    optional arguments:\\n      -h, --help            show this help message and exit\\n      -v, --version         Print version number and exit.\\n      -o PATH_PAT, --output PATH_PAT\\n                            Set output path (incl. the placeholders: dirname,\\n                            basename,base, ext, now) in both, %(name)s and {name}\\n                            notations.\\n\\n    examples:\\n      # convert path/file.svg to path/file.pdf\\n      svg2pdf path/file.svg\\n\\n      # convert file1.svg to file1.pdf and file2.svgz to file2.pdf\\n      svg2pdf file1.svg file2.svgz\\n\\n      # convert file.svg to out.pdf\\n      svg2pdf -o out.pdf file.svg\\n\\n      # convert all SVG files in path/ to PDF files with names like:\\n      # path/file1.svg -> file1.pdf\\n      svg2pdf -o \\\"%(base)s.pdf\\\" path/file*.svg\\n\\n      # like before but with timestamp in the PDF files:\\n      # path/file1.svg -> path/out-12-58-36-file1.pdf\\n      svg2pdf -o {{dirname}}/out-{{now.hour}}-{{now.minute}}-{{now.second}}-%(base)s.pdf path/file*.svg\\n\\n    issues/pull requests:\\n        https://github.com/deeplook/svglib\\n\\n\\nDependencies\\n------------\\n\\n``Svglib`` depends mainly on the ``reportlab`` package, which provides\\nthe abstractions for building complex ``Drawings`` which it can render\\ninto different fileformats, including PDF, EPS, SVG and various bitmaps\\nones. Other dependancies are ``lxml`` which is used in the context of SVG\\nCSS stylesheets.\\n\\nPrevious versions of this package included a way to run `cairo` without explicit\\ninstallation by the user; the dependency that took care of that no longer does\\nthis installation, and as such, the user must install `cairo` themselves. For\\ninstallation instructions, see the official website:\\nhttps://www.cairographics.org/download/\\n\\n\\nInstallation\\n------------\\n\\nThere are three ways to install ``svglib``.\\n\\n1. Using ``pip``\\n++++++++++++++++\\n\\nWith the ``pip`` command on your system and a working internet\\nconnection you can install the newest version of ``svglib`` with only\\none command in a terminal::\\n\\n    $ pip install svglib\\n\\nYou can also use ``pip`` to install the very latest version of the\\nrepository from GitHub, but then you won't be able to conveniently\\nrun the test suite::\\n\\n    $ pip install git+https://github.com/deeplook/svglib\\n\\n\\n2. Using ``conda``\\n++++++++++++++++++\\n\\nIf you use Anaconda_ or Miniconda_ you are surely using its respective package\\nmanager, Conda_, as well. In that case you should be able to install ``svglib``\\nusing these simple commands::\\n\\n    $ conda config --add channels conda-forge\\n    $ conda install svglib\\n\\n``Svglib`` was kindly packaged for ``conda`` by nicoddemus_. See here more about\\n`svglib with conda`_.\\n\\n\\n3. Manual installation\\n+++++++++++++++++++++++\\n\\nAlternatively, you can install a tarball like ``svglib-<version>.tar.gz``\\nafter downloading it from the `svglib page on PyPI`_ or the\\n`svglib releases page on GitHub`_ and installing it via ``pip``:\\n\\n    $ pip install svglib-<version>.tar.gz\\n\\nThis will install a Python package named ``svglib`` in the\\n``site-packages`` subfolder of your Python installation and a script\\ntool named ``svg2pdf`` in your ``bin`` directory, e.g. in\\n``/usr/local/bin``.\\n\\nDevelopment and Testing\\n-----------------------\\n\\nTo develop ``svglib``, install `uv`_ and run ``uv sync`` to install the\\nrequirements and development dependencies. To run the test suite with\\n``pytest``, run: ``uv run pytest``.\\n\\n.. _uv: https://docs.astral.sh/uv/\\n\\nDistribution Testing\\n--------------------\\n\\nThe ``svglib`` tarball distribution contains a PyTest_ test suite\\nin the ``tests`` directory. There, in ``tests/README.rst``, you can\\nalso read more about testing. You can run the testsuite e.g. like\\nshown in the following lines on the command-line::\\n\\n    $ make test\\n    ========================= test session starts =========================\\n    platform darwin -- Python 3.9.6, pytest-8.4.2, pluggy-1.6.0\\n    rootdir: /Users/dinu/dev/svglib\\n    configfile: pyproject.toml\\n    plugins: cov-7.0.0\\n    collected 115 items\\n\\n    tests/test_basic.py ............................................ [ 43%]\\n    ..................                                               [ 53%]\\n    tests/test_fonts.py ............s.....................s.....     [ 88%]\\n    tests/test_samples.py .s.s.s.s.....                              [100%]\\n\\n    ============== 109 passed, 6 skipped, 1 warning in 33.32s =============\\n\\n\\nBug reports\\n-----------\\n\\nPlease report bugs on the `svglib issue tracker`_ on GitHub (pull\\nrequests are also appreciated)!\\nIf necessary, please include information about the operating system, as\\nwell as the versions of ``svglib``, ReportLab and Python being used!\\n\\n\\n.. _SVG: http://www.w3.org/Graphics/SVG/\\n.. _W3C SVG test suite:\\n      http://www.w3.org/Graphics/SVG/WG/wiki/Test_Suite_Overview\\n.. _flags from Wikipedia:\\n      https://en.wikipedia.org/wiki/Gallery_of_sovereign_state_flags\\n.. _symbols from Wikipedia:\\n      https://en.wikipedia.org/wiki/List_of_symbols\\n.. _ReportLab: https://www.reportlab.com/opensource/\\n.. _RML: https://www.reportlab.com/software/rml-reference/\\n.. _svglib issue tracker: https://github.com/deeplook/svglib/issues\\n.. _PyTest: http://pytest.org\\n.. _svglib page on PyPI: https://pypi.org/project/svglib/\\n.. _svglib releases page on GitHub: https://github.com/deeplook/svglib/releases\\n.. _Python file object: https://docs.python.org/3/glossary.html#term-file-object\\n.. _Anaconda: https://www.anaconda.com/download/\\n.. _Miniconda: https://conda.io/miniconda.html\\n.. _Conda: https://conda.io\\n.. _svglib with conda: https://github.com/conda-forge/svglib-feedstock\\n.. _nicoddemus: https://github.com/nicoddemus\\n\", \"description_content_type\": \"text/x-rst\", \"summary\": \"A pure-Python library for reading and converting SVG\", \"latest_version\": \"1.6.0\", \"weekly_downloads\": 774045, \"description_cleaned\": \"About\\nSvglib\\nis a Python library for reading\\nSVG\\nfiles and converting\\nthem (to a reasonable degree) to other formats using the\\nReportLab\\nOpen\\nSource toolkit.\\nUsed as a package you can read existing SVG files and convert them into\\nReportLab\\nDrawing\\nobjects that can be used in a variety of contexts,\\ne.g. as ReportLab Platypus\\nFlowable\\nobjects or in\\nRML\\n.\\nAs a command-line tool it converts SVG files into PDF ones (but adding\\nother output formats like bitmap or EPS is really easy and will be better\\nsupported, soon).\\nTests include a huge\\nW3C SVG test suite\\nplus ca. 200\\nflags from\\nWikipedia\\nand some selected\\nsymbols from Wikipedia\\n(with increasingly\\nless pointing to missing features).\\nFeatures\\nconvert\\nSVG\\nfiles into\\nReportLab\\nGraphics\\nDrawing\\nobjects\\nhandle plain or compressed SVG files (.svg and .svgz)\\nallow patterns for output files on command-line\\ninstall a Python package named\\nsvglib\\ninstall a Python command-line script named\\nsvg2pdf\\nprovide a\\nPyTest\\ntest suite with over 90% code coverage\\ntest entire\\nW3C SVG test suite\\nafter pulling from the internet\\ntest all SVG\\nflags from Wikipedia\\nafter pulling from the internet\\ntest selected SVG\\nsymbols from Wikipedia\\nafter pulling from the net\\nsupport Python 3.9+ and PyPy3\\nKnown limitations\\n@import rules in stylesheets are ignored. CSS is supported, but the range\\nof supported attributes is still limited\\nclipping is limited to single paths, no mask support\\ncolor gradients are not supported (limitation of reportlab)\\nSVG\\nForeignObject\\nelements are not supported.\\nExamples\\nYou can use\\nsvglib\\nas a Python package e.g. like in the following\\ninteractive Python session:\\n>>>\\nfrom\\nsvglib.svglib\\nimport\\nsvg2rlg\\n>>>\\nfrom\\nreportlab.graphics\\nimport\\nrenderPDF\\n,\\nrenderPM\\n>>>\\n>>>\\ndrawing\\n=\\nsvg2rlg\\n(\\n\\\"file.svg\\\"\\n)\\n>>>\\nrenderPDF\\n.\\ndrawToFile\\n(\\ndrawing\\n,\\n\\\"file.pdf\\\"\\n)\\n>>>\\nrenderPM\\n.\\ndrawToFile\\n(\\ndrawing\\n,\\n\\\"file.png\\\"\\n,\\nfmt\\n=\\n\\\"PNG\\\"\\n)\\nNote that the second parameter of\\ndrawToFile\\ncan be any\\nPython file object\\n, like a\\nBytesIO\\nbuffer if you don\\u2019t want the result\\nto be written on disk for example.\\nIn addition a script named\\nsvg2pdf\\ncan be used more easily from\\nthe system command-line. Here is the output from\\nsvg2pdf\\n-h\\n:\\nusage: svg2pdf [-h] [-v] [-o PATH_PAT] [PATH [PATH ...]]\\n\\nsvg2pdf v. x.x.x\\nA converter from SVG to PDF (via ReportLab Graphics)\\n\\npositional arguments:\\n  PATH                  Input SVG file path with extension .svg or .svgz.\\n\\noptional arguments:\\n  -h, --help            show this help message and exit\\n  -v, --version         Print version number and exit.\\n  -o PATH_PAT, --output PATH_PAT\\n                        Set output path (incl. the placeholders: dirname,\\n                        basename,base, ext, now) in both, %(name)s and {name}\\n                        notations.\\n\\nexamples:\\n  # convert path/file.svg to path/file.pdf\\n  svg2pdf path/file.svg\\n\\n  # convert file1.svg to file1.pdf and file2.svgz to file2.pdf\\n  svg2pdf file1.svg file2.svgz\\n\\n  # convert file.svg to out.pdf\\n  svg2pdf -o out.pdf file.svg\\n\\n  # convert all SVG files in path/ to PDF files with names like:\\n  # path/file1.svg -> file1.pdf\\n  svg2pdf -o \\\"%(base)s.pdf\\\" path/file*.svg\\n\\n  # like before but with timestamp in the PDF files:\\n  # path/file1.svg -> path/out-12-58-36-file1.pdf\\n  svg2pdf -o {{dirname}}/out-{{now.hour}}-{{now.minute}}-{{now.second}}-%(base)s.pdf path/file*.svg\\n\\nissues/pull requests:\\n    https://github.com/deeplook/svglib\\nDependencies\\nSvglib\\ndepends mainly on the\\nreportlab\\npackage, which provides\\nthe abstractions for building complex\\nDrawings\\nwhich it can render\\ninto different fileformats, including PDF, EPS, SVG and various bitmaps\\nones. Other dependancies are\\nlxml\\nwhich is used in the context of SVG\\nCSS stylesheets.\\nPrevious versions of this package included a way to run\\ncairo\\nwithout explicit\\ninstallation by the user; the dependency that took care of that no longer does\\nthis installation, and as such, the user must install\\ncairo\\nthemselves. For\\ninstallation instructions, see the official website:\\nhttps://www.cairographics.org/download/\\nInstallation\\nThere are three ways to install\\nsvglib\\n.\\n1. Using\\npip\\nWith the\\npip\\ncommand on your system and a working internet\\nconnection you can install the newest version of\\nsvglib\\nwith only\\none command in a terminal:\\n$ pip install svglib\\nYou can also use\\npip\\nto install the very latest version of the\\nrepository from GitHub, but then you won\\u2019t be able to conveniently\\nrun the test suite:\\n$ pip install git+https://github.com/deeplook/svglib\\n2. Using\\nconda\\nIf you use\\nAnaconda\\nor\\nMiniconda\\nyou are surely using its respective package\\nmanager,\\nConda\\n, as well. In that case you should be able to install\\nsvglib\\nusing these simple commands:\\n$ conda config --add channels conda-forge\\n$ conda install svglib\\nSvglib\\nwas kindly packaged for\\nconda\\nby\\nnicoddemus\\n. See here more about\\nsvglib with conda\\n.\\n3. Manual installation\\nAlternatively, you can install a tarball like\\nsvglib-<version>.tar.gz\\nafter downloading it from the\\nsvglib page on PyPI\\nor the\\nsvglib releases page on GitHub\\nand installing it via\\npip\\n:\\n$ pip install svglib-<version>.tar.gz\\nThis will install a Python package named\\nsvglib\\nin the\\nsite-packages\\nsubfolder of your Python installation and a script\\ntool named\\nsvg2pdf\\nin your\\nbin\\ndirectory, e.g. in\\n/usr/local/bin\\n.\\nDevelopment and Testing\\nTo develop\\nsvglib\\n, install\\nuv\\nand run\\nuv sync\\nto install the\\nrequirements and development dependencies. To run the test suite with\\npytest\\n, run:\\nuv run pytest\\n.\\nDistribution Testing\\nThe\\nsvglib\\ntarball distribution contains a\\nPyTest\\ntest suite\\nin the\\ntests\\ndirectory. There, in\\ntests/README.rst\\n, you can\\nalso read more about testing. You can run the testsuite e.g. like\\nshown in the following lines on the command-line:\\n$ make test\\n========================= test session starts =========================\\nplatform darwin -- Python 3.9.6, pytest-8.4.2, pluggy-1.6.0\\nrootdir: /Users/dinu/dev/svglib\\nconfigfile: pyproject.toml\\nplugins: cov-7.0.0\\ncollected 115 items\\n\\ntests/test_basic.py ............................................ [ 43%]\\n..................                                               [ 53%]\\ntests/test_fonts.py ............s.....................s.....     [ 88%]\\ntests/test_samples.py .s.s.s.s.....                              [100%]\\n\\n============== 109 passed, 6 skipped, 1 warning in 33.32s =============\\nBug reports\\nPlease report bugs on the\\nsvglib issue tracker\\non GitHub (pull\\nrequests are also appreciated)!\\nIf necessary, please include information about the operating system, as\\nwell as the versions of\\nsvglib\\n, ReportLab and Python being used!\"}, {\"name\": \"pydruid\", \"description\": \"# pydruid\\n\\npydruid exposes a simple API to create, execute, and analyze [Druid](http://druid.io/) queries. pydruid can parse query results into [Pandas](http://pandas.pydata.org/) DataFrame objects for subsequent data analysis -- this offers a tight integration between [Druid](http://druid.io/), the [SciPy](http://www.scipy.org/stackspec.html) stack (for scientific computing) and [scikit-learn](http://scikit-learn.org/stable/) (for machine learning). pydruid can export query results into TSV or JSON for further processing with your favorite tool, e.g., R, Julia, Matlab, Excel. It provides both synchronous and asynchronous clients.\\n\\nAdditionally, pydruid implements the [Python DB API 2.0](https://www.python.org/dev/peps/pep-0249/), a [SQLAlchemy dialect](http://docs.sqlalchemy.org/en/latest/dialects/), and a provides a command line interface to interact with Druid.\\n\\nTo install:\\n```python\\npip install pydruid\\n# or, if you intend to use asynchronous client\\npip install pydruid[async]\\n# or, if you intend to export query results into pandas\\npip install pydruid[pandas]\\n# or, if you intend to do both\\npip install pydruid[async, pandas]\\n# or, if you want to use the SQLAlchemy engine\\npip install pydruid[sqlalchemy]\\n# or, if you want to use the CLI\\npip install pydruid[cli]\\n```\\nDocumentation: https://pythonhosted.org/pydruid/.\\n\\n# examples\\n\\nThe following exampes show how to execute and analyze the results of three types of queries: timeseries, topN, and groupby. We will use these queries to ask simple questions about twitter's public data set.\\n\\n## timeseries\\n\\nWhat was the average tweet length, per day, surrounding the 2014 Sochi olympics?\\n\\n```python\\nfrom pydruid.client import *\\nfrom pylab import plt\\n\\nquery = PyDruid(druid_url_goes_here, 'druid/v2')\\n\\nts = query.timeseries(\\n    datasource='twitterstream',\\n    granularity='day',\\n    intervals='2014-02-02/p4w',\\n    aggregations={'length': doublesum('tweet_length'), 'count': doublesum('count')},\\n    post_aggregations={'avg_tweet_length': (Field('length') / Field('count'))},\\n    filter=Dimension('first_hashtag') == 'sochi2014'\\n)\\ndf = query.export_pandas()\\ndf['timestamp'] = df['timestamp'].map(lambda x: x.split('T')[0])\\ndf.plot(x='timestamp', y='avg_tweet_length', ylim=(80, 140), rot=20,\\n        title='Sochi 2014')\\nplt.ylabel('avg tweet length (chars)')\\nplt.show()\\n```\\n\\n![alt text](https://github.com/metamx/pydruid/raw/master/docs/figures/avg_tweet_length.png \\\"Avg. tweet length\\\")\\n\\n## topN\\n\\nWho were the top ten mentions (@user_name) during the 2014 Oscars?\\n\\n```python\\ntop = query.topn(\\n    datasource='twitterstream',\\n    granularity='all',\\n    intervals='2014-03-03/p1d',  # utc time of 2014 oscars\\n    aggregations={'count': doublesum('count')},\\n    dimension='user_mention_name',\\n    filter=(Dimension('user_lang') == 'en') & (Dimension('first_hashtag') == 'oscars') &\\n           (Dimension('user_time_zone') == 'Pacific Time (US & Canada)') &\\n           ~(Dimension('user_mention_name') == 'No Mention'),\\n    metric='count',\\n    threshold=10\\n)\\n\\ndf = query.export_pandas()\\nprint df\\n\\n   count                 timestamp user_mention_name\\n0   1303  2014-03-03T00:00:00.000Z      TheEllenShow\\n1     44  2014-03-03T00:00:00.000Z        TheAcademy\\n2     21  2014-03-03T00:00:00.000Z               MTV\\n3     21  2014-03-03T00:00:00.000Z         peoplemag\\n4     17  2014-03-03T00:00:00.000Z               THR\\n5     16  2014-03-03T00:00:00.000Z      ItsQueenElsa\\n6     16  2014-03-03T00:00:00.000Z           eonline\\n7     15  2014-03-03T00:00:00.000Z       PerezHilton\\n8     14  2014-03-03T00:00:00.000Z     realjohngreen\\n9     12  2014-03-03T00:00:00.000Z       KevinSpacey\\n\\n```\\n\\n## groupby\\n\\nWhat does the social network of users replying to other users look like?\\n\\n```python\\nfrom igraph import *\\nfrom cairo import *\\nfrom pandas import concat\\n\\ngroup = query.groupby(\\n    datasource='twitterstream',\\n    granularity='hour',\\n    intervals='2013-10-04/pt12h',\\n    dimensions=[\\\"user_name\\\", \\\"reply_to_name\\\"],\\n    filter=(~(Dimension(\\\"reply_to_name\\\") == \\\"Not A Reply\\\")) &\\n           (Dimension(\\\"user_location\\\") == \\\"California\\\"),\\n    aggregations={\\\"count\\\": doublesum(\\\"count\\\")}\\n)\\n\\ndf = query.export_pandas()\\n\\n# map names to categorical variables with a lookup table\\nnames = concat([df['user_name'], df['reply_to_name']]).unique()\\nnameLookup = dict([pair[::-1] for pair in enumerate(names)])\\ndf['user_name_lookup'] = df['user_name'].map(nameLookup.get)\\ndf['reply_to_name_lookup'] = df['reply_to_name'].map(nameLookup.get)\\n\\n# create the graph with igraph\\ng = Graph(len(names), directed=False)\\nvertices = zip(df['user_name_lookup'], df['reply_to_name_lookup'])\\ng.vs[\\\"name\\\"] = names\\ng.add_edges(vertices)\\nlayout = g.layout_fruchterman_reingold()\\nplot(g, \\\"tweets.png\\\", layout=layout, vertex_size=2, bbox=(400, 400), margin=25, edge_width=1, vertex_color=\\\"blue\\\")\\n```\\n\\n![alt text](https://github.com/metamx/pydruid/raw/master/docs/figures/twitter_graph.png \\\"Social Network\\\")\\n\\n# asynchronous client\\n```pydruid.async_client.AsyncPyDruid``` implements an asynchronous client. To achieve that, it utilizes an asynchronous\\nHTTP client from ```Tornado``` framework. The asynchronous client is suitable for use with async frameworks such as Tornado\\nand provides much better performance at scale. It lets you serve multiple requests at the same time, without blocking on\\nDruid executing your queries.\\n\\n## example\\n```python\\nfrom tornado import gen\\nfrom pydruid.async_client import AsyncPyDruid\\nfrom pydruid.utils.aggregators import longsum\\nfrom pydruid.utils.filters import Dimension\\n\\nclient = AsyncPyDruid(url_to_druid_broker, 'druid/v2')\\n\\n@gen.coroutine\\ndef your_asynchronous_method_serving_top10_mentions_for_day(day\\n    top_mentions = yield client.topn(\\n        datasource='twitterstream',\\n        granularity='all',\\n        intervals=\\\"%s/p1d\\\" % (day, ),\\n        aggregations={'count': doublesum('count')},\\n        dimension='user_mention_name',\\n        filter=(Dimension('user_lang') == 'en') & (Dimension('first_hashtag') == 'oscars') &\\n               (Dimension('user_time_zone') == 'Pacific Time (US & Canada)') &\\n               ~(Dimension('user_mention_name') == 'No Mention'),\\n        metric='count',\\n        threshold=10)\\n\\n    # asynchronously return results\\n    # can be simply ```return top_mentions``` in python 3.x\\n    raise gen.Return(top_mentions)\\n```\\n\\n\\n# thetaSketches\\nTheta sketch Post aggregators are built slightly differently to normal Post Aggregators, as they have different operators.\\nNote: you must have the ```druid-datasketches``` extension loaded into your Druid cluster in order to use these.\\nSee the [Druid datasketches](http://druid.io/docs/latest/development/extensions-core/datasketches-aggregators.html) documentation for details.\\n\\n```python\\nfrom pydruid.client import *\\nfrom pydruid.utils import aggregators\\nfrom pydruid.utils import filters\\nfrom pydruid.utils import postaggregator\\n\\nquery = PyDruid(url_to_druid_broker, 'druid/v2')\\nts = query.groupby(\\n    datasource='test_datasource',\\n    granularity='all',\\n    intervals='2016-09-01/P1M',\\n    filter = ( filters.Dimension('product').in_(['product_A', 'product_B'])),\\n    aggregations={\\n        'product_A_users': aggregators.filtered(\\n            filters.Dimension('product') == 'product_A',\\n            aggregators.thetasketch('user_id')\\n            ),\\n        'product_B_users': aggregators.filtered(\\n            filters.Dimension('product') == 'product_B',\\n            aggregators.thetasketch('user_id')\\n            )\\n    },\\n    post_aggregations={\\n        'both_A_and_B': postaggregator.ThetaSketchEstimate(\\n            postaggregator.ThetaSketch('product_A_users') & postaggregator.ThetaSketch('product_B_users')\\n            )\\n    }\\n)\\n```\\n\\n# DB API\\n\\n```python\\nfrom pydruid.db import connect\\n\\nconn = connect(host='localhost', port=8082, path='/druid/v2/sql/', scheme='http')\\ncurs = conn.cursor()\\ncurs.execute(\\\"\\\"\\\"\\n    SELECT place,\\n           CAST(REGEXP_EXTRACT(place, '(.*),', 1) AS FLOAT) AS lat,\\n           CAST(REGEXP_EXTRACT(place, ',(.*)', 1) AS FLOAT) AS lon\\n      FROM places\\n     LIMIT 10\\n\\\"\\\"\\\")\\nfor row in curs:\\n    print(row)\\n```\\n\\n# SQLAlchemy\\n\\n```python\\nfrom sqlalchemy import *\\nfrom sqlalchemy.engine import create_engine\\nfrom sqlalchemy.schema import *\\n\\nengine = create_engine('druid://localhost:8082/druid/v2/sql/')  # uses HTTP by default :(\\n# engine = create_engine('druid+http://localhost:8082/druid/v2/sql/')\\n# engine = create_engine('druid+https://localhost:8082/druid/v2/sql/')\\n\\nplaces = Table('places', MetaData(bind=engine), autoload=True)\\nprint(select([func.count('*')], from_obj=places).scalar())\\n```\\n\\n\\n## Column headers\\n\\nIn version 0.13.0 Druid SQL added support for including the column names in the\\nresponse which can be requested via the \\\"header\\\" field in the request. This\\nhelps to ensure that the cursor description is defined (which is a requirement\\nfor SQLAlchemy query statements) regardless on whether the result set contains\\nany rows. Historically this was problematic for result sets which contained no\\nrows at one could not infer the expected column names.\\n\\nEnabling the header can be configured via the SQLAlchemy URI by using the query\\nparameter, i.e.,\\n\\n```python\\nengine = create_engine('druid://localhost:8082/druid/v2/sql?header=true')\\n```\\n\\nNote the current default is `false` to ensure backwards compatibility but should\\nbe set to `true` for Druid versions >= 0.13.0.\\n\\n\\n# Command line\\n\\n```bash\\n$ pydruid http://localhost:8082/druid/v2/sql/\\n> SELECT COUNT(*) AS cnt FROM places\\n  cnt\\n-----\\n12345\\n> SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES;\\nTABLE_NAME\\n----------\\ntest_table\\nCOLUMNS\\nSCHEMATA\\nTABLES\\n> BYE;\\nGoodBye!\\n```\\n\\n# Contributing\\n\\nContributions are welcomed of course. We like to use `black` and `flake8`.\\n\\n```bash\\npip install -r requirements-dev.txt  # installs useful dev deps\\npre-commit install  # installs useful commit hooks\\n```\", \"description_content_type\": \"text/markdown\", \"summary\": \"A Python connector for Druid.\", \"latest_version\": \"0.6.9\", \"weekly_downloads\": 773455, \"description_cleaned\": \"pydruid\\npydruid exposes a simple API to create, execute, and analyze\\nDruid\\nqueries. pydruid can parse query results into\\nPandas\\nDataFrame objects for subsequent data analysis -- this offers a tight integration between\\nDruid\\n, the\\nSciPy\\nstack (for scientific computing) and\\nscikit-learn\\n(for machine learning). pydruid can export query results into TSV or JSON for further processing with your favorite tool, e.g., R, Julia, Matlab, Excel. It provides both synchronous and asynchronous clients.\\nAdditionally, pydruid implements the\\nPython DB API 2.0\\n, a\\nSQLAlchemy dialect\\n, and a provides a command line interface to interact with Druid.\\nTo install:\\npip\\ninstall\\npydruid\\n# or, if you intend to use asynchronous client\\npip\\ninstall\\npydruid\\n[\\nasync\\n]\\n# or, if you intend to export query results into pandas\\npip\\ninstall\\npydruid\\n[\\npandas\\n]\\n# or, if you intend to do both\\npip\\ninstall\\npydruid\\n[\\nasync\\n,\\npandas\\n]\\n# or, if you want to use the SQLAlchemy engine\\npip\\ninstall\\npydruid\\n[\\nsqlalchemy\\n]\\n# or, if you want to use the CLI\\npip\\ninstall\\npydruid\\n[\\ncli\\n]\\nDocumentation:\\nhttps://pythonhosted.org/pydruid/\\n.\\nexamples\\nThe following exampes show how to execute and analyze the results of three types of queries: timeseries, topN, and groupby. We will use these queries to ask simple questions about twitter's public data set.\\ntimeseries\\nWhat was the average tweet length, per day, surrounding the 2014 Sochi olympics?\\nfrom\\npydruid.client\\nimport\\n*\\nfrom\\npylab\\nimport\\nplt\\nquery\\n=\\nPyDruid\\n(\\ndruid_url_goes_here\\n,\\n'druid/v2'\\n)\\nts\\n=\\nquery\\n.\\ntimeseries\\n(\\ndatasource\\n=\\n'twitterstream'\\n,\\ngranularity\\n=\\n'day'\\n,\\nintervals\\n=\\n'2014-02-02/p4w'\\n,\\naggregations\\n=\\n{\\n'length'\\n:\\ndoublesum\\n(\\n'tweet_length'\\n),\\n'count'\\n:\\ndoublesum\\n(\\n'count'\\n)},\\npost_aggregations\\n=\\n{\\n'avg_tweet_length'\\n:\\n(\\nField\\n(\\n'length'\\n)\\n/\\nField\\n(\\n'count'\\n))},\\nfilter\\n=\\nDimension\\n(\\n'first_hashtag'\\n)\\n==\\n'sochi2014'\\n)\\ndf\\n=\\nquery\\n.\\nexport_pandas\\n()\\ndf\\n[\\n'timestamp'\\n]\\n=\\ndf\\n[\\n'timestamp'\\n]\\n.\\nmap\\n(\\nlambda\\nx\\n:\\nx\\n.\\nsplit\\n(\\n'T'\\n)[\\n0\\n])\\ndf\\n.\\nplot\\n(\\nx\\n=\\n'timestamp'\\n,\\ny\\n=\\n'avg_tweet_length'\\n,\\nylim\\n=\\n(\\n80\\n,\\n140\\n),\\nrot\\n=\\n20\\n,\\ntitle\\n=\\n'Sochi 2014'\\n)\\nplt\\n.\\nylabel\\n(\\n'avg tweet length (chars)'\\n)\\nplt\\n.\\nshow\\n()\\ntopN\\nWho were the top ten mentions (@user_name) during the 2014 Oscars?\\ntop\\n=\\nquery\\n.\\ntopn\\n(\\ndatasource\\n=\\n'twitterstream'\\n,\\ngranularity\\n=\\n'all'\\n,\\nintervals\\n=\\n'2014-03-03/p1d'\\n,\\n# utc time of 2014 oscars\\naggregations\\n=\\n{\\n'count'\\n:\\ndoublesum\\n(\\n'count'\\n)},\\ndimension\\n=\\n'user_mention_name'\\n,\\nfilter\\n=\\n(\\nDimension\\n(\\n'user_lang'\\n)\\n==\\n'en'\\n)\\n&\\n(\\nDimension\\n(\\n'first_hashtag'\\n)\\n==\\n'oscars'\\n)\\n&\\n(\\nDimension\\n(\\n'user_time_zone'\\n)\\n==\\n'Pacific Time (US & Canada)'\\n)\\n&\\n~\\n(\\nDimension\\n(\\n'user_mention_name'\\n)\\n==\\n'No Mention'\\n),\\nmetric\\n=\\n'count'\\n,\\nthreshold\\n=\\n10\\n)\\ndf\\n=\\nquery\\n.\\nexport_pandas\\n()\\nprint\\ndf\\ncount\\ntimestamp\\nuser_mention_name\\n0\\n1303\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nTheEllenShow\\n1\\n44\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nTheAcademy\\n2\\n21\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nMTV\\n3\\n21\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\npeoplemag\\n4\\n17\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nTHR\\n5\\n16\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nItsQueenElsa\\n6\\n16\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\neonline\\n7\\n15\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nPerezHilton\\n8\\n14\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nrealjohngreen\\n9\\n12\\n2014\\n-\\n03\\n-\\n03\\nT00\\n:\\n00\\n:\\n00.000\\nZ\\nKevinSpacey\\ngroupby\\nWhat does the social network of users replying to other users look like?\\nfrom\\nigraph\\nimport\\n*\\nfrom\\ncairo\\nimport\\n*\\nfrom\\npandas\\nimport\\nconcat\\ngroup\\n=\\nquery\\n.\\ngroupby\\n(\\ndatasource\\n=\\n'twitterstream'\\n,\\ngranularity\\n=\\n'hour'\\n,\\nintervals\\n=\\n'2013-10-04/pt12h'\\n,\\ndimensions\\n=\\n[\\n\\\"user_name\\\"\\n,\\n\\\"reply_to_name\\\"\\n],\\nfilter\\n=\\n(\\n~\\n(\\nDimension\\n(\\n\\\"reply_to_name\\\"\\n)\\n==\\n\\\"Not A Reply\\\"\\n))\\n&\\n(\\nDimension\\n(\\n\\\"user_location\\\"\\n)\\n==\\n\\\"California\\\"\\n),\\naggregations\\n=\\n{\\n\\\"count\\\"\\n:\\ndoublesum\\n(\\n\\\"count\\\"\\n)}\\n)\\ndf\\n=\\nquery\\n.\\nexport_pandas\\n()\\n# map names to categorical variables with a lookup table\\nnames\\n=\\nconcat\\n([\\ndf\\n[\\n'user_name'\\n],\\ndf\\n[\\n'reply_to_name'\\n]])\\n.\\nunique\\n()\\nnameLookup\\n=\\ndict\\n([\\npair\\n[::\\n-\\n1\\n]\\nfor\\npair\\nin\\nenumerate\\n(\\nnames\\n)])\\ndf\\n[\\n'user_name_lookup'\\n]\\n=\\ndf\\n[\\n'user_name'\\n]\\n.\\nmap\\n(\\nnameLookup\\n.\\nget\\n)\\ndf\\n[\\n'reply_to_name_lookup'\\n]\\n=\\ndf\\n[\\n'reply_to_name'\\n]\\n.\\nmap\\n(\\nnameLookup\\n.\\nget\\n)\\n# create the graph with igraph\\ng\\n=\\nGraph\\n(\\nlen\\n(\\nnames\\n),\\ndirected\\n=\\nFalse\\n)\\nvertices\\n=\\nzip\\n(\\ndf\\n[\\n'user_name_lookup'\\n],\\ndf\\n[\\n'reply_to_name_lookup'\\n])\\ng\\n.\\nvs\\n[\\n\\\"name\\\"\\n]\\n=\\nnames\\ng\\n.\\nadd_edges\\n(\\nvertices\\n)\\nlayout\\n=\\ng\\n.\\nlayout_fruchterman_reingold\\n()\\nplot\\n(\\ng\\n,\\n\\\"tweets.png\\\"\\n,\\nlayout\\n=\\nlayout\\n,\\nvertex_size\\n=\\n2\\n,\\nbbox\\n=\\n(\\n400\\n,\\n400\\n),\\nmargin\\n=\\n25\\n,\\nedge_width\\n=\\n1\\n,\\nvertex_color\\n=\\n\\\"blue\\\"\\n)\\nasynchronous client\\npydruid.async_client.AsyncPyDruid\\nimplements an asynchronous client. To achieve that, it utilizes an asynchronous\\nHTTP client from\\nTornado\\nframework. The asynchronous client is suitable for use with async frameworks such as Tornado\\nand provides much better performance at scale. It lets you serve multiple requests at the same time, without blocking on\\nDruid executing your queries.\\nexample\\nfrom\\ntornado\\nimport\\ngen\\nfrom\\npydruid.async_client\\nimport\\nAsyncPyDruid\\nfrom\\npydruid.utils.aggregators\\nimport\\nlongsum\\nfrom\\npydruid.utils.filters\\nimport\\nDimension\\nclient\\n=\\nAsyncPyDruid\\n(\\nurl_to_druid_broker\\n,\\n'druid/v2'\\n)\\n@gen\\n.\\ncoroutine\\ndef\\nyour_asynchronous_method_serving_top10_mentions_for_day\\n(\\nday\\ntop_mentions\\n=\\nyield\\nclient\\n.\\ntopn\\n(\\ndatasource\\n=\\n'twitterstream'\\n,\\ngranularity\\n=\\n'all'\\n,\\nintervals\\n=\\n\\\"\\n%s\\n/p1d\\\"\\n%\\n(\\nday\\n,\\n),\\naggregations\\n=\\n{\\n'count'\\n:\\ndoublesum\\n(\\n'count'\\n)},\\ndimension\\n=\\n'user_mention_name'\\n,\\nfilter\\n=\\n(\\nDimension\\n(\\n'user_lang'\\n)\\n==\\n'en'\\n)\\n&\\n(\\nDimension\\n(\\n'first_hashtag'\\n)\\n==\\n'oscars'\\n)\\n&\\n(\\nDimension\\n(\\n'user_time_zone'\\n)\\n==\\n'Pacific Time (US & Canada)'\\n)\\n&\\n~\\n(\\nDimension\\n(\\n'user_mention_name'\\n)\\n==\\n'No Mention'\\n),\\nmetric\\n=\\n'count'\\n,\\nthreshold\\n=\\n10\\n)\\n# asynchronously return results\\n# can be simply ```return top_mentions``` in python 3.x\\nraise\\ngen\\n.\\nReturn\\n(\\ntop_mentions\\n)\\nthetaSketches\\nTheta sketch Post aggregators are built slightly differently to normal Post Aggregators, as they have different operators.\\nNote: you must have the\\ndruid-datasketches\\nextension loaded into your Druid cluster in order to use these.\\nSee the\\nDruid datasketches\\ndocumentation for details.\\nfrom\\npydruid.client\\nimport\\n*\\nfrom\\npydruid.utils\\nimport\\naggregators\\nfrom\\npydruid.utils\\nimport\\nfilters\\nfrom\\npydruid.utils\\nimport\\npostaggregator\\nquery\\n=\\nPyDruid\\n(\\nurl_to_druid_broker\\n,\\n'druid/v2'\\n)\\nts\\n=\\nquery\\n.\\ngroupby\\n(\\ndatasource\\n=\\n'test_datasource'\\n,\\ngranularity\\n=\\n'all'\\n,\\nintervals\\n=\\n'2016-09-01/P1M'\\n,\\nfilter\\n=\\n(\\nfilters\\n.\\nDimension\\n(\\n'product'\\n)\\n.\\nin_\\n([\\n'product_A'\\n,\\n'product_B'\\n])),\\naggregations\\n=\\n{\\n'product_A_users'\\n:\\naggregators\\n.\\nfiltered\\n(\\nfilters\\n.\\nDimension\\n(\\n'product'\\n)\\n==\\n'product_A'\\n,\\naggregators\\n.\\nthetasketch\\n(\\n'user_id'\\n)\\n),\\n'product_B_users'\\n:\\naggregators\\n.\\nfiltered\\n(\\nfilters\\n.\\nDimension\\n(\\n'product'\\n)\\n==\\n'product_B'\\n,\\naggregators\\n.\\nthetasketch\\n(\\n'user_id'\\n)\\n)\\n},\\npost_aggregations\\n=\\n{\\n'both_A_and_B'\\n:\\npostaggregator\\n.\\nThetaSketchEstimate\\n(\\npostaggregator\\n.\\nThetaSketch\\n(\\n'product_A_users'\\n)\\n&\\npostaggregator\\n.\\nThetaSketch\\n(\\n'product_B_users'\\n)\\n)\\n}\\n)\\nDB API\\nfrom\\npydruid.db\\nimport\\nconnect\\nconn\\n=\\nconnect\\n(\\nhost\\n=\\n'localhost'\\n,\\nport\\n=\\n8082\\n,\\npath\\n=\\n'/druid/v2/sql/'\\n,\\nscheme\\n=\\n'http'\\n)\\ncurs\\n=\\nconn\\n.\\ncursor\\n()\\ncurs\\n.\\nexecute\\n(\\n\\\"\\\"\\\"\\nSELECT place,\\nCAST(REGEXP_EXTRACT(place, '(.*),', 1) AS FLOAT) AS lat,\\nCAST(REGEXP_EXTRACT(place, ',(.*)', 1) AS FLOAT) AS lon\\nFROM places\\nLIMIT 10\\n\\\"\\\"\\\"\\n)\\nfor\\nrow\\nin\\ncurs\\n:\\nprint\\n(\\nrow\\n)\\nSQLAlchemy\\nfrom\\nsqlalchemy\\nimport\\n*\\nfrom\\nsqlalchemy.engine\\nimport\\ncreate_engine\\nfrom\\nsqlalchemy.schema\\nimport\\n*\\nengine\\n=\\ncreate_engine\\n(\\n'druid://localhost:8082/druid/v2/sql/'\\n)\\n# uses HTTP by default :(\\n# engine = create_engine('druid+http://localhost:8082/druid/v2/sql/')\\n# engine = create_engine('druid+https://localhost:8082/druid/v2/sql/')\\nplaces\\n=\\nTable\\n(\\n'places'\\n,\\nMetaData\\n(\\nbind\\n=\\nengine\\n),\\nautoload\\n=\\nTrue\\n)\\nprint\\n(\\nselect\\n([\\nfunc\\n.\\ncount\\n(\\n'*'\\n)],\\nfrom_obj\\n=\\nplaces\\n)\\n.\\nscalar\\n())\\nColumn headers\\nIn version 0.13.0 Druid SQL added support for including the column names in the\\nresponse which can be requested via the \\\"header\\\" field in the request. This\\nhelps to ensure that the cursor description is defined (which is a requirement\\nfor SQLAlchemy query statements) regardless on whether the result set contains\\nany rows. Historically this was problematic for result sets which contained no\\nrows at one could not infer the expected column names.\\nEnabling the header can be configured via the SQLAlchemy URI by using the query\\nparameter, i.e.,\\nengine\\n=\\ncreate_engine\\n(\\n'druid://localhost:8082/druid/v2/sql?header=true'\\n)\\nNote the current default is\\nfalse\\nto ensure backwards compatibility but should\\nbe set to\\ntrue\\nfor Druid versions >= 0.13.0.\\nCommand line\\n$\\npydruid\\nhttp://localhost:8082/druid/v2/sql/\\n>\\nSELECT\\nCOUNT\\n(\\n*\\n)\\nAS\\ncnt\\nFROM\\nplaces\\ncnt\\n-----\\n12345\\n>\\nSELECT\\nTABLE_NAME\\nFROM\\nINFORMATION_SCHEMA.TABLES\\n;\\nTABLE_NAME\\n----------\\ntest_table\\nCOLUMNS\\nSCHEMATA\\nTABLES\\n>\\nBYE\\n;\\nGoodBye!\\nContributing\\nContributions are welcomed of course. We like to use\\nblack\\nand\\nflake8\\n.\\npip\\ninstall\\n-r\\nrequirements-dev.txt\\n# installs useful dev deps\\npre-commit\\ninstall\\n# installs useful commit hooks\"}, {\"name\": \"eth-hash\", \"description\": \"# eth-hash\\n\\n[![Join the conversation on Discord](https://img.shields.io/discord/809793915578089484?color=blue&label=chat&logo=discord&logoColor=white)](https://discord.gg/GHryRvPB84)\\n[![Build Status](https://circleci.com/gh/ethereum/eth-hash.svg?style=shield)](https://circleci.com/gh/ethereum/eth-hash)\\n[![PyPI version](https://badge.fury.io/py/eth-hash.svg)](https://badge.fury.io/py/eth-hash)\\n[![Python versions](https://img.shields.io/pypi/pyversions/eth-hash.svg)](https://pypi.python.org/pypi/eth-hash)\\n[![Docs build](https://readthedocs.org/projects/eth-hash/badge/?version=latest)](https://eth-hash.readthedocs.io/en/latest/?badge=latest)\\n\\nThe Ethereum hashing function, keccak256, sometimes (erroneously) called sha3\\n\\nNote: the similarly named [pyethash](https://github.com/ethereum/ethash)\\nhas a completely different use: it generates proofs of work.\\n\\nThis is a low-level library, intended to be used internally by other Ethereum tools.\\nIf you're looking for a convenient hashing tool, check out\\n[`eth_utils.keccak()`](https://eth-utils.readthedocs.io/en/stable/utilities.html#keccak-bytes-int-bool-text-str-hexstr-str-bytes)\\nwhich will be a little friendlier, and provide access to other helpful utilities.\\n\\nRead the [documentation](https://eth-hash.readthedocs.io/).\\n\\n[View the change log](https://eth-hash.readthedocs.io/en/latest/release_notes.html).\\n\\n## Installation\\n\\n```sh\\npython -m pip install \\\"eth-hash[pycryptodome]\\\"\\n```\\n\\n```py\\n>>> from eth_hash.auto import keccak\\n>>> keccak(b'')\\nb\\\"\\\\xc5\\\\xd2F\\\\x01\\\\x86\\\\xf7#<\\\\x92~}\\\\xb2\\\\xdc\\\\xc7\\\\x03\\\\xc0\\\\xe5\\\\x00\\\\xb6S\\\\xca\\\\x82';{\\\\xfa\\\\xd8\\\\x04]\\\\x85\\\\xa4p\\\"\\n```\\n\\nSee the [docs](http://eth-hash.readthedocs.io/en/latest/quickstart.html#quickstart)\\nfor more about choosing and installing backends.\\n\", \"description_content_type\": \"text/markdown\", \"summary\": \"eth-hash: The Ethereum hashing function, keccak256, sometimes (erroneously) called sha3\", \"latest_version\": \"0.7.1\", \"weekly_downloads\": 773276, \"description_cleaned\": \"eth-hash\\nThe Ethereum hashing function, keccak256, sometimes (erroneously) called sha3\\nNote: the similarly named\\npyethash\\nhas a completely different use: it generates proofs of work.\\nThis is a low-level library, intended to be used internally by other Ethereum tools.\\nIf you're looking for a convenient hashing tool, check out\\neth_utils.keccak()\\nwhich will be a little friendlier, and provide access to other helpful utilities.\\nRead the\\ndocumentation\\n.\\nView the change log\\n.\\nInstallation\\npython\\n-m\\npip\\ninstall\\n\\\"eth-hash[pycryptodome]\\\"\\n>>>\\nfrom\\neth_hash.auto\\nimport\\nkeccak\\n>>>\\nkeccak\\n(\\nb\\n''\\n)\\nb\\n\\\"\\n\\\\xc5\\\\xd2\\nF\\n\\\\x01\\\\x86\\\\xf7\\n#<\\n\\\\x92\\n~}\\n\\\\xb2\\\\xdc\\\\xc7\\\\x03\\\\xc0\\\\xe5\\\\x00\\\\xb6\\nS\\n\\\\xca\\\\x82\\n';{\\n\\\\xfa\\\\xd8\\\\x04\\n]\\n\\\\x85\\\\xa4\\np\\\"\\nSee the\\ndocs\\nfor more about choosing and installing backends.\"}]"
                    }
                }
            ],
            "console": []
        },
        {
            "id": "CCKw",
            "code_hash": null,
            "outputs": [],
            "console": []
        }
    ]
}
